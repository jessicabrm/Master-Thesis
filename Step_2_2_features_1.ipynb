{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "\n",
    "1. Loading the Data\n",
    "1. Data Cleaning\n",
    "1. Feature Engineering\n",
    "\n",
    "\n",
    "Data cleaning and Feature engineering with NLTK (Bird et al., 2009).\n",
    "\n",
    "Spellchecking (PyEnchant, n.d.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jessi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Preprocessing\n",
    "import string \n",
    "import nltk\n",
    "# tokenization\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import re # punctuation removal\n",
    "from nltk.corpus import stopwords # removing stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# POS feature\n",
    "from collections import Counter\n",
    "\n",
    "#pip install -U pip setuptools wheel\n",
    "#pip install -U 'spacy[cuda114]'\n",
    "#python -m spacy download en_core_web_lg\n",
    "#python -m spacy download nl_core_news_lg\n",
    "import spacy\n",
    "\n",
    "#  pip install wget\n",
    "import wget\n",
    "# download once\n",
    "#wget.download(\"https://bda2019syllables.netlify.com/en_syllable_3grams.csv.zip\")\n",
    "\n",
    "# pip install lexical-diversity\n",
    "from lexical_diversity import lex_div as ld\n",
    "\n",
    "from string import ascii_lowercase\n",
    "import random\n",
    "\n",
    "\n",
    "import math\n",
    "import collections\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Kaggle data + preprocessed essays + detected errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1_errors_detected = pd.read_csv(\"csv_files/features_training_set_1.csv\")\n",
    "training_data_2_errors_detected = pd.read_csv(\"csv_files/features_training_set_2.csv\")\n",
    "training_data_3_errors_detected = pd.read_csv(\"csv_files/features_training_set_3.csv\")\n",
    "training_data_4_errors_detected = pd.read_csv(\"csv_files/features_training_set_4.csv\")\n",
    "training_data_5_errors_detected = pd.read_csv(\"csv_files/features_training_set_5.csv\")\n",
    "training_data_6_errors_detected = pd.read_csv(\"csv_files/features_training_set_6.csv\")\n",
    "training_data_7_errors_detected = pd.read_csv(\"csv_files/features_training_set_7.csv\")\n",
    "training_data_8_errors_detected = pd.read_csv(\"csv_files/features_training_set_8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from: https://www.geeksforgeeks.org/python-convert-string-enclosed-list-to-list/\n",
    "def convert(lst):\n",
    "    return eval(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [[[misspelling, EN_CONTRACTION_SPELLING]], [[u...\n",
       "1      [[[grammar, MOST_SUPERLATIVE]], [[typographica...\n",
       "2      [[[typographical, UPPERCASE_SENTENCE_START]], ...\n",
       "3      [[[whitespace, WHITESPACE_RULE]], [[typographi...\n",
       "4      [[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA],...\n",
       "                             ...                        \n",
       "718    [[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA]]...\n",
       "719    [[[uncategorized, AI_HYDRA_LEO_MISSING_OF]], [...\n",
       "720    [[[typographical, UPPERCASE_SENTENCE_START]], ...\n",
       "721    [[[misspelling, MORFOLOGIK_RULE_EN_US], [uncat...\n",
       "722    [[[typographical, UPPERCASE_SENTENCE_START], [...\n",
       "Name: error_types, Length: 723, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Driver code\n",
    "training_data_1_errors_detected[\"error_types\"] = training_data_1_errors_detected[\"error_types\"].apply(convert)\n",
    "training_data_2_errors_detected[\"error_types\"] = training_data_2_errors_detected[\"error_types\"].apply(convert)\n",
    "training_data_3_errors_detected[\"error_types\"] = training_data_3_errors_detected[\"error_types\"].apply(convert)\n",
    "training_data_4_errors_detected[\"error_types\"] = training_data_4_errors_detected[\"error_types\"].apply(convert)\n",
    "training_data_5_errors_detected[\"error_types\"] = training_data_5_errors_detected[\"error_types\"].apply(convert)\n",
    "training_data_6_errors_detected[\"error_types\"] = training_data_6_errors_detected[\"error_types\"].apply(convert)\n",
    "training_data_7_errors_detected[\"error_types\"] = training_data_7_errors_detected[\"error_types\"].apply(convert)\n",
    "training_data_8_errors_detected[\"error_types\"] = training_data_8_errors_detected[\"error_types\"].apply(convert)\n",
    "training_data_8_errors_detected[\"error_types\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>normalised_docs</th>\n",
       "      <th>word_tok</th>\n",
       "      <th>sentence_tok</th>\n",
       "      <th>stemmed_word_token</th>\n",
       "      <th>stemmed_no_stopwords</th>\n",
       "      <th>error_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dear local newspaper, i think effects computer...</td>\n",
       "      <td>[Dear, local, newspaper, I, think, effects, co...</td>\n",
       "      <td>[Dear local newspaper, I think effects compute...</td>\n",
       "      <td>[dear, local, newspap, i, think, effect, compu...</td>\n",
       "      <td>[dear, local, newspap, think, effect, comput, ...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dear @caps1 @caps2, i believe that using compu...</td>\n",
       "      <td>[Dear, CAPS1, CAPS2, I, believe, that, using, ...</td>\n",
       "      <td>[Dear @CAPS1, @CAPS2, I believe that using com...</td>\n",
       "      <td>[dear, caps1, caps2, i, believ, that, use, com...</td>\n",
       "      <td>[dear, caps1, caps2, believ, use, comput, bene...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US], [missp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dear, @caps1 @caps2 @caps3 more and more peopl...</td>\n",
       "      <td>[Dear, CAPS1, CAPS2, CAPS3, More, and, more, p...</td>\n",
       "      <td>[Dear, @CAPS1 @CAPS2 @CAPS3, More and more peo...</td>\n",
       "      <td>[dear, caps1, caps2, caps3, more, and, more, p...</td>\n",
       "      <td>[dear, caps1, caps2, caps3, peopl, use, comput...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START]], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dear local newspaper, @caps1 i have found that...</td>\n",
       "      <td>[Dear, Local, Newspaper, CAPS1, I, have, found...</td>\n",
       "      <td>[Dear Local Newspaper, @CAPS1, I have found th...</td>\n",
       "      <td>[dear, local, newspap, caps1, i, have, found, ...</td>\n",
       "      <td>[dear, local, newspap, caps1, found, mani, exp...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US]], [[unc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dear @location1, i know having computers has a...</td>\n",
       "      <td>[Dear, LOCATION1, I, know, having, computers, ...</td>\n",
       "      <td>[Dear @LOCATION1, I know having computers has ...</td>\n",
       "      <td>[dear, location1, i, know, have, comput, has, ...</td>\n",
       "      <td>[dear, location1, know, comput, posit, effect,...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>1783</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1, @CAPS2 several reasons on way I t...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dear @caps1, @caps2 several reasons on way i t...</td>\n",
       "      <td>[Dear, CAPS1, CAPS2, several, reasons, on, way...</td>\n",
       "      <td>[Dear @CAPS1, @CAPS2 several reasons on way I ...</td>\n",
       "      <td>[dear, caps1, caps2, sever, reason, on, way, i...</td>\n",
       "      <td>[dear, caps1, caps2, sever, reason, way, advan...</td>\n",
       "      <td>[[[non-conformance, CONFUSION_RULE_ON_ONE], [m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>1784</td>\n",
       "      <td>1</td>\n",
       "      <td>Do a adults and kids spend to much time on the...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>do a adults and kids spend to much time on the...</td>\n",
       "      <td>[Do, a, adults, and, kids, spend, to, much, ti...</td>\n",
       "      <td>[Do a adults and kids spend to much time on th...</td>\n",
       "      <td>[do, a, adult, and, kid, spend, to, much, time...</td>\n",
       "      <td>[adult, kid, spend, much, time, comput, well, ...</td>\n",
       "      <td>[[[misspelling, EN_A_VS_AN], [misspelling, TO_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>1785</td>\n",
       "      <td>1</td>\n",
       "      <td>My opinion is that people should have computer...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my opinion is that people should have computer...</td>\n",
       "      <td>[My, opinion, is, that, people, should, have, ...</td>\n",
       "      <td>[My opinion is that people should have compute...</td>\n",
       "      <td>[my, opinion, is, that, peopl, should, have, c...</td>\n",
       "      <td>[opinion, peopl, comput, home, comput, import,...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US], [uncat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>1786</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear readers, I think that its good and bad to...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dear readers, i think that its good and bad to...</td>\n",
       "      <td>[Dear, readers, I, think, that, its, good, and...</td>\n",
       "      <td>[Dear readers, I think that its good and bad t...</td>\n",
       "      <td>[dear, reader, i, think, that, it, good, and, ...</td>\n",
       "      <td>[dear, reader, think, good, bad, use, comput, ...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_APOSTROPHE_S_XS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>1787</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear - Local Newspaper I agree thats computers...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dear - local newspaper i agree thats computers...</td>\n",
       "      <td>[Dear, Local, Newspaper, I, agree, that, s, co...</td>\n",
       "      <td>[Dear - Local Newspaper I agree thats computer...</td>\n",
       "      <td>[dear, local, newspap, i, agre, that, s, compu...</td>\n",
       "      <td>[dear, local, newspap, agre, comput, good, soc...</td>\n",
       "      <td>[[[misspelling, EN_CONTRACTION_SPELLING]], [[u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1783 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id  essay_set                                              essay  \\\n",
       "0            1          1  Dear local newspaper, I think effects computer...   \n",
       "1            2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2            3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3            4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4            5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "...        ...        ...                                                ...   \n",
       "1778      1783          1  Dear @CAPS1, @CAPS2 several reasons on way I t...   \n",
       "1779      1784          1  Do a adults and kids spend to much time on the...   \n",
       "1780      1785          1  My opinion is that people should have computer...   \n",
       "1781      1786          1  Dear readers, I think that its good and bad to...   \n",
       "1782      1787          1  Dear - Local Newspaper I agree thats computers...   \n",
       "\n",
       "      rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0                4.0             4.0             NaN            8.0   \n",
       "1                5.0             4.0             NaN            9.0   \n",
       "2                4.0             3.0             NaN            7.0   \n",
       "3                5.0             5.0             NaN           10.0   \n",
       "4                4.0             4.0             NaN            8.0   \n",
       "...              ...             ...             ...            ...   \n",
       "1778             4.0             4.0             NaN            8.0   \n",
       "1779             3.0             4.0             NaN            7.0   \n",
       "1780             4.0             4.0             NaN            8.0   \n",
       "1781             1.0             1.0             NaN            2.0   \n",
       "1782             4.0             3.0             NaN            7.0   \n",
       "\n",
       "      rater1_domain2  rater2_domain2  domain2_score  ...  rater3_trait3  \\\n",
       "0                NaN             NaN            NaN  ...            NaN   \n",
       "1                NaN             NaN            NaN  ...            NaN   \n",
       "2                NaN             NaN            NaN  ...            NaN   \n",
       "3                NaN             NaN            NaN  ...            NaN   \n",
       "4                NaN             NaN            NaN  ...            NaN   \n",
       "...              ...             ...            ...  ...            ...   \n",
       "1778             NaN             NaN            NaN  ...            NaN   \n",
       "1779             NaN             NaN            NaN  ...            NaN   \n",
       "1780             NaN             NaN            NaN  ...            NaN   \n",
       "1781             NaN             NaN            NaN  ...            NaN   \n",
       "1782             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "      rater3_trait4  rater3_trait5  rater3_trait6  \\\n",
       "0               NaN            NaN            NaN   \n",
       "1               NaN            NaN            NaN   \n",
       "2               NaN            NaN            NaN   \n",
       "3               NaN            NaN            NaN   \n",
       "4               NaN            NaN            NaN   \n",
       "...             ...            ...            ...   \n",
       "1778            NaN            NaN            NaN   \n",
       "1779            NaN            NaN            NaN   \n",
       "1780            NaN            NaN            NaN   \n",
       "1781            NaN            NaN            NaN   \n",
       "1782            NaN            NaN            NaN   \n",
       "\n",
       "                                        normalised_docs  \\\n",
       "0     dear local newspaper, i think effects computer...   \n",
       "1     dear @caps1 @caps2, i believe that using compu...   \n",
       "2     dear, @caps1 @caps2 @caps3 more and more peopl...   \n",
       "3     dear local newspaper, @caps1 i have found that...   \n",
       "4     dear @location1, i know having computers has a...   \n",
       "...                                                 ...   \n",
       "1778  dear @caps1, @caps2 several reasons on way i t...   \n",
       "1779  do a adults and kids spend to much time on the...   \n",
       "1780  my opinion is that people should have computer...   \n",
       "1781  dear readers, i think that its good and bad to...   \n",
       "1782  dear - local newspaper i agree thats computers...   \n",
       "\n",
       "                                               word_tok  \\\n",
       "0     [Dear, local, newspaper, I, think, effects, co...   \n",
       "1     [Dear, CAPS1, CAPS2, I, believe, that, using, ...   \n",
       "2     [Dear, CAPS1, CAPS2, CAPS3, More, and, more, p...   \n",
       "3     [Dear, Local, Newspaper, CAPS1, I, have, found...   \n",
       "4     [Dear, LOCATION1, I, know, having, computers, ...   \n",
       "...                                                 ...   \n",
       "1778  [Dear, CAPS1, CAPS2, several, reasons, on, way...   \n",
       "1779  [Do, a, adults, and, kids, spend, to, much, ti...   \n",
       "1780  [My, opinion, is, that, people, should, have, ...   \n",
       "1781  [Dear, readers, I, think, that, its, good, and...   \n",
       "1782  [Dear, Local, Newspaper, I, agree, that, s, co...   \n",
       "\n",
       "                                           sentence_tok  \\\n",
       "0     [Dear local newspaper, I think effects compute...   \n",
       "1     [Dear @CAPS1, @CAPS2, I believe that using com...   \n",
       "2     [Dear, @CAPS1 @CAPS2 @CAPS3, More and more peo...   \n",
       "3     [Dear Local Newspaper, @CAPS1, I have found th...   \n",
       "4     [Dear @LOCATION1, I know having computers has ...   \n",
       "...                                                 ...   \n",
       "1778  [Dear @CAPS1, @CAPS2 several reasons on way I ...   \n",
       "1779  [Do a adults and kids spend to much time on th...   \n",
       "1780  [My opinion is that people should have compute...   \n",
       "1781  [Dear readers, I think that its good and bad t...   \n",
       "1782  [Dear - Local Newspaper I agree thats computer...   \n",
       "\n",
       "                                     stemmed_word_token  \\\n",
       "0     [dear, local, newspap, i, think, effect, compu...   \n",
       "1     [dear, caps1, caps2, i, believ, that, use, com...   \n",
       "2     [dear, caps1, caps2, caps3, more, and, more, p...   \n",
       "3     [dear, local, newspap, caps1, i, have, found, ...   \n",
       "4     [dear, location1, i, know, have, comput, has, ...   \n",
       "...                                                 ...   \n",
       "1778  [dear, caps1, caps2, sever, reason, on, way, i...   \n",
       "1779  [do, a, adult, and, kid, spend, to, much, time...   \n",
       "1780  [my, opinion, is, that, peopl, should, have, c...   \n",
       "1781  [dear, reader, i, think, that, it, good, and, ...   \n",
       "1782  [dear, local, newspap, i, agre, that, s, compu...   \n",
       "\n",
       "                                   stemmed_no_stopwords  \\\n",
       "0     [dear, local, newspap, think, effect, comput, ...   \n",
       "1     [dear, caps1, caps2, believ, use, comput, bene...   \n",
       "2     [dear, caps1, caps2, caps3, peopl, use, comput...   \n",
       "3     [dear, local, newspap, caps1, found, mani, exp...   \n",
       "4     [dear, location1, know, comput, posit, effect,...   \n",
       "...                                                 ...   \n",
       "1778  [dear, caps1, caps2, sever, reason, way, advan...   \n",
       "1779  [adult, kid, spend, much, time, comput, well, ...   \n",
       "1780  [opinion, peopl, comput, home, comput, import,...   \n",
       "1781  [dear, reader, think, good, bad, use, comput, ...   \n",
       "1782  [dear, local, newspap, agre, comput, good, soc...   \n",
       "\n",
       "                                            error_types  \n",
       "0     [[[typographical, UPPERCASE_SENTENCE_START], [...  \n",
       "1     [[[misspelling, MORFOLOGIK_RULE_EN_US], [missp...  \n",
       "2     [[[typographical, UPPERCASE_SENTENCE_START]], ...  \n",
       "3     [[[misspelling, MORFOLOGIK_RULE_EN_US]], [[unc...  \n",
       "4     [[[typographical, UPPERCASE_SENTENCE_START], [...  \n",
       "...                                                 ...  \n",
       "1778  [[[non-conformance, CONFUSION_RULE_ON_ONE], [m...  \n",
       "1779  [[[misspelling, EN_A_VS_AN], [misspelling, TO_...  \n",
       "1780  [[[misspelling, MORFOLOGIK_RULE_EN_US], [uncat...  \n",
       "1781  [[[uncategorized, AI_HYDRA_LEO_APOSTROPHE_S_XS...  \n",
       "1782  [[[misspelling, EN_CONTRACTION_SPELLING]], [[u...  \n",
       "\n",
       "[1783 rows x 34 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1 = pd.read_csv(\"csv_files/preprosessed_essays_training_set_1.csv\", converters={\"remove_single_char\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), \n",
    "                                                                             \"sentence_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), \n",
    "                                                                             \"stemmed_word_token\":lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "                                                                             \"stemmed_no_stopwords\":lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "                                                                             \"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "training_data_1 = training_data_1.set_index(\"essay_id\").join(training_data_1_errors_detected.set_index(\"essay_id\").loc[:, \"error_types\"]).reset_index().copy()\n",
    "training_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>normalised_docs</th>\n",
       "      <th>word_tok</th>\n",
       "      <th>sentence_tok</th>\n",
       "      <th>stemmed_word_token</th>\n",
       "      <th>stemmed_no_stopwords</th>\n",
       "      <th>error_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2978</td>\n",
       "      <td>2</td>\n",
       "      <td>Certain materials being removed from libraries...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>certain materials being removed from libraries...</td>\n",
       "      <td>[Certain, materials, being, removed, from, lib...</td>\n",
       "      <td>[\"Certain materials being removed from librari...</td>\n",
       "      <td>[certain, materi, be, remov, from, librari, su...</td>\n",
       "      <td>[certain, materi, remov, librari, book, music,...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START]], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2979</td>\n",
       "      <td>2</td>\n",
       "      <td>Write a persuasive essay to a newspaper reflec...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>write a persuasive essay to a newspaper reflec...</td>\n",
       "      <td>[Write, a, persuasive, essay, to, a, newspaper...</td>\n",
       "      <td>[Write a persuasive essay to a newspaper refle...</td>\n",
       "      <td>[write, a, persuas, essay, to, a, newspap, ref...</td>\n",
       "      <td>[write, persuas, essay, newspap, reflect, view...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2980</td>\n",
       "      <td>2</td>\n",
       "      <td>Do you think that libraries should remove cert...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>do you think that libraries should remove cert...</td>\n",
       "      <td>[Do, you, think, that, libraries, should, remo...</td>\n",
       "      <td>[Do you think that libraries should remove cer...</td>\n",
       "      <td>[do, you, think, that, librari, should, remov,...</td>\n",
       "      <td>[think, librari, remov, certain, materi, shelv...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US]], [[typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2981</td>\n",
       "      <td>2</td>\n",
       "      <td>In @DATE1's world, there are many things found...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in @date1's world, there are many things found...</td>\n",
       "      <td>[In, DATE1, s, world, there, are, many, things...</td>\n",
       "      <td>[\"In @DATE1s world, there are many things foun...</td>\n",
       "      <td>[in, date1, s, world, there, are, mani, thing,...</td>\n",
       "      <td>[date1, world, mani, thing, found, offens, eve...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START]], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2982</td>\n",
       "      <td>2</td>\n",
       "      <td>In life you have the 'offensive things'. The l...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in life you have the 'offensive things'. the l...</td>\n",
       "      <td>[In, life, you, have, the, offensive, things, ...</td>\n",
       "      <td>[\"In life you have the offensive things.\", The...</td>\n",
       "      <td>[in, life, you, have, the, offens, thing, the,...</td>\n",
       "      <td>[life, offens, thing, littl, stuff, get, skin,...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA]]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>4773</td>\n",
       "      <td>2</td>\n",
       "      <td>The author is writting about taking books off ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the author is writting about taking books off ...</td>\n",
       "      <td>[The, author, is, writting, about, taking, boo...</td>\n",
       "      <td>[\"The author is writting about taking books of...</td>\n",
       "      <td>[the, author, is, writ, about, take, book, off...</td>\n",
       "      <td>[author, writ, take, book, adult, nt, want, bo...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US], [missp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>4774</td>\n",
       "      <td>2</td>\n",
       "      <td>I do not think that materials, such as books, ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i do not think that materials, such as books, ...</td>\n",
       "      <td>[I, do, not, think, that, materials, such, as,...</td>\n",
       "      <td>[I do not think that materials, such as books,...</td>\n",
       "      <td>[i, do, not, think, that, materi, such, as, bo...</td>\n",
       "      <td>[think, materi, book, music, movi, magazin, re...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START]], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>4775</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes we should keep the books,music,movies,an m...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes we should keep the books,music,movies,an m...</td>\n",
       "      <td>[Yes, we, should, keep, the, books, music, mov...</td>\n",
       "      <td>[Yes we should keep the books,music,movies,an ...</td>\n",
       "      <td>[yes, we, should, keep, the, book, music, movi...</td>\n",
       "      <td>[yes, keep, book, music, movi, magazin, magazi...</td>\n",
       "      <td>[[[whitespace, COMMA_PARENTHESIS_WHITESPACE], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>4776</td>\n",
       "      <td>2</td>\n",
       "      <td>I do believe that  book, magazines, music, mov...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i do believe that  book, magazines, music, mov...</td>\n",
       "      <td>[I, do, believe, that, book, magazines, music,...</td>\n",
       "      <td>[I do believe that  book, magazines, music, mo...</td>\n",
       "      <td>[i, do, believ, that, book, magazin, music, mo...</td>\n",
       "      <td>[believ, book, magazin, music, movi, etc, take...</td>\n",
       "      <td>[[[whitespace, WHITESPACE_RULE]], [[typographi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>4777</td>\n",
       "      <td>2</td>\n",
       "      <td>Different Then Everyone Else     @CAPS1 do peo...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>different then everyone else     @caps1 do peo...</td>\n",
       "      <td>[Different, Then, Everyone, Else, CAPS1, do, p...</td>\n",
       "      <td>[Different Then Everyone Else     @CAPS1 do pe...</td>\n",
       "      <td>[differ, then, everyon, els, caps1, do, peopl,...</td>\n",
       "      <td>[differ, everyon, els, caps1, peopl, find, sma...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_CP_THEN_THAN], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id  essay_set                                              essay  \\\n",
       "0         2978          2  Certain materials being removed from libraries...   \n",
       "1         2979          2  Write a persuasive essay to a newspaper reflec...   \n",
       "2         2980          2  Do you think that libraries should remove cert...   \n",
       "3         2981          2  In @DATE1's world, there are many things found...   \n",
       "4         2982          2  In life you have the 'offensive things'. The l...   \n",
       "...        ...        ...                                                ...   \n",
       "1795      4773          2  The author is writting about taking books off ...   \n",
       "1796      4774          2  I do not think that materials, such as books, ...   \n",
       "1797      4775          2  Yes we should keep the books,music,movies,an m...   \n",
       "1798      4776          2  I do believe that  book, magazines, music, mov...   \n",
       "1799      4777          2  Different Then Everyone Else     @CAPS1 do peo...   \n",
       "\n",
       "      rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0                4.0             4.0             NaN            4.0   \n",
       "1                1.0             2.0             NaN            1.0   \n",
       "2                2.0             3.0             NaN            2.0   \n",
       "3                4.0             4.0             NaN            4.0   \n",
       "4                4.0             4.0             NaN            4.0   \n",
       "...              ...             ...             ...            ...   \n",
       "1795             3.0             2.0             NaN            3.0   \n",
       "1796             3.0             3.0             NaN            3.0   \n",
       "1797             2.0             2.0             NaN            2.0   \n",
       "1798             3.0             4.0             NaN            3.0   \n",
       "1799             3.0             3.0             NaN            3.0   \n",
       "\n",
       "      rater1_domain2  rater2_domain2  domain2_score  ...  rater3_trait3  \\\n",
       "0                4.0             4.0            4.0  ...            NaN   \n",
       "1                1.0             2.0            1.0  ...            NaN   \n",
       "2                3.0             3.0            3.0  ...            NaN   \n",
       "3                4.0             4.0            4.0  ...            NaN   \n",
       "4                4.0             4.0            4.0  ...            NaN   \n",
       "...              ...             ...            ...  ...            ...   \n",
       "1795             3.0             2.0            3.0  ...            NaN   \n",
       "1796             3.0             3.0            3.0  ...            NaN   \n",
       "1797             2.0             2.0            2.0  ...            NaN   \n",
       "1798             4.0             3.0            4.0  ...            NaN   \n",
       "1799             2.0             3.0            2.0  ...            NaN   \n",
       "\n",
       "      rater3_trait4  rater3_trait5  rater3_trait6  \\\n",
       "0               NaN            NaN            NaN   \n",
       "1               NaN            NaN            NaN   \n",
       "2               NaN            NaN            NaN   \n",
       "3               NaN            NaN            NaN   \n",
       "4               NaN            NaN            NaN   \n",
       "...             ...            ...            ...   \n",
       "1795            NaN            NaN            NaN   \n",
       "1796            NaN            NaN            NaN   \n",
       "1797            NaN            NaN            NaN   \n",
       "1798            NaN            NaN            NaN   \n",
       "1799            NaN            NaN            NaN   \n",
       "\n",
       "                                        normalised_docs  \\\n",
       "0     certain materials being removed from libraries...   \n",
       "1     write a persuasive essay to a newspaper reflec...   \n",
       "2     do you think that libraries should remove cert...   \n",
       "3     in @date1's world, there are many things found...   \n",
       "4     in life you have the 'offensive things'. the l...   \n",
       "...                                                 ...   \n",
       "1795  the author is writting about taking books off ...   \n",
       "1796  i do not think that materials, such as books, ...   \n",
       "1797  yes we should keep the books,music,movies,an m...   \n",
       "1798  i do believe that  book, magazines, music, mov...   \n",
       "1799  different then everyone else     @caps1 do peo...   \n",
       "\n",
       "                                               word_tok  \\\n",
       "0     [Certain, materials, being, removed, from, lib...   \n",
       "1     [Write, a, persuasive, essay, to, a, newspaper...   \n",
       "2     [Do, you, think, that, libraries, should, remo...   \n",
       "3     [In, DATE1, s, world, there, are, many, things...   \n",
       "4     [In, life, you, have, the, offensive, things, ...   \n",
       "...                                                 ...   \n",
       "1795  [The, author, is, writting, about, taking, boo...   \n",
       "1796  [I, do, not, think, that, materials, such, as,...   \n",
       "1797  [Yes, we, should, keep, the, books, music, mov...   \n",
       "1798  [I, do, believe, that, book, magazines, music,...   \n",
       "1799  [Different, Then, Everyone, Else, CAPS1, do, p...   \n",
       "\n",
       "                                           sentence_tok  \\\n",
       "0     [\"Certain materials being removed from librari...   \n",
       "1     [Write a persuasive essay to a newspaper refle...   \n",
       "2     [Do you think that libraries should remove cer...   \n",
       "3     [\"In @DATE1s world, there are many things foun...   \n",
       "4     [\"In life you have the offensive things.\", The...   \n",
       "...                                                 ...   \n",
       "1795  [\"The author is writting about taking books of...   \n",
       "1796  [I do not think that materials, such as books,...   \n",
       "1797  [Yes we should keep the books,music,movies,an ...   \n",
       "1798  [I do believe that  book, magazines, music, mo...   \n",
       "1799  [Different Then Everyone Else     @CAPS1 do pe...   \n",
       "\n",
       "                                     stemmed_word_token  \\\n",
       "0     [certain, materi, be, remov, from, librari, su...   \n",
       "1     [write, a, persuas, essay, to, a, newspap, ref...   \n",
       "2     [do, you, think, that, librari, should, remov,...   \n",
       "3     [in, date1, s, world, there, are, mani, thing,...   \n",
       "4     [in, life, you, have, the, offens, thing, the,...   \n",
       "...                                                 ...   \n",
       "1795  [the, author, is, writ, about, take, book, off...   \n",
       "1796  [i, do, not, think, that, materi, such, as, bo...   \n",
       "1797  [yes, we, should, keep, the, book, music, movi...   \n",
       "1798  [i, do, believ, that, book, magazin, music, mo...   \n",
       "1799  [differ, then, everyon, els, caps1, do, peopl,...   \n",
       "\n",
       "                                   stemmed_no_stopwords  \\\n",
       "0     [certain, materi, remov, librari, book, music,...   \n",
       "1     [write, persuas, essay, newspap, reflect, view...   \n",
       "2     [think, librari, remov, certain, materi, shelv...   \n",
       "3     [date1, world, mani, thing, found, offens, eve...   \n",
       "4     [life, offens, thing, littl, stuff, get, skin,...   \n",
       "...                                                 ...   \n",
       "1795  [author, writ, take, book, adult, nt, want, bo...   \n",
       "1796  [think, materi, book, music, movi, magazin, re...   \n",
       "1797  [yes, keep, book, music, movi, magazin, magazi...   \n",
       "1798  [believ, book, magazin, music, movi, etc, take...   \n",
       "1799  [differ, everyon, els, caps1, peopl, find, sma...   \n",
       "\n",
       "                                            error_types  \n",
       "0     [[[typographical, UPPERCASE_SENTENCE_START]], ...  \n",
       "1     [[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA],...  \n",
       "2     [[[misspelling, MORFOLOGIK_RULE_EN_US]], [[typ...  \n",
       "3     [[[typographical, UPPERCASE_SENTENCE_START]], ...  \n",
       "4     [[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA]]...  \n",
       "...                                                 ...  \n",
       "1795  [[[misspelling, MORFOLOGIK_RULE_EN_US], [missp...  \n",
       "1796  [[[typographical, UPPERCASE_SENTENCE_START]], ...  \n",
       "1797  [[[whitespace, COMMA_PARENTHESIS_WHITESPACE], ...  \n",
       "1798  [[[whitespace, WHITESPACE_RULE]], [[typographi...  \n",
       "1799  [[[uncategorized, AI_HYDRA_LEO_CP_THEN_THAN], ...  \n",
       "\n",
       "[1800 rows x 34 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_2 = pd.read_csv(\"csv_files/preprosessed_essays_training_set_2.csv\", converters={\"remove_single_char\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), \n",
    "                                                                             \"sentence_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), \n",
    "                                                                             \"stemmed_word_token\":lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "                                                                             \"stemmed_no_stopwords\":lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "                                                                             \"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "training_data_2 = training_data_2.set_index(\"essay_id\").join(training_data_2_errors_detected.set_index(\"essay_id\").loc[:, \"error_types\"]).reset_index().copy()\n",
    "training_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>normalised_docs</th>\n",
       "      <th>word_tok</th>\n",
       "      <th>sentence_tok</th>\n",
       "      <th>stemmed_word_token</th>\n",
       "      <th>stemmed_no_stopwords</th>\n",
       "      <th>error_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5978</td>\n",
       "      <td>3</td>\n",
       "      <td>The features of the setting affect the cyclist...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the features of the setting affect the cyclist...</td>\n",
       "      <td>[The, features, of, the, setting, affect, the,...</td>\n",
       "      <td>[The features of the setting affect the cyclis...</td>\n",
       "      <td>[the, featur, of, the, set, affect, the, cycli...</td>\n",
       "      <td>[featur, set, affect, cyclist, mani, way, feat...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US], [uncat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5979</td>\n",
       "      <td>3</td>\n",
       "      <td>The features of the setting affected the cycli...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the features of the setting affected the cycli...</td>\n",
       "      <td>[The, features, of, the, setting, affected, th...</td>\n",
       "      <td>[The features of the setting affected the cycl...</td>\n",
       "      <td>[the, featur, of, the, set, affect, the, cycli...</td>\n",
       "      <td>[featur, set, affect, cyclist, negat, way, des...</td>\n",
       "      <td>[[[style, IN_A_X_MANNER]], [[typographical, UP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5980</td>\n",
       "      <td>3</td>\n",
       "      <td>Everyone travels to unfamiliar places. Sometim...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>everyone travels to unfamiliar places. sometim...</td>\n",
       "      <td>[Everyone, travels, to, unfamiliar, places, So...</td>\n",
       "      <td>[Everyone travels to unfamiliar places., Somet...</td>\n",
       "      <td>[everyon, travel, to, unfamiliar, place, somet...</td>\n",
       "      <td>[everyon, travel, unfamiliar, place, sometim, ...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA]]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5981</td>\n",
       "      <td>3</td>\n",
       "      <td>I believe the features of the cyclist affected...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i believe the features of the cyclist affected...</td>\n",
       "      <td>[I, believe, the, features, of, the, cyclist, ...</td>\n",
       "      <td>[I believe the features of the cyclist affecte...</td>\n",
       "      <td>[i, believ, the, featur, of, the, cyclist, aff...</td>\n",
       "      <td>[believ, featur, cyclist, affect, becaus, impa...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5982</td>\n",
       "      <td>3</td>\n",
       "      <td>The setting effects the cyclist because of the...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the setting effects the cyclist because of the...</td>\n",
       "      <td>[The, setting, effects, the, cyclist, because,...</td>\n",
       "      <td>[The setting effects the cyclist because of th...</td>\n",
       "      <td>[the, set, effect, the, cyclist, becaus, of, t...</td>\n",
       "      <td>[set, effect, cyclist, becaus, set, diffrent, ...</td>\n",
       "      <td>[[[non-conformance, CONFUSION_RULE_EFFECTS_AFF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>7704</td>\n",
       "      <td>3</td>\n",
       "      <td>In the story, the setting affected the cyclist...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in the story, the setting affected the cyclist...</td>\n",
       "      <td>[In, the, story, the, setting, affected, the, ...</td>\n",
       "      <td>[In the story, the setting affected the cyclis...</td>\n",
       "      <td>[in, the, stori, the, set, affect, the, cyclis...</td>\n",
       "      <td>[stori, set, affect, cyclist, mani, way, examp...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START]], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>7705</td>\n",
       "      <td>3</td>\n",
       "      <td>The features of the setting affect the cyclist...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the features of the setting affect the cyclist...</td>\n",
       "      <td>[The, features, of, the, setting, affect, the,...</td>\n",
       "      <td>[The features of the setting affect the cyclis...</td>\n",
       "      <td>[the, featur, of, the, set, affect, the, cycli...</td>\n",
       "      <td>[featur, set, affect, cyclist, like, goup, hil...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>7706</td>\n",
       "      <td>3</td>\n",
       "      <td>The setting greatly affects the cyclist trying...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the setting greatly affects the cyclist trying...</td>\n",
       "      <td>[The, setting, greatly, affects, the, cyclist,...</td>\n",
       "      <td>[The setting greatly affects the cyclist tryin...</td>\n",
       "      <td>[the, set, great, affect, the, cyclist, tri, t...</td>\n",
       "      <td>[set, great, affect, cyclist, tri, get, yosemi...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START]], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>7707</td>\n",
       "      <td>3</td>\n",
       "      <td>The features of the setting affected the cycli...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the features of the setting affected the cycli...</td>\n",
       "      <td>[The, features, of, the, setting, affected, th...</td>\n",
       "      <td>[The features of the setting affected the cycl...</td>\n",
       "      <td>[the, featur, of, the, set, affect, the, cycli...</td>\n",
       "      <td>[featur, set, affect, cyclist, author, say, ca...</td>\n",
       "      <td>[[[typographical, COMMA_COMPOUND_SENTENCE]], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>7708</td>\n",
       "      <td>3</td>\n",
       "      <td>The features of the setting in “Rough Road Ahe...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the features of the setting in “rough road ahe...</td>\n",
       "      <td>[The, features, of, the, setting, in, “, Rough...</td>\n",
       "      <td>[The features of the setting in “Rough Road Ah...</td>\n",
       "      <td>[the, featur, of, the, set, in, “, rough, road...</td>\n",
       "      <td>[featur, set, “, rough, road, ahead, exceed, p...</td>\n",
       "      <td>[[[whitespace, COMMA_PARENTHESIS_WHITESPACE]],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1726 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id  essay_set                                              essay  \\\n",
       "0         5978          3  The features of the setting affect the cyclist...   \n",
       "1         5979          3  The features of the setting affected the cycli...   \n",
       "2         5980          3  Everyone travels to unfamiliar places. Sometim...   \n",
       "3         5981          3  I believe the features of the cyclist affected...   \n",
       "4         5982          3  The setting effects the cyclist because of the...   \n",
       "...        ...        ...                                                ...   \n",
       "1721      7704          3  In the story, the setting affected the cyclist...   \n",
       "1722      7705          3  The features of the setting affect the cyclist...   \n",
       "1723      7706          3  The setting greatly affects the cyclist trying...   \n",
       "1724      7707          3  The features of the setting affected the cycli...   \n",
       "1725      7708          3  The features of the setting in “Rough Road Ahe...   \n",
       "\n",
       "      rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0                1.0             1.0             NaN            1.0   \n",
       "1                2.0             2.0             NaN            2.0   \n",
       "2                1.0             1.0             NaN            1.0   \n",
       "3                1.0             1.0             NaN            1.0   \n",
       "4                2.0             2.0             NaN            2.0   \n",
       "...              ...             ...             ...            ...   \n",
       "1721             2.0             2.0             NaN            2.0   \n",
       "1722             1.0             1.0             NaN            1.0   \n",
       "1723             1.0             2.0             NaN            2.0   \n",
       "1724             2.0             2.0             NaN            2.0   \n",
       "1725             2.0             3.0             NaN            3.0   \n",
       "\n",
       "      rater1_domain2  rater2_domain2  domain2_score  ...  rater3_trait3  \\\n",
       "0                NaN             NaN            NaN  ...            NaN   \n",
       "1                NaN             NaN            NaN  ...            NaN   \n",
       "2                NaN             NaN            NaN  ...            NaN   \n",
       "3                NaN             NaN            NaN  ...            NaN   \n",
       "4                NaN             NaN            NaN  ...            NaN   \n",
       "...              ...             ...            ...  ...            ...   \n",
       "1721             NaN             NaN            NaN  ...            NaN   \n",
       "1722             NaN             NaN            NaN  ...            NaN   \n",
       "1723             NaN             NaN            NaN  ...            NaN   \n",
       "1724             NaN             NaN            NaN  ...            NaN   \n",
       "1725             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "      rater3_trait4  rater3_trait5  rater3_trait6  \\\n",
       "0               NaN            NaN            NaN   \n",
       "1               NaN            NaN            NaN   \n",
       "2               NaN            NaN            NaN   \n",
       "3               NaN            NaN            NaN   \n",
       "4               NaN            NaN            NaN   \n",
       "...             ...            ...            ...   \n",
       "1721            NaN            NaN            NaN   \n",
       "1722            NaN            NaN            NaN   \n",
       "1723            NaN            NaN            NaN   \n",
       "1724            NaN            NaN            NaN   \n",
       "1725            NaN            NaN            NaN   \n",
       "\n",
       "                                        normalised_docs  \\\n",
       "0     the features of the setting affect the cyclist...   \n",
       "1     the features of the setting affected the cycli...   \n",
       "2     everyone travels to unfamiliar places. sometim...   \n",
       "3     i believe the features of the cyclist affected...   \n",
       "4     the setting effects the cyclist because of the...   \n",
       "...                                                 ...   \n",
       "1721  in the story, the setting affected the cyclist...   \n",
       "1722  the features of the setting affect the cyclist...   \n",
       "1723  the setting greatly affects the cyclist trying...   \n",
       "1724  the features of the setting affected the cycli...   \n",
       "1725  the features of the setting in “rough road ahe...   \n",
       "\n",
       "                                               word_tok  \\\n",
       "0     [The, features, of, the, setting, affect, the,...   \n",
       "1     [The, features, of, the, setting, affected, th...   \n",
       "2     [Everyone, travels, to, unfamiliar, places, So...   \n",
       "3     [I, believe, the, features, of, the, cyclist, ...   \n",
       "4     [The, setting, effects, the, cyclist, because,...   \n",
       "...                                                 ...   \n",
       "1721  [In, the, story, the, setting, affected, the, ...   \n",
       "1722  [The, features, of, the, setting, affect, the,...   \n",
       "1723  [The, setting, greatly, affects, the, cyclist,...   \n",
       "1724  [The, features, of, the, setting, affected, th...   \n",
       "1725  [The, features, of, the, setting, in, “, Rough...   \n",
       "\n",
       "                                           sentence_tok  \\\n",
       "0     [The features of the setting affect the cyclis...   \n",
       "1     [The features of the setting affected the cycl...   \n",
       "2     [Everyone travels to unfamiliar places., Somet...   \n",
       "3     [I believe the features of the cyclist affecte...   \n",
       "4     [The setting effects the cyclist because of th...   \n",
       "...                                                 ...   \n",
       "1721  [In the story, the setting affected the cyclis...   \n",
       "1722  [The features of the setting affect the cyclis...   \n",
       "1723  [The setting greatly affects the cyclist tryin...   \n",
       "1724  [The features of the setting affected the cycl...   \n",
       "1725  [The features of the setting in “Rough Road Ah...   \n",
       "\n",
       "                                     stemmed_word_token  \\\n",
       "0     [the, featur, of, the, set, affect, the, cycli...   \n",
       "1     [the, featur, of, the, set, affect, the, cycli...   \n",
       "2     [everyon, travel, to, unfamiliar, place, somet...   \n",
       "3     [i, believ, the, featur, of, the, cyclist, aff...   \n",
       "4     [the, set, effect, the, cyclist, becaus, of, t...   \n",
       "...                                                 ...   \n",
       "1721  [in, the, stori, the, set, affect, the, cyclis...   \n",
       "1722  [the, featur, of, the, set, affect, the, cycli...   \n",
       "1723  [the, set, great, affect, the, cyclist, tri, t...   \n",
       "1724  [the, featur, of, the, set, affect, the, cycli...   \n",
       "1725  [the, featur, of, the, set, in, “, rough, road...   \n",
       "\n",
       "                                   stemmed_no_stopwords  \\\n",
       "0     [featur, set, affect, cyclist, mani, way, feat...   \n",
       "1     [featur, set, affect, cyclist, negat, way, des...   \n",
       "2     [everyon, travel, unfamiliar, place, sometim, ...   \n",
       "3     [believ, featur, cyclist, affect, becaus, impa...   \n",
       "4     [set, effect, cyclist, becaus, set, diffrent, ...   \n",
       "...                                                 ...   \n",
       "1721  [stori, set, affect, cyclist, mani, way, examp...   \n",
       "1722  [featur, set, affect, cyclist, like, goup, hil...   \n",
       "1723  [set, great, affect, cyclist, tri, get, yosemi...   \n",
       "1724  [featur, set, affect, cyclist, author, say, ca...   \n",
       "1725  [featur, set, “, rough, road, ahead, exceed, p...   \n",
       "\n",
       "                                            error_types  \n",
       "0     [[[misspelling, MORFOLOGIK_RULE_EN_US], [uncat...  \n",
       "1     [[[style, IN_A_X_MANNER]], [[typographical, UP...  \n",
       "2     [[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA]]...  \n",
       "3     [[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA],...  \n",
       "4     [[[non-conformance, CONFUSION_RULE_EFFECTS_AFF...  \n",
       "...                                                 ...  \n",
       "1721  [[[typographical, UPPERCASE_SENTENCE_START]], ...  \n",
       "1722  [[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA],...  \n",
       "1723  [[[typographical, UPPERCASE_SENTENCE_START]], ...  \n",
       "1724  [[[typographical, COMMA_COMPOUND_SENTENCE]], [...  \n",
       "1725  [[[whitespace, COMMA_PARENTHESIS_WHITESPACE]],...  \n",
       "\n",
       "[1726 rows x 34 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_3 = pd.read_csv(\"csv_files/preprosessed_essays_training_set_3.csv\", converters={\"remove_single_char\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), \n",
    "                                                                             \"sentence_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), \n",
    "                                                                             \"stemmed_word_token\":lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "                                                                             \"stemmed_no_stopwords\":lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "                                                                             \"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "training_data_3 = training_data_3.set_index(\"essay_id\").join(training_data_3_errors_detected.set_index(\"essay_id\").loc[:, \"error_types\"]).reset_index().copy()\n",
    "training_data_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>normalised_docs</th>\n",
       "      <th>word_tok</th>\n",
       "      <th>sentence_tok</th>\n",
       "      <th>stemmed_word_token</th>\n",
       "      <th>stemmed_no_stopwords</th>\n",
       "      <th>error_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8863</td>\n",
       "      <td>4</td>\n",
       "      <td>The author concludes the story with this becau...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the author concludes the story with this becau...</td>\n",
       "      <td>[The, author, concludes, the, story, with, thi...</td>\n",
       "      <td>[The author concludes the story with this beca...</td>\n",
       "      <td>[the, author, conclud, the, stori, with, this,...</td>\n",
       "      <td>[author, conclud, stori, becaus, garden, grow,...</td>\n",
       "      <td>[[[duplication, ENGLISH_WORD_REPEAT_RULE], [mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8864</td>\n",
       "      <td>4</td>\n",
       "      <td>The narrater has that in with Paragraph becuse...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the narrater has that in with paragraph becuse...</td>\n",
       "      <td>[The, narrater, has, that, in, with, Paragraph...</td>\n",
       "      <td>[\"The narrater has that in with Paragraph becu...</td>\n",
       "      <td>[the, narrat, has, that, in, with, paragraph, ...</td>\n",
       "      <td>[narrat, paragraph, becus, make, senc, goss, a...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US], [missp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8865</td>\n",
       "      <td>4</td>\n",
       "      <td>The author concludes the story with that passa...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the author concludes the story with that passa...</td>\n",
       "      <td>[The, author, concludes, the, story, with, tha...</td>\n",
       "      <td>[The author concludes the story with that pass...</td>\n",
       "      <td>[the, author, conclud, the, stori, with, that,...</td>\n",
       "      <td>[author, conclud, stori, passag, show, import,...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US]], [[mis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8866</td>\n",
       "      <td>4</td>\n",
       "      <td>The author ended the story with this paragraph...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the author ended the story with this paragraph...</td>\n",
       "      <td>[The, author, ended, the, story, with, this, p...</td>\n",
       "      <td>[The author ended the story with this paragrap...</td>\n",
       "      <td>[the, author, end, the, stori, with, this, par...</td>\n",
       "      <td>[author, end, stori, paragraph, becaus, show, ...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA]]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8867</td>\n",
       "      <td>4</td>\n",
       "      <td>The author concludes the story with this parag...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the author concludes the story with this parag...</td>\n",
       "      <td>[The, author, concludes, the, story, with, thi...</td>\n",
       "      <td>[The author concludes the story with this para...</td>\n",
       "      <td>[the, author, conclud, the, stori, with, this,...</td>\n",
       "      <td>[author, conclud, stori, paragraph, “, winter,...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START]], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>10638</td>\n",
       "      <td>4</td>\n",
       "      <td>To me it seam like the whoever was saying that...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to me it seam like the whoever was saying that...</td>\n",
       "      <td>[To, me, it, seam, like, the, whoever, was, sa...</td>\n",
       "      <td>[To me it seam like the whoever was saying tha...</td>\n",
       "      <td>[to, me, it, seam, like, the, whoever, was, sa...</td>\n",
       "      <td>[seam, like, whoever, say, must, go, lot, thin...</td>\n",
       "      <td>[[[typographical, PRP_COMMA], [grammar, IT_VBZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>10639</td>\n",
       "      <td>4</td>\n",
       "      <td>The author concludes the story with this becau...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the author concludes the story with this becau...</td>\n",
       "      <td>[The, author, concludes, the, story, with, thi...</td>\n",
       "      <td>[The author concludes the story with this beca...</td>\n",
       "      <td>[the, author, conclud, the, stori, with, this,...</td>\n",
       "      <td>[author, conclud, stori, becaus, show, still, ...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US]], [[unc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>10640</td>\n",
       "      <td>4</td>\n",
       "      <td>The author uses this conclusion for a reason. ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the author uses this conclusion for a reason. ...</td>\n",
       "      <td>[The, author, uses, this, conclusion, for, a, ...</td>\n",
       "      <td>[The author uses this conclusion for a reason....</td>\n",
       "      <td>[the, author, use, this, conclus, for, a, reas...</td>\n",
       "      <td>[author, use, conclus, reason, reason, author,...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US]], [[typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>10641</td>\n",
       "      <td>4</td>\n",
       "      <td>The author concludes the story with this parag...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the author concludes the story with this parag...</td>\n",
       "      <td>[The, author, concludes, the, story, with, thi...</td>\n",
       "      <td>[The author concludes the story with this para...</td>\n",
       "      <td>[the, author, conclud, the, stori, with, this,...</td>\n",
       "      <td>[author, conclud, stori, paragraph, becaus, pr...</td>\n",
       "      <td>[[[misspelling, EN_CONTRACTION_SPELLING]], [[t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>10642</td>\n",
       "      <td>4</td>\n",
       "      <td>There was a specific reason as to why the auth...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there was a specific reason as to why the auth...</td>\n",
       "      <td>[There, was, a, specific, reason, as, to, why,...</td>\n",
       "      <td>[There was a specific reason as to why the aut...</td>\n",
       "      <td>[there, was, a, specif, reason, as, to, whi, t...</td>\n",
       "      <td>[specif, reason, whi, author, conclud, stori, ...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US]], [[mis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1772 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id  essay_set                                              essay  \\\n",
       "0         8863          4  The author concludes the story with this becau...   \n",
       "1         8864          4  The narrater has that in with Paragraph becuse...   \n",
       "2         8865          4  The author concludes the story with that passa...   \n",
       "3         8866          4  The author ended the story with this paragraph...   \n",
       "4         8867          4  The author concludes the story with this parag...   \n",
       "...        ...        ...                                                ...   \n",
       "1767     10638          4  To me it seam like the whoever was saying that...   \n",
       "1768     10639          4  The author concludes the story with this becau...   \n",
       "1769     10640          4  The author uses this conclusion for a reason. ...   \n",
       "1770     10641          4  The author concludes the story with this parag...   \n",
       "1771     10642          4  There was a specific reason as to why the auth...   \n",
       "\n",
       "      rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0                0.0             0.0             NaN            0.0   \n",
       "1                0.0             0.0             NaN            0.0   \n",
       "2                3.0             2.0             NaN            3.0   \n",
       "3                1.0             2.0             NaN            2.0   \n",
       "4                2.0             2.0             NaN            2.0   \n",
       "...              ...             ...             ...            ...   \n",
       "1767             0.0             0.0             NaN            0.0   \n",
       "1768             2.0             2.0             NaN            2.0   \n",
       "1769             1.0             1.0             NaN            1.0   \n",
       "1770             0.0             0.0             NaN            0.0   \n",
       "1771             1.0             2.0             NaN            2.0   \n",
       "\n",
       "      rater1_domain2  rater2_domain2  domain2_score  ...  rater3_trait3  \\\n",
       "0                NaN             NaN            NaN  ...            NaN   \n",
       "1                NaN             NaN            NaN  ...            NaN   \n",
       "2                NaN             NaN            NaN  ...            NaN   \n",
       "3                NaN             NaN            NaN  ...            NaN   \n",
       "4                NaN             NaN            NaN  ...            NaN   \n",
       "...              ...             ...            ...  ...            ...   \n",
       "1767             NaN             NaN            NaN  ...            NaN   \n",
       "1768             NaN             NaN            NaN  ...            NaN   \n",
       "1769             NaN             NaN            NaN  ...            NaN   \n",
       "1770             NaN             NaN            NaN  ...            NaN   \n",
       "1771             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "      rater3_trait4  rater3_trait5  rater3_trait6  \\\n",
       "0               NaN            NaN            NaN   \n",
       "1               NaN            NaN            NaN   \n",
       "2               NaN            NaN            NaN   \n",
       "3               NaN            NaN            NaN   \n",
       "4               NaN            NaN            NaN   \n",
       "...             ...            ...            ...   \n",
       "1767            NaN            NaN            NaN   \n",
       "1768            NaN            NaN            NaN   \n",
       "1769            NaN            NaN            NaN   \n",
       "1770            NaN            NaN            NaN   \n",
       "1771            NaN            NaN            NaN   \n",
       "\n",
       "                                        normalised_docs  \\\n",
       "0     the author concludes the story with this becau...   \n",
       "1     the narrater has that in with paragraph becuse...   \n",
       "2     the author concludes the story with that passa...   \n",
       "3     the author ended the story with this paragraph...   \n",
       "4     the author concludes the story with this parag...   \n",
       "...                                                 ...   \n",
       "1767  to me it seam like the whoever was saying that...   \n",
       "1768  the author concludes the story with this becau...   \n",
       "1769  the author uses this conclusion for a reason. ...   \n",
       "1770  the author concludes the story with this parag...   \n",
       "1771  there was a specific reason as to why the auth...   \n",
       "\n",
       "                                               word_tok  \\\n",
       "0     [The, author, concludes, the, story, with, thi...   \n",
       "1     [The, narrater, has, that, in, with, Paragraph...   \n",
       "2     [The, author, concludes, the, story, with, tha...   \n",
       "3     [The, author, ended, the, story, with, this, p...   \n",
       "4     [The, author, concludes, the, story, with, thi...   \n",
       "...                                                 ...   \n",
       "1767  [To, me, it, seam, like, the, whoever, was, sa...   \n",
       "1768  [The, author, concludes, the, story, with, thi...   \n",
       "1769  [The, author, uses, this, conclusion, for, a, ...   \n",
       "1770  [The, author, concludes, the, story, with, thi...   \n",
       "1771  [There, was, a, specific, reason, as, to, why,...   \n",
       "\n",
       "                                           sentence_tok  \\\n",
       "0     [The author concludes the story with this beca...   \n",
       "1     [\"The narrater has that in with Paragraph becu...   \n",
       "2     [The author concludes the story with that pass...   \n",
       "3     [The author ended the story with this paragrap...   \n",
       "4     [The author concludes the story with this para...   \n",
       "...                                                 ...   \n",
       "1767  [To me it seam like the whoever was saying tha...   \n",
       "1768  [The author concludes the story with this beca...   \n",
       "1769  [The author uses this conclusion for a reason....   \n",
       "1770  [The author concludes the story with this para...   \n",
       "1771  [There was a specific reason as to why the aut...   \n",
       "\n",
       "                                     stemmed_word_token  \\\n",
       "0     [the, author, conclud, the, stori, with, this,...   \n",
       "1     [the, narrat, has, that, in, with, paragraph, ...   \n",
       "2     [the, author, conclud, the, stori, with, that,...   \n",
       "3     [the, author, end, the, stori, with, this, par...   \n",
       "4     [the, author, conclud, the, stori, with, this,...   \n",
       "...                                                 ...   \n",
       "1767  [to, me, it, seam, like, the, whoever, was, sa...   \n",
       "1768  [the, author, conclud, the, stori, with, this,...   \n",
       "1769  [the, author, use, this, conclus, for, a, reas...   \n",
       "1770  [the, author, conclud, the, stori, with, this,...   \n",
       "1771  [there, was, a, specif, reason, as, to, whi, t...   \n",
       "\n",
       "                                   stemmed_no_stopwords  \\\n",
       "0     [author, conclud, stori, becaus, garden, grow,...   \n",
       "1     [narrat, paragraph, becus, make, senc, goss, a...   \n",
       "2     [author, conclud, stori, passag, show, import,...   \n",
       "3     [author, end, stori, paragraph, becaus, show, ...   \n",
       "4     [author, conclud, stori, paragraph, “, winter,...   \n",
       "...                                                 ...   \n",
       "1767  [seam, like, whoever, say, must, go, lot, thin...   \n",
       "1768  [author, conclud, stori, becaus, show, still, ...   \n",
       "1769  [author, use, conclus, reason, reason, author,...   \n",
       "1770  [author, conclud, stori, paragraph, becaus, pr...   \n",
       "1771  [specif, reason, whi, author, conclud, stori, ...   \n",
       "\n",
       "                                            error_types  \n",
       "0     [[[duplication, ENGLISH_WORD_REPEAT_RULE], [mi...  \n",
       "1     [[[misspelling, MORFOLOGIK_RULE_EN_US], [missp...  \n",
       "2     [[[misspelling, MORFOLOGIK_RULE_EN_US]], [[mis...  \n",
       "3     [[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA]]...  \n",
       "4     [[[typographical, UPPERCASE_SENTENCE_START]], ...  \n",
       "...                                                 ...  \n",
       "1767  [[[typographical, PRP_COMMA], [grammar, IT_VBZ...  \n",
       "1768  [[[misspelling, MORFOLOGIK_RULE_EN_US]], [[unc...  \n",
       "1769  [[[misspelling, MORFOLOGIK_RULE_EN_US]], [[typ...  \n",
       "1770  [[[misspelling, EN_CONTRACTION_SPELLING]], [[t...  \n",
       "1771  [[[misspelling, MORFOLOGIK_RULE_EN_US]], [[mis...  \n",
       "\n",
       "[1772 rows x 34 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_4 = pd.read_csv(\"csv_files/preprosessed_essays_training_set_4.csv\", converters={\"remove_single_char\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), \n",
    "                                                                             \"sentence_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), \n",
    "                                                                             \"stemmed_word_token\":lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "                                                                             \"stemmed_no_stopwords\":lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "                                                                             \"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "training_data_4 = training_data_4.set_index(\"essay_id\").join(training_data_4_errors_detected.set_index(\"essay_id\").loc[:, \"error_types\"]).reset_index().copy()\n",
    "training_data_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>normalised_docs</th>\n",
       "      <th>word_tok</th>\n",
       "      <th>sentence_tok</th>\n",
       "      <th>stemmed_word_token</th>\n",
       "      <th>stemmed_no_stopwords</th>\n",
       "      <th>error_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11827</td>\n",
       "      <td>5</td>\n",
       "      <td>In this memoir of Narciso Rodriguez, @PERSON3'...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in this memoir of narciso rodriguez, @person3'...</td>\n",
       "      <td>[In, this, memoir, of, Narciso, Rodriguez, PER...</td>\n",
       "      <td>[\"In this memoir of Narciso Rodriguez, @PERSON...</td>\n",
       "      <td>[in, this, memoir, of, narciso, rodriguez, per...</td>\n",
       "      <td>[memoir, narciso, rodriguez, person3, life, mo...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US]], [[typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11828</td>\n",
       "      <td>5</td>\n",
       "      <td>Throughout the excerpt from Home the Blueprint...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>throughout the excerpt from home the blueprint...</td>\n",
       "      <td>[Throughout, the, excerpt, from, Home, the, Bl...</td>\n",
       "      <td>[Throughout the excerpt from Home the Blueprin...</td>\n",
       "      <td>[throughout, the, excerpt, from, home, the, bl...</td>\n",
       "      <td>[throughout, excerpt, home, blueprint, live, n...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA]]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11829</td>\n",
       "      <td>5</td>\n",
       "      <td>The mood the author created in the memoir is l...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the mood the author created in the memoir is l...</td>\n",
       "      <td>[The, mood, the, author, created, in, the, mem...</td>\n",
       "      <td>[The mood the author created in the memoir is ...</td>\n",
       "      <td>[the, mood, the, author, creat, in, the, memoi...</td>\n",
       "      <td>[mood, author, creat, memoir, love, author, fi...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11830</td>\n",
       "      <td>5</td>\n",
       "      <td>The mood created by the author is showing how ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the mood created by the author is showing how ...</td>\n",
       "      <td>[The, mood, created, by, the, author, is, show...</td>\n",
       "      <td>[\"The mood created by the author is showing ho...</td>\n",
       "      <td>[the, mood, creat, by, the, author, is, show, ...</td>\n",
       "      <td>[mood, creat, author, show, cuban, live, cultr...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US], [missp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11831</td>\n",
       "      <td>5</td>\n",
       "      <td>The mood created in the memoir is happiness an...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the mood created in the memoir is happiness an...</td>\n",
       "      <td>[The, mood, created, in, the, memoir, is, happ...</td>\n",
       "      <td>[The mood created in the memoir is happiness a...</td>\n",
       "      <td>[the, mood, creat, in, the, memoir, is, happi,...</td>\n",
       "      <td>[mood, creat, memoir, happi, gratitud, narciso...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US]], [[typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>13627</td>\n",
       "      <td>5</td>\n",
       "      <td>The mood of this memoir is nonfiction. The moo...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the mood of this memoir is nonfiction. the moo...</td>\n",
       "      <td>[The, mood, of, this, memoir, is, nonfiction, ...</td>\n",
       "      <td>[The mood of this memoir is nonfiction., The m...</td>\n",
       "      <td>[the, mood, of, this, memoir, is, nonfict, the...</td>\n",
       "      <td>[mood, memoir, nonfict, mood, becaus, narciso,...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US]], [[typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>13628</td>\n",
       "      <td>5</td>\n",
       "      <td>The mood was created by the author in the memo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the mood was created by the author in the memo...</td>\n",
       "      <td>[The, mood, was, created, by, the, author, in,...</td>\n",
       "      <td>[The mood was created by the author in the mem...</td>\n",
       "      <td>[the, mood, was, creat, by, the, author, in, t...</td>\n",
       "      <td>[mood, creat, author, memoir, specic, imformat...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US], [missp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>13629</td>\n",
       "      <td>5</td>\n",
       "      <td>In the memoir \"Narciso Rodriguez\", the mood cr...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in the memoir \"narciso rodriguez\", the mood cr...</td>\n",
       "      <td>[In, the, memoir, Narciso, Rodriguez, the, moo...</td>\n",
       "      <td>[In the memoir \"Narciso Rodriguez\", the mood c...</td>\n",
       "      <td>[in, the, memoir, narciso, rodriguez, the, moo...</td>\n",
       "      <td>[memoir, narciso, rodriguez, mood, creat, auth...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US]], [[typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>13630</td>\n",
       "      <td>5</td>\n",
       "      <td>The mood created @CAPS3 the author, Narciso Ro...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the mood created @caps3 the author, narciso ro...</td>\n",
       "      <td>[The, mood, created, CAPS3, the, author, Narci...</td>\n",
       "      <td>[The mood created @CAPS3 the author, Narciso R...</td>\n",
       "      <td>[the, mood, creat, caps3, the, author, narciso...</td>\n",
       "      <td>[mood, creat, caps3, author, narciso, rodrigue...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US], [missp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>13631</td>\n",
       "      <td>5</td>\n",
       "      <td>The author created such a specific mood for th...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the author created such a specific mood for th...</td>\n",
       "      <td>[The, author, created, such, a, specific, mood...</td>\n",
       "      <td>[The author created such a specific mood for t...</td>\n",
       "      <td>[the, author, creat, such, a, specif, mood, fo...</td>\n",
       "      <td>[author, creat, specif, mood, memoir, caps1, t...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US], [uncat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1805 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id  essay_set                                              essay  \\\n",
       "0        11827          5  In this memoir of Narciso Rodriguez, @PERSON3'...   \n",
       "1        11828          5  Throughout the excerpt from Home the Blueprint...   \n",
       "2        11829          5  The mood the author created in the memoir is l...   \n",
       "3        11830          5  The mood created by the author is showing how ...   \n",
       "4        11831          5  The mood created in the memoir is happiness an...   \n",
       "...        ...        ...                                                ...   \n",
       "1800     13627          5  The mood of this memoir is nonfiction. The moo...   \n",
       "1801     13628          5  The mood was created by the author in the memo...   \n",
       "1802     13629          5  In the memoir \"Narciso Rodriguez\", the mood cr...   \n",
       "1803     13630          5  The mood created @CAPS3 the author, Narciso Ro...   \n",
       "1804     13631          5  The author created such a specific mood for th...   \n",
       "\n",
       "      rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0                2.0             2.0             NaN            2.0   \n",
       "1                2.0             2.0             NaN            2.0   \n",
       "2                3.0             3.0             NaN            3.0   \n",
       "3                1.0             0.0             NaN            1.0   \n",
       "4                2.0             3.0             NaN            3.0   \n",
       "...              ...             ...             ...            ...   \n",
       "1800             2.0             1.0             NaN            2.0   \n",
       "1801             0.0             0.0             NaN            0.0   \n",
       "1802             3.0             4.0             NaN            4.0   \n",
       "1803             3.0             2.0             NaN            3.0   \n",
       "1804             2.0             2.0             NaN            2.0   \n",
       "\n",
       "      rater1_domain2  rater2_domain2  domain2_score  ...  rater3_trait3  \\\n",
       "0                NaN             NaN            NaN  ...            NaN   \n",
       "1                NaN             NaN            NaN  ...            NaN   \n",
       "2                NaN             NaN            NaN  ...            NaN   \n",
       "3                NaN             NaN            NaN  ...            NaN   \n",
       "4                NaN             NaN            NaN  ...            NaN   \n",
       "...              ...             ...            ...  ...            ...   \n",
       "1800             NaN             NaN            NaN  ...            NaN   \n",
       "1801             NaN             NaN            NaN  ...            NaN   \n",
       "1802             NaN             NaN            NaN  ...            NaN   \n",
       "1803             NaN             NaN            NaN  ...            NaN   \n",
       "1804             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "      rater3_trait4  rater3_trait5  rater3_trait6  \\\n",
       "0               NaN            NaN            NaN   \n",
       "1               NaN            NaN            NaN   \n",
       "2               NaN            NaN            NaN   \n",
       "3               NaN            NaN            NaN   \n",
       "4               NaN            NaN            NaN   \n",
       "...             ...            ...            ...   \n",
       "1800            NaN            NaN            NaN   \n",
       "1801            NaN            NaN            NaN   \n",
       "1802            NaN            NaN            NaN   \n",
       "1803            NaN            NaN            NaN   \n",
       "1804            NaN            NaN            NaN   \n",
       "\n",
       "                                        normalised_docs  \\\n",
       "0     in this memoir of narciso rodriguez, @person3'...   \n",
       "1     throughout the excerpt from home the blueprint...   \n",
       "2     the mood the author created in the memoir is l...   \n",
       "3     the mood created by the author is showing how ...   \n",
       "4     the mood created in the memoir is happiness an...   \n",
       "...                                                 ...   \n",
       "1800  the mood of this memoir is nonfiction. the moo...   \n",
       "1801  the mood was created by the author in the memo...   \n",
       "1802  in the memoir \"narciso rodriguez\", the mood cr...   \n",
       "1803  the mood created @caps3 the author, narciso ro...   \n",
       "1804  the author created such a specific mood for th...   \n",
       "\n",
       "                                               word_tok  \\\n",
       "0     [In, this, memoir, of, Narciso, Rodriguez, PER...   \n",
       "1     [Throughout, the, excerpt, from, Home, the, Bl...   \n",
       "2     [The, mood, the, author, created, in, the, mem...   \n",
       "3     [The, mood, created, by, the, author, is, show...   \n",
       "4     [The, mood, created, in, the, memoir, is, happ...   \n",
       "...                                                 ...   \n",
       "1800  [The, mood, of, this, memoir, is, nonfiction, ...   \n",
       "1801  [The, mood, was, created, by, the, author, in,...   \n",
       "1802  [In, the, memoir, Narciso, Rodriguez, the, moo...   \n",
       "1803  [The, mood, created, CAPS3, the, author, Narci...   \n",
       "1804  [The, author, created, such, a, specific, mood...   \n",
       "\n",
       "                                           sentence_tok  \\\n",
       "0     [\"In this memoir of Narciso Rodriguez, @PERSON...   \n",
       "1     [Throughout the excerpt from Home the Blueprin...   \n",
       "2     [The mood the author created in the memoir is ...   \n",
       "3     [\"The mood created by the author is showing ho...   \n",
       "4     [The mood created in the memoir is happiness a...   \n",
       "...                                                 ...   \n",
       "1800  [The mood of this memoir is nonfiction., The m...   \n",
       "1801  [The mood was created by the author in the mem...   \n",
       "1802  [In the memoir \"Narciso Rodriguez\", the mood c...   \n",
       "1803  [The mood created @CAPS3 the author, Narciso R...   \n",
       "1804  [The author created such a specific mood for t...   \n",
       "\n",
       "                                     stemmed_word_token  \\\n",
       "0     [in, this, memoir, of, narciso, rodriguez, per...   \n",
       "1     [throughout, the, excerpt, from, home, the, bl...   \n",
       "2     [the, mood, the, author, creat, in, the, memoi...   \n",
       "3     [the, mood, creat, by, the, author, is, show, ...   \n",
       "4     [the, mood, creat, in, the, memoir, is, happi,...   \n",
       "...                                                 ...   \n",
       "1800  [the, mood, of, this, memoir, is, nonfict, the...   \n",
       "1801  [the, mood, was, creat, by, the, author, in, t...   \n",
       "1802  [in, the, memoir, narciso, rodriguez, the, moo...   \n",
       "1803  [the, mood, creat, caps3, the, author, narciso...   \n",
       "1804  [the, author, creat, such, a, specif, mood, fo...   \n",
       "\n",
       "                                   stemmed_no_stopwords  \\\n",
       "0     [memoir, narciso, rodriguez, person3, life, mo...   \n",
       "1     [throughout, excerpt, home, blueprint, live, n...   \n",
       "2     [mood, author, creat, memoir, love, author, fi...   \n",
       "3     [mood, creat, author, show, cuban, live, cultr...   \n",
       "4     [mood, creat, memoir, happi, gratitud, narciso...   \n",
       "...                                                 ...   \n",
       "1800  [mood, memoir, nonfict, mood, becaus, narciso,...   \n",
       "1801  [mood, creat, author, memoir, specic, imformat...   \n",
       "1802  [memoir, narciso, rodriguez, mood, creat, auth...   \n",
       "1803  [mood, creat, caps3, author, narciso, rodrigue...   \n",
       "1804  [author, creat, specif, mood, memoir, caps1, t...   \n",
       "\n",
       "                                            error_types  \n",
       "0     [[[misspelling, MORFOLOGIK_RULE_EN_US]], [[typ...  \n",
       "1     [[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA]]...  \n",
       "2     [[[typographical, UPPERCASE_SENTENCE_START], [...  \n",
       "3     [[[misspelling, MORFOLOGIK_RULE_EN_US], [missp...  \n",
       "4     [[[misspelling, MORFOLOGIK_RULE_EN_US]], [[typ...  \n",
       "...                                                 ...  \n",
       "1800  [[[misspelling, MORFOLOGIK_RULE_EN_US]], [[typ...  \n",
       "1801  [[[misspelling, MORFOLOGIK_RULE_EN_US], [missp...  \n",
       "1802  [[[misspelling, MORFOLOGIK_RULE_EN_US]], [[typ...  \n",
       "1803  [[[misspelling, MORFOLOGIK_RULE_EN_US], [missp...  \n",
       "1804  [[[misspelling, MORFOLOGIK_RULE_EN_US], [uncat...  \n",
       "\n",
       "[1805 rows x 34 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_5 = pd.read_csv(\"csv_files/preprosessed_essays_training_set_5.csv\", converters={\"remove_single_char\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), \n",
    "                                                                             \"sentence_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), \n",
    "                                                                             \"stemmed_word_token\":lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "                                                                             \"stemmed_no_stopwords\":lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "                                                                             \"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "training_data_5 = training_data_5.set_index(\"essay_id\").join(training_data_5_errors_detected.set_index(\"essay_id\").loc[:, \"error_types\"]).reset_index().copy()\n",
    "training_data_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>normalised_docs</th>\n",
       "      <th>word_tok</th>\n",
       "      <th>sentence_tok</th>\n",
       "      <th>stemmed_word_token</th>\n",
       "      <th>stemmed_no_stopwords</th>\n",
       "      <th>error_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14834</td>\n",
       "      <td>6</td>\n",
       "      <td>There were many obstacles that the builders fa...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there were many obstacles that the builders fa...</td>\n",
       "      <td>[There, were, many, obstacles, that, the, buil...</td>\n",
       "      <td>[There were many obstacles that the builders f...</td>\n",
       "      <td>[there, were, mani, obstacl, that, the, builde...</td>\n",
       "      <td>[mani, obstacl, builder, face, attempt, dirig,...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14835</td>\n",
       "      <td>6</td>\n",
       "      <td>Him from the start, there would have been many...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>him from the start, there would have been many...</td>\n",
       "      <td>[Him, from, the, start, there, would, have, be...</td>\n",
       "      <td>[Him from the start, there would have been man...</td>\n",
       "      <td>[him, from, the, start, there, would, have, be...</td>\n",
       "      <td>[start, would, mani, problem, allow, dirig, do...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START]], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14836</td>\n",
       "      <td>6</td>\n",
       "      <td>The builders of the Empire State Building face...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the builders of the empire state building face...</td>\n",
       "      <td>[The, builders, of, the, Empire, State, Buildi...</td>\n",
       "      <td>[The builders of the Empire State Building fac...</td>\n",
       "      <td>[the, builder, of, the, empir, state, build, f...</td>\n",
       "      <td>[builder, empir, state, build, face, mani, obs...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START]], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14837</td>\n",
       "      <td>6</td>\n",
       "      <td>In the passage The Mooring Mast by Marcia Amid...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in the passage the mooring mast by marcia amid...</td>\n",
       "      <td>[In, the, passage, The, Mooring, Mast, by, Mar...</td>\n",
       "      <td>[In the passage The Mooring Mast by Marcia Ami...</td>\n",
       "      <td>[in, the, passag, the, moor, mast, by, marcia,...</td>\n",
       "      <td>[passag, moor, mast, marcia, amidon, caps1, bu...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US]], [[mis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14838</td>\n",
       "      <td>6</td>\n",
       "      <td>The builders of the Empire State Building face...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the builders of the empire state building face...</td>\n",
       "      <td>[The, builders, of, the, Empire, State, Buildi...</td>\n",
       "      <td>[The builders of the Empire State Building fac...</td>\n",
       "      <td>[the, builder, of, the, empir, state, build, f...</td>\n",
       "      <td>[builder, empir, state, build, face, mani, obs...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA]]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>16629</td>\n",
       "      <td>6</td>\n",
       "      <td>The one obstacle the builders had when trying ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the one obstacle the builders had when trying ...</td>\n",
       "      <td>[The, one, obstacle, the, builders, had, when,...</td>\n",
       "      <td>[The one obstacle the builders had when trying...</td>\n",
       "      <td>[the, one, obstacl, the, builder, had, when, t...</td>\n",
       "      <td>[one, obstacl, builder, tri, build, build, awa...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US], [missp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>16630</td>\n",
       "      <td>6</td>\n",
       "      <td>Some of the problems with the constructing of ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>some of the problems with the constructing of ...</td>\n",
       "      <td>[Some, of, the, problems, with, the, construct...</td>\n",
       "      <td>[Some of the problems with the constructing of...</td>\n",
       "      <td>[some, of, the, problem, with, the, construct,...</td>\n",
       "      <td>[problem, construct, dock, dirig, natur, caus,...</td>\n",
       "      <td>[[[style, SOME_OF_THE]], [[typographical, UPPE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>16631</td>\n",
       "      <td>6</td>\n",
       "      <td>The builders of the Empire State building face...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the builders of the empire state building face...</td>\n",
       "      <td>[The, builders, of, the, Empire, State, buildi...</td>\n",
       "      <td>[The builders of the Empire State building fac...</td>\n",
       "      <td>[the, builder, of, the, empir, state, build, f...</td>\n",
       "      <td>[builder, empir, state, build, face, obstacl, ...</td>\n",
       "      <td>[[[misspelling, EN_SPECIFIC_CASE], [uncategori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>16632</td>\n",
       "      <td>6</td>\n",
       "      <td>The obstacles the builders of the Empire State...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the obstacles the builders of the empire state...</td>\n",
       "      <td>[The, obstacles, the, builders, of, the, Empir...</td>\n",
       "      <td>[The obstacles the builders of the Empire Stat...</td>\n",
       "      <td>[the, obstacl, the, builder, of, the, empir, s...</td>\n",
       "      <td>[obstacl, builder, empir, state, build, could,...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_MISSING_TO], [m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>16633</td>\n",
       "      <td>6</td>\n",
       "      <td>You want me to tell you what they had to go th...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>you want me to tell you what they had to go th...</td>\n",
       "      <td>[You, want, me, to, tell, you, what, they, had...</td>\n",
       "      <td>[You want me to tell you what they had to go t...</td>\n",
       "      <td>[you, want, me, to, tell, you, what, they, had...</td>\n",
       "      <td>[want, tell, go, attempt, allow, dirig, dock, ...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_REPLACE_IN_TO],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id  essay_set                                              essay  \\\n",
       "0        14834          6  There were many obstacles that the builders fa...   \n",
       "1        14835          6  Him from the start, there would have been many...   \n",
       "2        14836          6  The builders of the Empire State Building face...   \n",
       "3        14837          6  In the passage The Mooring Mast by Marcia Amid...   \n",
       "4        14838          6  The builders of the Empire State Building face...   \n",
       "...        ...        ...                                                ...   \n",
       "1795     16629          6  The one obstacle the builders had when trying ...   \n",
       "1796     16630          6  Some of the problems with the constructing of ...   \n",
       "1797     16631          6  The builders of the Empire State building face...   \n",
       "1798     16632          6  The obstacles the builders of the Empire State...   \n",
       "1799     16633          6  You want me to tell you what they had to go th...   \n",
       "\n",
       "      rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0                2.0             2.0             NaN            2.0   \n",
       "1                3.0             3.0             NaN            3.0   \n",
       "2                3.0             4.0             NaN            4.0   \n",
       "3                1.0             1.0             NaN            1.0   \n",
       "4                3.0             3.0             NaN            3.0   \n",
       "...              ...             ...             ...            ...   \n",
       "1795             2.0             0.0             NaN            0.0   \n",
       "1796             1.0             2.0             NaN            2.0   \n",
       "1797             2.0             3.0             NaN            3.0   \n",
       "1798             2.0             1.0             NaN            2.0   \n",
       "1799             2.0             2.0             NaN            2.0   \n",
       "\n",
       "      rater1_domain2  rater2_domain2  domain2_score  ...  rater3_trait3  \\\n",
       "0                NaN             NaN            NaN  ...            NaN   \n",
       "1                NaN             NaN            NaN  ...            NaN   \n",
       "2                NaN             NaN            NaN  ...            NaN   \n",
       "3                NaN             NaN            NaN  ...            NaN   \n",
       "4                NaN             NaN            NaN  ...            NaN   \n",
       "...              ...             ...            ...  ...            ...   \n",
       "1795             NaN             NaN            NaN  ...            NaN   \n",
       "1796             NaN             NaN            NaN  ...            NaN   \n",
       "1797             NaN             NaN            NaN  ...            NaN   \n",
       "1798             NaN             NaN            NaN  ...            NaN   \n",
       "1799             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "      rater3_trait4  rater3_trait5  rater3_trait6  \\\n",
       "0               NaN            NaN            NaN   \n",
       "1               NaN            NaN            NaN   \n",
       "2               NaN            NaN            NaN   \n",
       "3               NaN            NaN            NaN   \n",
       "4               NaN            NaN            NaN   \n",
       "...             ...            ...            ...   \n",
       "1795            NaN            NaN            NaN   \n",
       "1796            NaN            NaN            NaN   \n",
       "1797            NaN            NaN            NaN   \n",
       "1798            NaN            NaN            NaN   \n",
       "1799            NaN            NaN            NaN   \n",
       "\n",
       "                                        normalised_docs  \\\n",
       "0     there were many obstacles that the builders fa...   \n",
       "1     him from the start, there would have been many...   \n",
       "2     the builders of the empire state building face...   \n",
       "3     in the passage the mooring mast by marcia amid...   \n",
       "4     the builders of the empire state building face...   \n",
       "...                                                 ...   \n",
       "1795  the one obstacle the builders had when trying ...   \n",
       "1796  some of the problems with the constructing of ...   \n",
       "1797  the builders of the empire state building face...   \n",
       "1798  the obstacles the builders of the empire state...   \n",
       "1799  you want me to tell you what they had to go th...   \n",
       "\n",
       "                                               word_tok  \\\n",
       "0     [There, were, many, obstacles, that, the, buil...   \n",
       "1     [Him, from, the, start, there, would, have, be...   \n",
       "2     [The, builders, of, the, Empire, State, Buildi...   \n",
       "3     [In, the, passage, The, Mooring, Mast, by, Mar...   \n",
       "4     [The, builders, of, the, Empire, State, Buildi...   \n",
       "...                                                 ...   \n",
       "1795  [The, one, obstacle, the, builders, had, when,...   \n",
       "1796  [Some, of, the, problems, with, the, construct...   \n",
       "1797  [The, builders, of, the, Empire, State, buildi...   \n",
       "1798  [The, obstacles, the, builders, of, the, Empir...   \n",
       "1799  [You, want, me, to, tell, you, what, they, had...   \n",
       "\n",
       "                                           sentence_tok  \\\n",
       "0     [There were many obstacles that the builders f...   \n",
       "1     [Him from the start, there would have been man...   \n",
       "2     [The builders of the Empire State Building fac...   \n",
       "3     [In the passage The Mooring Mast by Marcia Ami...   \n",
       "4     [The builders of the Empire State Building fac...   \n",
       "...                                                 ...   \n",
       "1795  [The one obstacle the builders had when trying...   \n",
       "1796  [Some of the problems with the constructing of...   \n",
       "1797  [The builders of the Empire State building fac...   \n",
       "1798  [The obstacles the builders of the Empire Stat...   \n",
       "1799  [You want me to tell you what they had to go t...   \n",
       "\n",
       "                                     stemmed_word_token  \\\n",
       "0     [there, were, mani, obstacl, that, the, builde...   \n",
       "1     [him, from, the, start, there, would, have, be...   \n",
       "2     [the, builder, of, the, empir, state, build, f...   \n",
       "3     [in, the, passag, the, moor, mast, by, marcia,...   \n",
       "4     [the, builder, of, the, empir, state, build, f...   \n",
       "...                                                 ...   \n",
       "1795  [the, one, obstacl, the, builder, had, when, t...   \n",
       "1796  [some, of, the, problem, with, the, construct,...   \n",
       "1797  [the, builder, of, the, empir, state, build, f...   \n",
       "1798  [the, obstacl, the, builder, of, the, empir, s...   \n",
       "1799  [you, want, me, to, tell, you, what, they, had...   \n",
       "\n",
       "                                   stemmed_no_stopwords  \\\n",
       "0     [mani, obstacl, builder, face, attempt, dirig,...   \n",
       "1     [start, would, mani, problem, allow, dirig, do...   \n",
       "2     [builder, empir, state, build, face, mani, obs...   \n",
       "3     [passag, moor, mast, marcia, amidon, caps1, bu...   \n",
       "4     [builder, empir, state, build, face, mani, obs...   \n",
       "...                                                 ...   \n",
       "1795  [one, obstacl, builder, tri, build, build, awa...   \n",
       "1796  [problem, construct, dock, dirig, natur, caus,...   \n",
       "1797  [builder, empir, state, build, face, obstacl, ...   \n",
       "1798  [obstacl, builder, empir, state, build, could,...   \n",
       "1799  [want, tell, go, attempt, allow, dirig, dock, ...   \n",
       "\n",
       "                                            error_types  \n",
       "0     [[[typographical, UPPERCASE_SENTENCE_START], [...  \n",
       "1     [[[typographical, UPPERCASE_SENTENCE_START]], ...  \n",
       "2     [[[typographical, UPPERCASE_SENTENCE_START]], ...  \n",
       "3     [[[misspelling, MORFOLOGIK_RULE_EN_US]], [[mis...  \n",
       "4     [[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA]]...  \n",
       "...                                                 ...  \n",
       "1795  [[[misspelling, MORFOLOGIK_RULE_EN_US], [missp...  \n",
       "1796  [[[style, SOME_OF_THE]], [[typographical, UPPE...  \n",
       "1797  [[[misspelling, EN_SPECIFIC_CASE], [uncategori...  \n",
       "1798  [[[uncategorized, AI_HYDRA_LEO_MISSING_TO], [m...  \n",
       "1799  [[[uncategorized, AI_HYDRA_LEO_REPLACE_IN_TO],...  \n",
       "\n",
       "[1800 rows x 34 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_6 = pd.read_csv(\"csv_files/preprosessed_essays_training_set_6.csv\", converters={\"remove_single_char\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), \n",
    "                                                                             \"sentence_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), \n",
    "                                                                             \"stemmed_word_token\":lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "                                                                             \"stemmed_no_stopwords\":lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "                                                                             \"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "training_data_6 = training_data_6.set_index(\"essay_id\").join(training_data_6_errors_detected.set_index(\"essay_id\").loc[:, \"error_types\"]).reset_index().copy()\n",
    "training_data_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>normalised_docs</th>\n",
       "      <th>word_tok</th>\n",
       "      <th>sentence_tok</th>\n",
       "      <th>stemmed_word_token</th>\n",
       "      <th>stemmed_no_stopwords</th>\n",
       "      <th>error_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17834</td>\n",
       "      <td>7</td>\n",
       "      <td>Patience is when your waiting .I was patience ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>patience is when your waiting .i was patience ...</td>\n",
       "      <td>[Patience, is, when, your, waiting, I, was, pa...</td>\n",
       "      <td>[Patience is when your waiting .I was patience...</td>\n",
       "      <td>[patienc, is, when, your, wait, i, was, patien...</td>\n",
       "      <td>[patienc, wait, patienc, line, wait, lunch, nt...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_CP_YOUR_YOUARE]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17836</td>\n",
       "      <td>7</td>\n",
       "      <td>I am not a patience person, like I can’t sit i...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i am not a patience person, like i can’t sit i...</td>\n",
       "      <td>[I, am, not, a, patience, person, like, I, ca,...</td>\n",
       "      <td>[I am not a patience person, like I can’t sit ...</td>\n",
       "      <td>[i, am, not, a, patienc, person, like, i, ca, ...</td>\n",
       "      <td>[patienc, person, like, ca, nt, sit, sit, five...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START]], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17837</td>\n",
       "      <td>7</td>\n",
       "      <td>One day I was at basketball practice and I was...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one day i was at basketball practice and i was...</td>\n",
       "      <td>[One, day, I, was, at, basketball, practice, a...</td>\n",
       "      <td>[One day I was at basketball practice and I wa...</td>\n",
       "      <td>[one, day, i, was, at, basketbal, practic, and...</td>\n",
       "      <td>[one, day, basketbal, practic, run, team, get,...</td>\n",
       "      <td>[[[typographical, COMMA_COMPOUND_SENTENCE], [w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17838</td>\n",
       "      <td>7</td>\n",
       "      <td>I going to write about a time when I went to t...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i going to write about a time when i went to t...</td>\n",
       "      <td>[I, going, to, write, about, a, time, when, I,...</td>\n",
       "      <td>[I going to write about a time when I went to ...</td>\n",
       "      <td>[i, go, to, write, about, a, time, when, i, we...</td>\n",
       "      <td>[go, write, time, went, organization1, fair, f...</td>\n",
       "      <td>[[[grammar, PRP_VBG]], [[typographical, UPPERC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17839</td>\n",
       "      <td>7</td>\n",
       "      <td>It can be very hard for somebody to be patient...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it can be very hard for somebody to be patient...</td>\n",
       "      <td>[It, can, be, very, hard, for, somebody, to, b...</td>\n",
       "      <td>[It can be very hard for somebody to be patien...</td>\n",
       "      <td>[it, can, be, veri, hard, for, somebodi, to, b...</td>\n",
       "      <td>[veri, hard, somebodi, patient, patient, under...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START]], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>19558</td>\n",
       "      <td>7</td>\n",
       "      <td>One time I was getting a cool @CAPS1 game it w...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one time i was getting a cool @caps1 game it w...</td>\n",
       "      <td>[One, time, I, was, getting, a, cool, CAPS1, g...</td>\n",
       "      <td>[One time I was getting a cool @CAPS1 game it ...</td>\n",
       "      <td>[one, time, i, was, get, a, cool, caps1, game,...</td>\n",
       "      <td>[one, time, get, cool, caps1, game, sooo, awso...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>19559</td>\n",
       "      <td>7</td>\n",
       "      <td>A patent person in my life is my mom. Aicason ...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a patent person in my life is my mom. aicason ...</td>\n",
       "      <td>[A, patent, person, in, my, life, is, my, mom,...</td>\n",
       "      <td>[A patent person in my life is my mom., Aicaso...</td>\n",
       "      <td>[a, patent, person, in, my, life, is, my, mom,...</td>\n",
       "      <td>[patent, person, life, mom, aicason, mom, pati...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US]], [[typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>19561</td>\n",
       "      <td>7</td>\n",
       "      <td>A time when someone else I know was patient wa...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a time when someone else i know was patient wa...</td>\n",
       "      <td>[A, time, when, someone, else, I, know, was, p...</td>\n",
       "      <td>[A time when someone else I know was patient w...</td>\n",
       "      <td>[a, time, when, someon, els, i, know, was, pat...</td>\n",
       "      <td>[time, someon, els, know, patient, mom, look, ...</td>\n",
       "      <td>[[[typographical, RB_RB_COMMA]], [[typographic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>19562</td>\n",
       "      <td>7</td>\n",
       "      <td>I hate weddings. I love when people get marrie...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i hate weddings. i love when people get marrie...</td>\n",
       "      <td>[I, hate, weddings, I, love, when, people, get...</td>\n",
       "      <td>[I hate weddings., I love when people get marr...</td>\n",
       "      <td>[i, hate, wed, i, love, when, peopl, get, marr...</td>\n",
       "      <td>[hate, wed, love, peopl, get, marri, hate, cer...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START]], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>19563</td>\n",
       "      <td>7</td>\n",
       "      <td>A few weeks ago, we had a garage sale and a mo...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a few weeks ago, we had a garage sale and a mo...</td>\n",
       "      <td>[A, few, weeks, ago, we, had, a, garage, sale,...</td>\n",
       "      <td>[A few weeks ago, we had a garage sale and a m...</td>\n",
       "      <td>[a, few, week, ago, we, had, a, garag, sale, a...</td>\n",
       "      <td>[week, ago, garag, sale, mom, mom, sale, patie...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START]], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1569 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id  essay_set                                              essay  \\\n",
       "0        17834          7  Patience is when your waiting .I was patience ...   \n",
       "1        17836          7  I am not a patience person, like I can’t sit i...   \n",
       "2        17837          7  One day I was at basketball practice and I was...   \n",
       "3        17838          7  I going to write about a time when I went to t...   \n",
       "4        17839          7  It can be very hard for somebody to be patient...   \n",
       "...        ...        ...                                                ...   \n",
       "1564     19558          7  One time I was getting a cool @CAPS1 game it w...   \n",
       "1565     19559          7  A patent person in my life is my mom. Aicason ...   \n",
       "1566     19561          7  A time when someone else I know was patient wa...   \n",
       "1567     19562          7  I hate weddings. I love when people get marrie...   \n",
       "1568     19563          7  A few weeks ago, we had a garage sale and a mo...   \n",
       "\n",
       "      rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0                8.0             7.0             NaN           15.0   \n",
       "1                6.0             7.0             NaN           13.0   \n",
       "2                7.0             8.0             NaN           15.0   \n",
       "3                8.0             9.0             NaN           17.0   \n",
       "4                7.0             6.0             NaN           13.0   \n",
       "...              ...             ...             ...            ...   \n",
       "1564             6.0             6.0             NaN           12.0   \n",
       "1565             9.0             7.0             NaN           16.0   \n",
       "1566            11.0             8.0             NaN           19.0   \n",
       "1567            12.0            10.0             NaN           22.0   \n",
       "1568             7.0             8.0             NaN           15.0   \n",
       "\n",
       "      rater1_domain2  rater2_domain2  domain2_score  ...  rater3_trait3  \\\n",
       "0                NaN             NaN            NaN  ...            NaN   \n",
       "1                NaN             NaN            NaN  ...            NaN   \n",
       "2                NaN             NaN            NaN  ...            NaN   \n",
       "3                NaN             NaN            NaN  ...            NaN   \n",
       "4                NaN             NaN            NaN  ...            NaN   \n",
       "...              ...             ...            ...  ...            ...   \n",
       "1564             NaN             NaN            NaN  ...            NaN   \n",
       "1565             NaN             NaN            NaN  ...            NaN   \n",
       "1566             NaN             NaN            NaN  ...            NaN   \n",
       "1567             NaN             NaN            NaN  ...            NaN   \n",
       "1568             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "      rater3_trait4  rater3_trait5  rater3_trait6  \\\n",
       "0               NaN            NaN            NaN   \n",
       "1               NaN            NaN            NaN   \n",
       "2               NaN            NaN            NaN   \n",
       "3               NaN            NaN            NaN   \n",
       "4               NaN            NaN            NaN   \n",
       "...             ...            ...            ...   \n",
       "1564            NaN            NaN            NaN   \n",
       "1565            NaN            NaN            NaN   \n",
       "1566            NaN            NaN            NaN   \n",
       "1567            NaN            NaN            NaN   \n",
       "1568            NaN            NaN            NaN   \n",
       "\n",
       "                                        normalised_docs  \\\n",
       "0     patience is when your waiting .i was patience ...   \n",
       "1     i am not a patience person, like i can’t sit i...   \n",
       "2     one day i was at basketball practice and i was...   \n",
       "3     i going to write about a time when i went to t...   \n",
       "4     it can be very hard for somebody to be patient...   \n",
       "...                                                 ...   \n",
       "1564  one time i was getting a cool @caps1 game it w...   \n",
       "1565  a patent person in my life is my mom. aicason ...   \n",
       "1566  a time when someone else i know was patient wa...   \n",
       "1567  i hate weddings. i love when people get marrie...   \n",
       "1568  a few weeks ago, we had a garage sale and a mo...   \n",
       "\n",
       "                                               word_tok  \\\n",
       "0     [Patience, is, when, your, waiting, I, was, pa...   \n",
       "1     [I, am, not, a, patience, person, like, I, ca,...   \n",
       "2     [One, day, I, was, at, basketball, practice, a...   \n",
       "3     [I, going, to, write, about, a, time, when, I,...   \n",
       "4     [It, can, be, very, hard, for, somebody, to, b...   \n",
       "...                                                 ...   \n",
       "1564  [One, time, I, was, getting, a, cool, CAPS1, g...   \n",
       "1565  [A, patent, person, in, my, life, is, my, mom,...   \n",
       "1566  [A, time, when, someone, else, I, know, was, p...   \n",
       "1567  [I, hate, weddings, I, love, when, people, get...   \n",
       "1568  [A, few, weeks, ago, we, had, a, garage, sale,...   \n",
       "\n",
       "                                           sentence_tok  \\\n",
       "0     [Patience is when your waiting .I was patience...   \n",
       "1     [I am not a patience person, like I can’t sit ...   \n",
       "2     [One day I was at basketball practice and I wa...   \n",
       "3     [I going to write about a time when I went to ...   \n",
       "4     [It can be very hard for somebody to be patien...   \n",
       "...                                                 ...   \n",
       "1564  [One time I was getting a cool @CAPS1 game it ...   \n",
       "1565  [A patent person in my life is my mom., Aicaso...   \n",
       "1566  [A time when someone else I know was patient w...   \n",
       "1567  [I hate weddings., I love when people get marr...   \n",
       "1568  [A few weeks ago, we had a garage sale and a m...   \n",
       "\n",
       "                                     stemmed_word_token  \\\n",
       "0     [patienc, is, when, your, wait, i, was, patien...   \n",
       "1     [i, am, not, a, patienc, person, like, i, ca, ...   \n",
       "2     [one, day, i, was, at, basketbal, practic, and...   \n",
       "3     [i, go, to, write, about, a, time, when, i, we...   \n",
       "4     [it, can, be, veri, hard, for, somebodi, to, b...   \n",
       "...                                                 ...   \n",
       "1564  [one, time, i, was, get, a, cool, caps1, game,...   \n",
       "1565  [a, patent, person, in, my, life, is, my, mom,...   \n",
       "1566  [a, time, when, someon, els, i, know, was, pat...   \n",
       "1567  [i, hate, wed, i, love, when, peopl, get, marr...   \n",
       "1568  [a, few, week, ago, we, had, a, garag, sale, a...   \n",
       "\n",
       "                                   stemmed_no_stopwords  \\\n",
       "0     [patienc, wait, patienc, line, wait, lunch, nt...   \n",
       "1     [patienc, person, like, ca, nt, sit, sit, five...   \n",
       "2     [one, day, basketbal, practic, run, team, get,...   \n",
       "3     [go, write, time, went, organization1, fair, f...   \n",
       "4     [veri, hard, somebodi, patient, patient, under...   \n",
       "...                                                 ...   \n",
       "1564  [one, time, get, cool, caps1, game, sooo, awso...   \n",
       "1565  [patent, person, life, mom, aicason, mom, pati...   \n",
       "1566  [time, someon, els, know, patient, mom, look, ...   \n",
       "1567  [hate, wed, love, peopl, get, marri, hate, cer...   \n",
       "1568  [week, ago, garag, sale, mom, mom, sale, patie...   \n",
       "\n",
       "                                            error_types  \n",
       "0     [[[uncategorized, AI_HYDRA_LEO_CP_YOUR_YOUARE]...  \n",
       "1     [[[typographical, UPPERCASE_SENTENCE_START]], ...  \n",
       "2     [[[typographical, COMMA_COMPOUND_SENTENCE], [w...  \n",
       "3     [[[grammar, PRP_VBG]], [[typographical, UPPERC...  \n",
       "4     [[[typographical, UPPERCASE_SENTENCE_START]], ...  \n",
       "...                                                 ...  \n",
       "1564  [[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA],...  \n",
       "1565  [[[misspelling, MORFOLOGIK_RULE_EN_US]], [[typ...  \n",
       "1566  [[[typographical, RB_RB_COMMA]], [[typographic...  \n",
       "1567  [[[typographical, UPPERCASE_SENTENCE_START]], ...  \n",
       "1568  [[[typographical, UPPERCASE_SENTENCE_START]], ...  \n",
       "\n",
       "[1569 rows x 34 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_7 = pd.read_csv(\"csv_files/preprosessed_essays_training_set_7.csv\", converters={\"remove_single_char\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), \n",
    "                                                                             \"sentence_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), \n",
    "                                                                             \"stemmed_word_token\":lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "                                                                             \"stemmed_no_stopwords\":lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "                                                                             \"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "training_data_7 = training_data_7.set_index(\"essay_id\").join(training_data_7_errors_detected.set_index(\"essay_id\").loc[:, \"error_types\"]).reset_index().copy()\n",
    "training_data_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>normalised_docs</th>\n",
       "      <th>word_tok</th>\n",
       "      <th>sentence_tok</th>\n",
       "      <th>stemmed_word_token</th>\n",
       "      <th>stemmed_no_stopwords</th>\n",
       "      <th>error_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20716</td>\n",
       "      <td>8</td>\n",
       "      <td>A long time ago when I was in third grade I h...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a long time ago when i was in third grade i h...</td>\n",
       "      <td>[A, long, time, ago, when, I, was, in, third, ...</td>\n",
       "      <td>[\"A long time ago when I was in third grade I ...</td>\n",
       "      <td>[a, long, time, ago, when, i, was, in, third, ...</td>\n",
       "      <td>[long, time, ago, third, grade, friend, person...</td>\n",
       "      <td>[[[misspelling, EN_CONTRACTION_SPELLING]], [[u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20717</td>\n",
       "      <td>8</td>\n",
       "      <td>Softball has to be one of the single most gre...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>softball has to be one of the single most gre...</td>\n",
       "      <td>[Softball, has, to, be, one, of, the, single, ...</td>\n",
       "      <td>[Softball has to be one of the single most gre...</td>\n",
       "      <td>[softbal, has, to, be, one, of, the, singl, mo...</td>\n",
       "      <td>[softbal, one, singl, greatest, sport, aliv, p...</td>\n",
       "      <td>[[[grammar, MOST_SUPERLATIVE]], [[typographica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20718</td>\n",
       "      <td>8</td>\n",
       "      <td>Some people like making people laugh, I love ...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>some people like making people laugh, i love ...</td>\n",
       "      <td>[Some, people, like, making, people, laugh, I,...</td>\n",
       "      <td>[Some people like making people laugh, I love ...</td>\n",
       "      <td>[some, peopl, like, make, peopl, laugh, i, lov...</td>\n",
       "      <td>[peopl, like, make, peopl, laugh, love, anyth,...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START]], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20719</td>\n",
       "      <td>8</td>\n",
       "      <td>\"LAUGHTER\"  @CAPS1 I hang out with my friends...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>\"laughter\"  @caps1 i hang out with my friends...</td>\n",
       "      <td>[LAUGHTER, CAPS1, I, hang, out, with, my, frie...</td>\n",
       "      <td>[\"LAUGHTER\"  @CAPS1 I hang out with my friends...</td>\n",
       "      <td>[laughter, caps1, i, hang, out, with, my, frie...</td>\n",
       "      <td>[laughter, caps1, hang, friend, one, thing, be...</td>\n",
       "      <td>[[[whitespace, WHITESPACE_RULE]], [[typographi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20721</td>\n",
       "      <td>8</td>\n",
       "      <td>Well ima tell a story about the time i got @CA...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>well ima tell a story about the time i got @ca...</td>\n",
       "      <td>[Well, i, m, a, tell, a, story, about, the, ti...</td>\n",
       "      <td>[Well ima tell a story about the time i got @C...</td>\n",
       "      <td>[well, i, m, a, tell, a, stori, about, the, ti...</td>\n",
       "      <td>[well, tell, stori, time, got, caps3, town, he...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>21626</td>\n",
       "      <td>8</td>\n",
       "      <td>In most stories mothers and daughters are eit...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in most stories mothers and daughters are eit...</td>\n",
       "      <td>[In, most, stories, mothers, and, daughters, a...</td>\n",
       "      <td>[In most stories mothers and daughters are eit...</td>\n",
       "      <td>[in, most, stori, mother, and, daughter, are, ...</td>\n",
       "      <td>[stori, mother, daughter, either, enemi, frien...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA]]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>21628</td>\n",
       "      <td>8</td>\n",
       "      <td>I never understood the meaning laughter is th...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i never understood the meaning laughter is th...</td>\n",
       "      <td>[I, never, understood, the, meaning, laughter,...</td>\n",
       "      <td>[I never understood the meaning laughter is th...</td>\n",
       "      <td>[i, never, understood, the, mean, laughter, is...</td>\n",
       "      <td>[never, understood, mean, laughter, shortest, ...</td>\n",
       "      <td>[[[uncategorized, AI_HYDRA_LEO_MISSING_OF]], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>when you laugh, is @caps5 out of habit, or is ...</td>\n",
       "      <td>[When, you, laugh, is, CAPS5, out, of, habit, ...</td>\n",
       "      <td>[When you laugh, is @CAPS5 out of habit, or is...</td>\n",
       "      <td>[when, you, laugh, is, caps5, out, of, habit, ...</td>\n",
       "      <td>[laugh, caps5, habit, caps1, caus, caus, laugh...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START]], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>Trippin' on fen...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>trippin' on fen...</td>\n",
       "      <td>[Trippin, on, fences, I, am, NUM1, years, youn...</td>\n",
       "      <td>[\"Trippin on fences I am @NUM1 years young, an...</td>\n",
       "      <td>[trippin, on, fenc, i, am, num1, year, young, ...</td>\n",
       "      <td>[trippin, fenc, num1, year, young, short, num1...</td>\n",
       "      <td>[[[misspelling, MORFOLOGIK_RULE_EN_US], [uncat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>Many people believe that laughter can improve...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>many people believe that laughter can improve...</td>\n",
       "      <td>[Many, people, believe, that, laughter, can, i...</td>\n",
       "      <td>[Many people believe that laughter can improve...</td>\n",
       "      <td>[mani, peopl, believ, that, laughter, can, imp...</td>\n",
       "      <td>[mani, peopl, believ, laughter, improv, life, ...</td>\n",
       "      <td>[[[typographical, UPPERCASE_SENTENCE_START], [...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>723 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     essay_id  essay_set                                              essay  \\\n",
       "0       20716          8   A long time ago when I was in third grade I h...   \n",
       "1       20717          8   Softball has to be one of the single most gre...   \n",
       "2       20718          8   Some people like making people laugh, I love ...   \n",
       "3       20719          8   \"LAUGHTER\"  @CAPS1 I hang out with my friends...   \n",
       "4       20721          8  Well ima tell a story about the time i got @CA...   \n",
       "..        ...        ...                                                ...   \n",
       "718     21626          8   In most stories mothers and daughters are eit...   \n",
       "719     21628          8   I never understood the meaning laughter is th...   \n",
       "720     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
       "721     21630          8                                 Trippin' on fen...   \n",
       "722     21633          8   Many people believe that laughter can improve...   \n",
       "\n",
       "     rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0              18.0            16.0             NaN           34.0   \n",
       "1              21.0            26.0            46.0           46.0   \n",
       "2              15.0            20.0            40.0           40.0   \n",
       "3              12.0            20.0            30.0           30.0   \n",
       "4              11.0            15.0             NaN           26.0   \n",
       "..              ...             ...             ...            ...   \n",
       "718            17.0            18.0             NaN           35.0   \n",
       "719            15.0            17.0             NaN           32.0   \n",
       "720            20.0            26.0            40.0           40.0   \n",
       "721            20.0            20.0             NaN           40.0   \n",
       "722            20.0            20.0             NaN           40.0   \n",
       "\n",
       "     rater1_domain2  rater2_domain2  domain2_score  ...  rater3_trait3  \\\n",
       "0               NaN             NaN            NaN  ...            NaN   \n",
       "1               NaN             NaN            NaN  ...            5.0   \n",
       "2               NaN             NaN            NaN  ...            4.0   \n",
       "3               NaN             NaN            NaN  ...            3.0   \n",
       "4               NaN             NaN            NaN  ...            NaN   \n",
       "..              ...             ...            ...  ...            ...   \n",
       "718             NaN             NaN            NaN  ...            NaN   \n",
       "719             NaN             NaN            NaN  ...            NaN   \n",
       "720             NaN             NaN            NaN  ...            4.0   \n",
       "721             NaN             NaN            NaN  ...            NaN   \n",
       "722             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "     rater3_trait4  rater3_trait5  rater3_trait6  \\\n",
       "0              NaN            NaN            NaN   \n",
       "1              5.0            5.0            4.0   \n",
       "2              4.0            4.0            4.0   \n",
       "3              3.0            3.0            3.0   \n",
       "4              NaN            NaN            NaN   \n",
       "..             ...            ...            ...   \n",
       "718            NaN            NaN            NaN   \n",
       "719            NaN            NaN            NaN   \n",
       "720            4.0            4.0            4.0   \n",
       "721            NaN            NaN            NaN   \n",
       "722            NaN            NaN            NaN   \n",
       "\n",
       "                                       normalised_docs  \\\n",
       "0     a long time ago when i was in third grade i h...   \n",
       "1     softball has to be one of the single most gre...   \n",
       "2     some people like making people laugh, i love ...   \n",
       "3     \"laughter\"  @caps1 i hang out with my friends...   \n",
       "4    well ima tell a story about the time i got @ca...   \n",
       "..                                                 ...   \n",
       "718   in most stories mothers and daughters are eit...   \n",
       "719   i never understood the meaning laughter is th...   \n",
       "720  when you laugh, is @caps5 out of habit, or is ...   \n",
       "721                                 trippin' on fen...   \n",
       "722   many people believe that laughter can improve...   \n",
       "\n",
       "                                              word_tok  \\\n",
       "0    [A, long, time, ago, when, I, was, in, third, ...   \n",
       "1    [Softball, has, to, be, one, of, the, single, ...   \n",
       "2    [Some, people, like, making, people, laugh, I,...   \n",
       "3    [LAUGHTER, CAPS1, I, hang, out, with, my, frie...   \n",
       "4    [Well, i, m, a, tell, a, story, about, the, ti...   \n",
       "..                                                 ...   \n",
       "718  [In, most, stories, mothers, and, daughters, a...   \n",
       "719  [I, never, understood, the, meaning, laughter,...   \n",
       "720  [When, you, laugh, is, CAPS5, out, of, habit, ...   \n",
       "721  [Trippin, on, fences, I, am, NUM1, years, youn...   \n",
       "722  [Many, people, believe, that, laughter, can, i...   \n",
       "\n",
       "                                          sentence_tok  \\\n",
       "0    [\"A long time ago when I was in third grade I ...   \n",
       "1    [Softball has to be one of the single most gre...   \n",
       "2    [Some people like making people laugh, I love ...   \n",
       "3    [\"LAUGHTER\"  @CAPS1 I hang out with my friends...   \n",
       "4    [Well ima tell a story about the time i got @C...   \n",
       "..                                                 ...   \n",
       "718  [In most stories mothers and daughters are eit...   \n",
       "719  [I never understood the meaning laughter is th...   \n",
       "720  [When you laugh, is @CAPS5 out of habit, or is...   \n",
       "721  [\"Trippin on fences I am @NUM1 years young, an...   \n",
       "722  [Many people believe that laughter can improve...   \n",
       "\n",
       "                                    stemmed_word_token  \\\n",
       "0    [a, long, time, ago, when, i, was, in, third, ...   \n",
       "1    [softbal, has, to, be, one, of, the, singl, mo...   \n",
       "2    [some, peopl, like, make, peopl, laugh, i, lov...   \n",
       "3    [laughter, caps1, i, hang, out, with, my, frie...   \n",
       "4    [well, i, m, a, tell, a, stori, about, the, ti...   \n",
       "..                                                 ...   \n",
       "718  [in, most, stori, mother, and, daughter, are, ...   \n",
       "719  [i, never, understood, the, mean, laughter, is...   \n",
       "720  [when, you, laugh, is, caps5, out, of, habit, ...   \n",
       "721  [trippin, on, fenc, i, am, num1, year, young, ...   \n",
       "722  [mani, peopl, believ, that, laughter, can, imp...   \n",
       "\n",
       "                                  stemmed_no_stopwords  \\\n",
       "0    [long, time, ago, third, grade, friend, person...   \n",
       "1    [softbal, one, singl, greatest, sport, aliv, p...   \n",
       "2    [peopl, like, make, peopl, laugh, love, anyth,...   \n",
       "3    [laughter, caps1, hang, friend, one, thing, be...   \n",
       "4    [well, tell, stori, time, got, caps3, town, he...   \n",
       "..                                                 ...   \n",
       "718  [stori, mother, daughter, either, enemi, frien...   \n",
       "719  [never, understood, mean, laughter, shortest, ...   \n",
       "720  [laugh, caps5, habit, caps1, caus, caus, laugh...   \n",
       "721  [trippin, fenc, num1, year, young, short, num1...   \n",
       "722  [mani, peopl, believ, laughter, improv, life, ...   \n",
       "\n",
       "                                           error_types  \n",
       "0    [[[misspelling, EN_CONTRACTION_SPELLING]], [[u...  \n",
       "1    [[[grammar, MOST_SUPERLATIVE]], [[typographica...  \n",
       "2    [[[typographical, UPPERCASE_SENTENCE_START]], ...  \n",
       "3    [[[whitespace, WHITESPACE_RULE]], [[typographi...  \n",
       "4    [[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA],...  \n",
       "..                                                 ...  \n",
       "718  [[[uncategorized, AI_HYDRA_LEO_MISSING_COMMA]]...  \n",
       "719  [[[uncategorized, AI_HYDRA_LEO_MISSING_OF]], [...  \n",
       "720  [[[typographical, UPPERCASE_SENTENCE_START]], ...  \n",
       "721  [[[misspelling, MORFOLOGIK_RULE_EN_US], [uncat...  \n",
       "722  [[[typographical, UPPERCASE_SENTENCE_START], [...  \n",
       "\n",
       "[723 rows x 34 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_8 = pd.read_csv(\"csv_files/preprosessed_essays_training_set_8.csv\", converters={\"remove_single_char\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), \n",
    "                                                                             \"sentence_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), \n",
    "                                                                             \"stemmed_word_token\":lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "                                                                             \"stemmed_no_stopwords\":lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "                                                                             \"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "training_data_8 = training_data_8.set_index(\"essay_id\").join(training_data_8_errors_detected.set_index(\"essay_id\").loc[:, \"error_types\"]).reset_index().copy()\n",
    "training_data_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set grade column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1[\"grade\"] = training_data_1[\"domain1_score\"].copy()\n",
    "training_data_2[\"grade\"] = training_data_2[\"domain1_score\"].copy() \n",
    "training_data_3[\"grade\"] = training_data_3[\"domain1_score\"].copy()\n",
    "training_data_4[\"grade\"] = training_data_4[\"domain1_score\"].copy()\n",
    "training_data_5[\"grade\"] = training_data_5[\"domain1_score\"].copy()\n",
    "training_data_6[\"grade\"] = training_data_6[\"domain1_score\"].copy()\n",
    "training_data_7[\"grade\"] = training_data_7[\"domain1_score\"].copy()\n",
    "training_data_8[\"grade\"] = training_data_8[\"domain1_score\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1771, 35)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove NAs \n",
    "training_data_4 = training_data_4.dropna(subset = [\"grade\"], axis = 0).copy()\n",
    "training_data_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_data_1[\"error_types\"] = training_data_1[\"error_types\"].apply(lambda x: x.strip(\"[]\").replace(\"\\'\",\"\").split(\", \"))\\ntraining_data_2[\"error_types\"] = training_data_2[\"error_types\"].apply(lambda x: x.strip(\"[]\").replace(\"\\'\",\"\").split(\", \"))\\ntraining_data_3[\"error_types\"] = training_data_3[\"error_types\"].apply(lambda x: x.strip(\"[]\").replace(\"\\'\",\"\").split(\", \"))\\ntraining_data_4[\"error_types\"] = training_data_4[\"error_types\"].apply(lambda x: x.strip(\"[]\").replace(\"\\'\",\"\").split(\", \"))\\ntraining_data_5[\"error_types\"] = training_data_5[\"error_types\"].apply(lambda x: x.strip(\"[]\").replace(\"\\'\",\"\").split(\", \"))\\ntraining_data_6[\"error_types\"] = training_data_6[\"error_types\"].apply(lambda x: x.strip(\"[]\").replace(\"\\'\",\"\").split(\", \"))\\ntraining_data_7[\"error_types\"] = training_data_7[\"error_types\"].apply(lambda x: x.strip(\"[]\").replace(\"\\'\",\"\").split(\", \"))\\ntraining_data_8[\"error_types\"] = training_data_8[\"error_types\"].apply(lambda x: x.strip(\"[]\").replace(\"\\'\",\"\").split(\", \"))'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"error_types\"] = training_data_1[\"error_types\"].apply(lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"))\n",
    "training_data_2[\"error_types\"] = training_data_2[\"error_types\"].apply(lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"))\n",
    "training_data_3[\"error_types\"] = training_data_3[\"error_types\"].apply(lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"))\n",
    "training_data_4[\"error_types\"] = training_data_4[\"error_types\"].apply(lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"))\n",
    "training_data_5[\"error_types\"] = training_data_5[\"error_types\"].apply(lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"))\n",
    "training_data_6[\"error_types\"] = training_data_6[\"error_types\"].apply(lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"))\n",
    "training_data_7[\"error_types\"] = training_data_7[\"error_types\"].apply(lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"))\n",
    "training_data_8[\"error_types\"] = training_data_8[\"error_types\"].apply(lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_english = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_count(tok_words, stop_words):\n",
    "    return len([word for word in tok_words if word in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"nr_stopwords\"] = training_data_1[\"word_tok\"].apply(stopwords_count, args = (stopwords.words(\"english\"), ))\n",
    "training_data_2[\"nr_stopwords\"] = training_data_2[\"word_tok\"].apply(stopwords_count, args = (stopwords.words(\"english\"), ))\n",
    "training_data_3[\"nr_stopwords\"] = training_data_3[\"word_tok\"].apply(stopwords_count, args = (stopwords.words(\"english\"), ))\n",
    "training_data_4[\"nr_stopwords\"] = training_data_4[\"word_tok\"].apply(stopwords_count, args = (stopwords.words(\"english\"), ))\n",
    "training_data_5[\"nr_stopwords\"] = training_data_5[\"word_tok\"].apply(stopwords_count, args = (stopwords.words(\"english\"), ))\n",
    "training_data_6[\"nr_stopwords\"] = training_data_6[\"word_tok\"].apply(stopwords_count, args = (stopwords.words(\"english\"), ))\n",
    "training_data_7[\"nr_stopwords\"] = training_data_7[\"word_tok\"].apply(stopwords_count, args = (stopwords.words(\"english\"), ))\n",
    "training_data_8[\"nr_stopwords\"] = training_data_8[\"word_tok\"].apply(stopwords_count, args = (stopwords.words(\"english\"), ))\n",
    "training_data_8[\"nr_stopwords\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS & TAG dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS\n",
    "training_data_1[\"token_pos\"] = training_data_1[\"essay\"].apply(lambda text: [(token, token.pos_) for token in nlp_english(text)]) # tagging\n",
    "training_data_1[\"pos_dict\"] = training_data_1[\"token_pos\"].apply(lambda token_pos: Counter([j for i,j in token_pos])) # counting\n",
    "\n",
    "# TAG (fine grained)\n",
    "training_data_1[\"token_tag\"] = training_data_1[\"essay\"].apply(lambda text: [(token, token.tag_) for token in nlp_english(text)]) # tagging\n",
    "training_data_1[\"tag_dict\"] = training_data_1[\"token_tag\"].apply(lambda token_tag: Counter([j for i,j in token_tag])) # counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS\n",
    "training_data_2[\"token_pos\"] = training_data_2[\"essay\"].apply(lambda text: [(token, token.pos_) for token in nlp_english(text)]) # tagging\n",
    "training_data_2[\"pos_dict\"] = training_data_2[\"token_pos\"].apply(lambda token_pos: Counter([j for i,j in token_pos])) # counting\n",
    "\n",
    "# TAG (fine grained)\n",
    "training_data_2[\"token_tag\"] = training_data_2[\"essay\"].apply(lambda text: [(token, token.tag_) for token in nlp_english(text)]) # tagging\n",
    "training_data_2[\"tag_dict\"] = training_data_2[\"token_tag\"].apply(lambda token_tag: Counter([j for i,j in token_tag])) # counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS\n",
    "training_data_3[\"token_pos\"] = training_data_3[\"essay\"].apply(lambda text: [(token, token.pos_) for token in nlp_english(text)]) # tagging\n",
    "training_data_3[\"pos_dict\"] = training_data_3[\"token_pos\"].apply(lambda token_pos: Counter([j for i,j in token_pos])) # counting\n",
    "\n",
    "# TAG (fine grained)\n",
    "training_data_3[\"token_tag\"] = training_data_3[\"essay\"].apply(lambda text: [(token, token.tag_) for token in nlp_english(text)]) # tagging\n",
    "training_data_3[\"tag_dict\"] = training_data_3[\"token_tag\"].apply(lambda token_tag: Counter([j for i,j in token_tag])) # counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS\n",
    "training_data_4[\"token_pos\"] = training_data_4[\"essay\"].apply(lambda text: [(token, token.pos_) for token in nlp_english(text)]) # tagging\n",
    "training_data_4[\"pos_dict\"] = training_data_4[\"token_pos\"].apply(lambda token_pos: Counter([j for i,j in token_pos])) # counting\n",
    "\n",
    "# TAG (fine grained)\n",
    "training_data_4[\"token_tag\"] = training_data_4[\"essay\"].apply(lambda text: [(token, token.tag_) for token in nlp_english(text)]) # tagging\n",
    "training_data_4[\"tag_dict\"] = training_data_4[\"token_tag\"].apply(lambda token_tag: Counter([j for i,j in token_tag])) # counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS\n",
    "training_data_5[\"token_pos\"] = training_data_5[\"essay\"].apply(lambda text: [(token, token.pos_) for token in nlp_english(text)]) # tagging\n",
    "training_data_5[\"pos_dict\"] = training_data_5[\"token_pos\"].apply(lambda token_pos: Counter([j for i,j in token_pos])) # counting\n",
    "\n",
    "# TAG (fine grained)\n",
    "training_data_5[\"token_tag\"] = training_data_5[\"essay\"].apply(lambda text: [(token, token.tag_) for token in nlp_english(text)]) # tagging\n",
    "training_data_5[\"tag_dict\"] = training_data_5[\"token_tag\"].apply(lambda token_tag: Counter([j for i,j in token_tag])) # counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS\n",
    "training_data_6[\"token_pos\"] = training_data_6[\"essay\"].apply(lambda text: [(token, token.pos_) for token in nlp_english(text)]) # tagging\n",
    "training_data_6[\"pos_dict\"] = training_data_6[\"token_pos\"].apply(lambda token_pos: Counter([j for i,j in token_pos])) # counting\n",
    "\n",
    "# TAG (fine grained)\n",
    "training_data_6[\"token_tag\"] = training_data_6[\"essay\"].apply(lambda text: [(token, token.tag_) for token in nlp_english(text)]) # tagging\n",
    "training_data_6[\"tag_dict\"] = training_data_6[\"token_tag\"].apply(lambda token_tag: Counter([j for i,j in token_tag])) # counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS\n",
    "training_data_7[\"token_pos\"] = training_data_7[\"essay\"].apply(lambda text: [(token, token.pos_) for token in nlp_english(text)]) # tagging\n",
    "training_data_7[\"pos_dict\"] = training_data_7[\"token_pos\"].apply(lambda token_pos: Counter([j for i,j in token_pos])) # counting\n",
    "\n",
    "# TAG (fine grained)\n",
    "training_data_7[\"token_tag\"] = training_data_7[\"essay\"].apply(lambda text: [(token, token.tag_) for token in nlp_english(text)]) # tagging\n",
    "training_data_7[\"tag_dict\"] = training_data_7[\"token_tag\"].apply(lambda token_tag: Counter([j for i,j in token_tag])) # counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS\n",
    "training_data_8[\"token_pos\"] = training_data_8[\"essay\"].apply(lambda text: [(token, token.pos_) for token in nlp_english(text)]) # tagging\n",
    "training_data_8[\"pos_dict\"] = training_data_1[\"token_pos\"].apply(lambda token_pos: Counter([j for i,j in token_pos])) # counting\n",
    "\n",
    "# TAG (fine grained)\n",
    "training_data_8[\"token_tag\"] = training_data_8[\"essay\"].apply(lambda text: [(token, token.tag_) for token in nlp_english(text)]) # tagging\n",
    "training_data_8[\"tag_dict\"] = training_data_8[\"token_tag\"].apply(lambda token_tag: Counter([j for i,j in token_tag])) # counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_count(dict, pos_tag):\n",
    "    \"\"\"\n",
    "    Applied to a column in a data frame where pos_tag_count_dict was applied before.\n",
    "    Input: \n",
    "        pos_dict: a dictionary with pos tag as key and frequency as value\n",
    "        pos_tag: a string\n",
    "    \"\"\"\n",
    "\n",
    "    for key, value in dict.items():\n",
    "        if key == pos_tag:\n",
    "            return value\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparative adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1[\"comparative_adj\"] = training_data_1[\"tag_dict\"].apply(pos_tag_count, args = (\"JJR\",))\n",
    "training_data_2[\"comparative_adj\"] = training_data_2[\"tag_dict\"].apply(pos_tag_count, args = (\"JJR\",))\n",
    "training_data_3[\"comparative_adj\"] = training_data_3[\"tag_dict\"].apply(pos_tag_count, args = (\"JJR\",))\n",
    "training_data_4[\"comparative_adj\"] = training_data_4[\"tag_dict\"].apply(pos_tag_count, args = (\"JJR\",))\n",
    "training_data_5[\"comparative_adj\"] = training_data_5[\"tag_dict\"].apply(pos_tag_count, args = (\"JJR\",))\n",
    "training_data_6[\"comparative_adj\"] = training_data_6[\"tag_dict\"].apply(pos_tag_count, args = (\"JJR\",))\n",
    "training_data_7[\"comparative_adj\"] = training_data_7[\"tag_dict\"].apply(pos_tag_count, args = (\"JJR\",))\n",
    "training_data_8[\"comparative_adj\"] = training_data_8[\"tag_dict\"].apply(pos_tag_count, args = (\"JJR\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Superlative adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1[\"superlative_adj\"] = training_data_1[\"tag_dict\"].apply(pos_tag_count, args = (\"JJS\",))\n",
    "training_data_2[\"superlative_adj\"] = training_data_2[\"tag_dict\"].apply(pos_tag_count, args = (\"JJS\",))\n",
    "training_data_3[\"superlative_adj\"] = training_data_3[\"tag_dict\"].apply(pos_tag_count, args = (\"JJS\",))\n",
    "training_data_4[\"superlative_adj\"] = training_data_4[\"tag_dict\"].apply(pos_tag_count, args = (\"JJS\",))\n",
    "training_data_5[\"superlative_adj\"] = training_data_5[\"tag_dict\"].apply(pos_tag_count, args = (\"JJS\",))\n",
    "training_data_6[\"superlative_adj\"] = training_data_6[\"tag_dict\"].apply(pos_tag_count, args = (\"JJS\",))\n",
    "training_data_7[\"superlative_adj\"] = training_data_7[\"tag_dict\"].apply(pos_tag_count, args = (\"JJS\",))\n",
    "training_data_8[\"superlative_adj\"] = training_data_8[\"tag_dict\"].apply(pos_tag_count, args = (\"JJS\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modal auxiliary\n",
    "* English modal verbs are can, could, may, might, shall, should, will, would, and must\n",
    "* example: This **could** work\n",
    "* dutch kunnen moeten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1[\"modal_aux\"] = training_data_1[\"tag_dict\"].apply(pos_tag_count, args = (\"MD\",))\n",
    "training_data_2[\"modal_aux\"] = training_data_2[\"tag_dict\"].apply(pos_tag_count, args = (\"MD\",))\n",
    "training_data_3[\"modal_aux\"] = training_data_3[\"tag_dict\"].apply(pos_tag_count, args = (\"MD\",))\n",
    "training_data_4[\"modal_aux\"] = training_data_4[\"tag_dict\"].apply(pos_tag_count, args = (\"MD\",))\n",
    "training_data_5[\"modal_aux\"] = training_data_5[\"tag_dict\"].apply(pos_tag_count, args = (\"MD\",))\n",
    "training_data_6[\"modal_aux\"] = training_data_6[\"tag_dict\"].apply(pos_tag_count, args = (\"MD\",))\n",
    "training_data_7[\"modal_aux\"] = training_data_7[\"tag_dict\"].apply(pos_tag_count, args = (\"MD\",))\n",
    "training_data_8[\"modal_aux\"] = training_data_8[\"tag_dict\"].apply(pos_tag_count, args = (\"MD\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Participle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "VBG = training_data_1[\"tag_dict\"].apply(pos_tag_count, args = (\"VBG\",))\n",
    "VBN = training_data_1[\"tag_dict\"].apply(pos_tag_count, args = (\"VBN\",))\n",
    "training_data_1[\"participle\"] = VBG + VBN\n",
    "del VBG\n",
    "del VBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "VBG = training_data_2[\"tag_dict\"].apply(pos_tag_count, args = (\"VBG\",))\n",
    "VBN = training_data_2[\"tag_dict\"].apply(pos_tag_count, args = (\"VBN\",))\n",
    "training_data_2[\"participle\"] = VBG + VBN\n",
    "del VBG\n",
    "del VBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "VBG = training_data_3[\"tag_dict\"].apply(pos_tag_count, args = (\"VBG\",))\n",
    "VBN = training_data_3[\"tag_dict\"].apply(pos_tag_count, args = (\"VBN\",))\n",
    "training_data_3[\"participle\"] = VBG + VBN\n",
    "del VBG\n",
    "del VBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "VBG = training_data_4[\"tag_dict\"].apply(pos_tag_count, args = (\"VBG\",))\n",
    "VBN = training_data_4[\"tag_dict\"].apply(pos_tag_count, args = (\"VBN\",))\n",
    "training_data_4[\"participle\"] = VBG + VBN\n",
    "del VBG\n",
    "del VBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "VBG = training_data_5[\"tag_dict\"].apply(pos_tag_count, args = (\"VBG\",))\n",
    "VBN = training_data_5[\"tag_dict\"].apply(pos_tag_count, args = (\"VBN\",))\n",
    "training_data_5[\"participle\"] = VBG + VBN\n",
    "del VBG\n",
    "del VBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "VBG = training_data_6[\"tag_dict\"].apply(pos_tag_count, args = (\"VBG\",))\n",
    "VBN = training_data_6[\"tag_dict\"].apply(pos_tag_count, args = (\"VBN\",))\n",
    "training_data_6[\"participle\"] = VBG + VBN\n",
    "del VBG\n",
    "del VBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "VBG = training_data_7[\"tag_dict\"].apply(pos_tag_count, args = (\"VBG\",))\n",
    "VBN = training_data_7[\"tag_dict\"].apply(pos_tag_count, args = (\"VBN\",))\n",
    "training_data_7[\"participle\"] = VBG + VBN\n",
    "del VBG\n",
    "del VBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "VBG = training_data_8[\"tag_dict\"].apply(pos_tag_count, args = (\"VBG\",))\n",
    "VBN = training_data_8[\"tag_dict\"].apply(pos_tag_count, args = (\"VBN\",))\n",
    "training_data_8[\"participle\"] = VBG + VBN\n",
    "del VBG\n",
    "del VBN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Particle, “to” as preposition or infinitive marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1[\"infinitive_marker\"] = training_data_1[\"tag_dict\"].apply(pos_tag_count, args = (\"TO\",))\n",
    "training_data_2[\"infinitive_marker\"] = training_data_2[\"tag_dict\"].apply(pos_tag_count, args = (\"TO\",))\n",
    "training_data_3[\"infinitive_marker\"] = training_data_3[\"tag_dict\"].apply(pos_tag_count, args = (\"TO\",))\n",
    "training_data_4[\"infinitive_marker\"] = training_data_4[\"tag_dict\"].apply(pos_tag_count, args = (\"TO\",))\n",
    "training_data_5[\"infinitive_marker\"] = training_data_5[\"tag_dict\"].apply(pos_tag_count, args = (\"TO\",))\n",
    "training_data_6[\"infinitive_marker\"] = training_data_6[\"tag_dict\"].apply(pos_tag_count, args = (\"TO\",))\n",
    "training_data_7[\"infinitive_marker\"] = training_data_7[\"tag_dict\"].apply(pos_tag_count, args = (\"TO\",))\n",
    "training_data_8[\"infinitive_marker\"] = training_data_8[\"tag_dict\"].apply(pos_tag_count, args = (\"TO\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verb baseform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1[\"verb_baseform\"] = training_data_1[\"tag_dict\"].apply(pos_tag_count, args = (\"VB\",))\n",
    "training_data_2[\"verb_baseform\"] = training_data_2[\"tag_dict\"].apply(pos_tag_count, args = (\"VB\",))\n",
    "training_data_3[\"verb_baseform\"] = training_data_3[\"tag_dict\"].apply(pos_tag_count, args = (\"VB\",))\n",
    "training_data_4[\"verb_baseform\"] = training_data_4[\"tag_dict\"].apply(pos_tag_count, args = (\"VB\",))\n",
    "training_data_5[\"verb_baseform\"] = training_data_5[\"tag_dict\"].apply(pos_tag_count, args = (\"VB\",))\n",
    "training_data_6[\"verb_baseform\"] = training_data_6[\"tag_dict\"].apply(pos_tag_count, args = (\"VB\",))\n",
    "training_data_7[\"verb_baseform\"] = training_data_7[\"tag_dict\"].apply(pos_tag_count, args = (\"VB\",))\n",
    "training_data_8[\"verb_baseform\"] = training_data_8[\"tag_dict\"].apply(pos_tag_count, args = (\"VB\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verb past tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1[\"verb_past_tense\"] = training_data_1[\"tag_dict\"].apply(pos_tag_count, args = (\"VBD\",))\n",
    "training_data_2[\"verb_past_tense\"] = training_data_2[\"tag_dict\"].apply(pos_tag_count, args = (\"VBD\",))\n",
    "training_data_3[\"verb_past_tense\"] = training_data_3[\"tag_dict\"].apply(pos_tag_count, args = (\"VBD\",))\n",
    "training_data_4[\"verb_past_tense\"] = training_data_4[\"tag_dict\"].apply(pos_tag_count, args = (\"VBD\",))\n",
    "training_data_5[\"verb_past_tense\"] = training_data_5[\"tag_dict\"].apply(pos_tag_count, args = (\"VBD\",))\n",
    "training_data_6[\"verb_past_tense\"] = training_data_6[\"tag_dict\"].apply(pos_tag_count, args = (\"VBD\",))\n",
    "training_data_7[\"verb_past_tense\"] = training_data_7[\"tag_dict\"].apply(pos_tag_count, args = (\"VBD\",))\n",
    "training_data_8[\"verb_past_tense\"] = training_data_8[\"tag_dict\"].apply(pos_tag_count, args = (\"VBD\",))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing features\n",
    "\n",
    "* Past participle\n",
    "* gerund/presentparticiple\n",
    "* 3rd person sing.present\n",
    "* wh-determiner\n",
    "*  wh-pronoun\n",
    "* wh-adverb\n",
    "\n",
    "cant find the dutch tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Height of the tree presenting sentence structure\n",
    "\n",
    "https://stackoverflow.com/questions/64591644/how-to-get-height-of-dependency-tree-with-spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_tree(node, depth):\n",
    "\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return max(walk_tree(child, depth + 1) for child in node.children)\n",
    "    else:\n",
    "        return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1[\"avg_tree_height\"] = training_data_1[\"essay\"].apply(lambda text: np.array([walk_tree(sent.root, 0) for sent in nlp_english(text).sents]).mean())\n",
    "training_data_2[\"avg_tree_height\"] = training_data_2[\"essay\"].apply(lambda text: np.array([walk_tree(sent.root, 0) for sent in nlp_english(text).sents]).mean())\n",
    "training_data_3[\"avg_tree_height\"] = training_data_3[\"essay\"].apply(lambda text: np.array([walk_tree(sent.root, 0) for sent in nlp_english(text).sents]).mean())\n",
    "training_data_4[\"avg_tree_height\"] = training_data_4[\"essay\"].apply(lambda text: np.array([walk_tree(sent.root, 0) for sent in nlp_english(text).sents]).mean())\n",
    "training_data_5[\"avg_tree_height\"] = training_data_5[\"essay\"].apply(lambda text: np.array([walk_tree(sent.root, 0) for sent in nlp_english(text).sents]).mean())\n",
    "training_data_6[\"avg_tree_height\"] = training_data_6[\"essay\"].apply(lambda text: np.array([walk_tree(sent.root, 0) for sent in nlp_english(text).sents]).mean())\n",
    "training_data_7[\"avg_tree_height\"] = training_data_7[\"essay\"].apply(lambda text: np.array([walk_tree(sent.root, 0) for sent in nlp_english(text).sents]).mean())\n",
    "training_data_8[\"avg_tree_height\"] = training_data_8[\"essay\"].apply(lambda text: np.array([walk_tree(sent.root, 0) for sent in nlp_english(text).sents]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine number of syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English\n",
    "\n",
    "Idea and Data from https://www.kaggle.com/code/datasniffer/a-syllable-counting-model-exercise-with-caret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>nsyl</th>\n",
       "      <th>a</th>\n",
       "      <th>h</th>\n",
       "      <th>r</th>\n",
       "      <th>t</th>\n",
       "      <th>ar</th>\n",
       "      <th>ha</th>\n",
       "      <th>rt</th>\n",
       "      <th>art</th>\n",
       "      <th>...</th>\n",
       "      <th>hso</th>\n",
       "      <th>ths</th>\n",
       "      <th>iez</th>\n",
       "      <th>zoe</th>\n",
       "      <th>di</th>\n",
       "      <th>myo</th>\n",
       "      <th>yoc</th>\n",
       "      <th>nax</th>\n",
       "      <th>nd</th>\n",
       "      <th>zod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hart</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bise</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scad</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ques</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exp</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>incommensurability</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>propionibacteria</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>zodiacal constellation</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>microspectrophotometric</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>palaeoentomologist</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 4550 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         word  nsyl  a  h  r  t  ar  ha  rt  art  ...  hso  \\\n",
       "0                        hart     1  1  1  1  1   1   1   1    1  ...    0   \n",
       "1                        bise     1  0  0  0  0   0   0   0    0  ...    0   \n",
       "2                        scad     1  1  0  0  0   0   0   0    0  ...    0   \n",
       "3                        ques     1  0  0  0  0   0   0   0    0  ...    0   \n",
       "4                         exp     1  0  0  0  0   0   0   0    0  ...    0   \n",
       "...                       ...   ... .. .. .. ..  ..  ..  ..  ...  ...  ...   \n",
       "3195       incommensurability     8  1  0  1  1   0   0   0    0  ...    0   \n",
       "3196         propionibacteria     8  2  0  2  1   0   0   0    0  ...    0   \n",
       "3197   zodiacal constellation     8  3  0  0  2   0   0   0    0  ...    0   \n",
       "3198  microspectrophotometric     8  0  1  3  3   0   0   0    0  ...    0   \n",
       "3199       palaeoentomologist     8  2  0  0  2   0   0   0    0  ...    0   \n",
       "\n",
       "      ths  iez  zoe  di   myo  yoc  nax  nd   zod  \n",
       "0       0    0    0    0    0    0    0    0    0  \n",
       "1       0    0    0    0    0    0    0    0    0  \n",
       "2       0    0    0    0    0    0    0    0    0  \n",
       "3       0    0    0    0    0    0    0    0    0  \n",
       "4       0    0    0    0    0    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "3195    0    0    0    0    0    0    0    0    0  \n",
       "3196    0    0    0    0    0    0    0    0    0  \n",
       "3197    0    0    0    0    0    0    0    0    1  \n",
       "3198    0    0    0    0    0    0    0    0    0  \n",
       "3199    0    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[3200 rows x 4550 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syllable_data = pd.read_csv(\"csv_files/features/en_syllable_3grams.csv\")\n",
    "syllable_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>h</th>\n",
       "      <th>r</th>\n",
       "      <th>t</th>\n",
       "      <th>b</th>\n",
       "      <th>e</th>\n",
       "      <th>i</th>\n",
       "      <th>s</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>...</th>\n",
       "      <th>n</th>\n",
       "      <th>l</th>\n",
       "      <th>m</th>\n",
       "      <th>er</th>\n",
       "      <th>in</th>\n",
       "      <th>y</th>\n",
       "      <th>al</th>\n",
       "      <th>ti</th>\n",
       "      <th>on</th>\n",
       "      <th>ic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      a  h  r  t  b  e  i  s  c  d  ...  n  l  m  er  in  y  al  ti  on  ic\n",
       "0     1  1  1  1  0  0  0  0  0  0  ...  0  0  0   0   0  0   0   0   0   0\n",
       "1     0  0  0  0  1  1  1  1  0  0  ...  0  0  0   0   0  0   0   0   0   0\n",
       "2     1  0  0  0  0  0  0  1  1  1  ...  0  0  0   0   0  0   0   0   0   0\n",
       "3     0  0  0  0  0  1  0  1  0  0  ...  0  0  0   0   0  0   0   0   0   0\n",
       "4     0  0  0  0  0  1  0  0  0  0  ...  0  0  0   0   0  0   0   0   0   0\n",
       "...  .. .. .. .. .. .. .. .. .. ..  ... .. .. ..  ..  .. ..  ..  ..  ..  ..\n",
       "3195  1  0  1  1  1  1  3  1  1  0  ...  2  1  2   0   1  1   0   0   0   0\n",
       "3196  2  0  2  1  1  1  3  0  1  0  ...  1  0  0   1   0  0   0   0   1   0\n",
       "3197  3  0  0  2  0  1  2  1  2  1  ...  2  3  0   0   0  0   1   1   2   0\n",
       "3198  0  1  3  3  0  2  2  1  3  0  ...  0  0  2   0   0  0   0   0   0   2\n",
       "3199  2  0  0  2  0  2  1  1  0  0  ...  1  2  1   0   0  0   1   0   0   0\n",
       "\n",
       "[3200 rows x 24 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code from https://www.analyticsvidhya.com/blog/2021/04/beginners-guide-to-low-variance-filter-and-its-implementation/\n",
    "\n",
    "def del_nzv_features(features_df, threshhold = .8 * (1 - .8)):\n",
    "\n",
    "    # calculate variance of the features\n",
    "    variance = features_df.var()\n",
    "\n",
    "    columns = features_df.columns\n",
    "    variable = [ ]\n",
    "  \n",
    "    #saving the names of variables having variance more than a threshold value\n",
    "    for i in range(0, len(variance)):\n",
    "        if variance[i] >= threshhold: \n",
    "            variable.append(columns[i])\n",
    "\n",
    "    return features_df[variable]\n",
    "\n",
    "\n",
    "nzv_features = del_nzv_features(syllable_data.iloc[:,2:])\n",
    "nzv_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['a', 'h', 'r', 't', 'b', 'e', 'i', 's', 'c', 'd', 'u', 'p', 'o', 'g',\n",
       "       'n', 'l', 'm', 'er', 'in', 'y', 'al', 'ti', 'on', 'ic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nzv_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English syllable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9399865837447998"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = nzv_features\n",
    "y = np.array(syllable_data.iloc[:,1])\n",
    "reg = LinearRegression().fit(X, y)\n",
    "reg.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict\n",
    "\n",
    "Build a vocabulary and predict the number of syllables of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(word_tok_col):\n",
    "\n",
    "    # Build vocabulary https://analyticsindiamag.com/how-to-create-a-vocabulary-builder-for-nlp-tasks/\n",
    "\n",
    "    flattened = []\n",
    "    for sublist in word_tok_col:\n",
    "        for val in sublist:\n",
    "            flattened.append(val)\n",
    "\n",
    "    vocab = []\n",
    "    for item in flattened:\n",
    "        if not item in vocab:\n",
    "            vocab.append(item)\n",
    "\n",
    "    return pd.DataFrame({\"words\": vocab})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_training_data_1 = build_vocab(training_data_1[\"word_tok\"])\n",
    "vocab_training_data_2 = build_vocab(training_data_2[\"word_tok\"])\n",
    "vocab_training_data_3 = build_vocab(training_data_3[\"word_tok\"])\n",
    "vocab_training_data_4 = build_vocab(training_data_4[\"word_tok\"])\n",
    "vocab_training_data_5 = build_vocab(training_data_5[\"word_tok\"])\n",
    "vocab_training_data_6 = build_vocab(training_data_6[\"word_tok\"])\n",
    "vocab_training_data_7 = build_vocab(training_data_7[\"word_tok\"])\n",
    "vocab_training_data_8 = build_vocab(training_data_8[\"word_tok\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_letters(word, occurence, letter):\n",
    "    return len(re.findall(\"[^\" + letter + \"]*\" + letter + \"{\" + str(occurence) + \"}[^\" + letter + \"]*\", word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_letter_combinations(vocabulary):\n",
    "    # ONE letter\n",
    "    for i in list(ascii_lowercase):\n",
    "        vocabulary[str(i)] = vocabulary[\"words\"].apply(count_letters, args = (1, str(i), )).copy()\n",
    "\n",
    "    # TWO letter combinations\n",
    "    two_letter_combinations = [letter1 + letter2 for letter1 in ascii_lowercase for letter2 in ascii_lowercase]\n",
    "\n",
    "    for i in two_letter_combinations:\n",
    "        vocabulary[str(i)] = vocabulary[\"words\"].apply(count_letters, args = (2, str(i), )).copy()\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_18032/1635023604.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vocabulary[str(i)] = vocabulary[\"words\"].apply(count_letters, args = (2, str(i), )).copy()\n"
     ]
    }
   ],
   "source": [
    "vocab_comb_training_data_1 = get_letter_combinations(vocab_training_data_1)\n",
    "vocab_comb_training_data_2 = get_letter_combinations(vocab_training_data_2)\n",
    "vocab_comb_training_data_3 = get_letter_combinations(vocab_training_data_3)\n",
    "vocab_comb_training_data_4 = get_letter_combinations(vocab_training_data_4)\n",
    "vocab_comb_training_data_5 = get_letter_combinations(vocab_training_data_5)\n",
    "vocab_comb_training_data_6 = get_letter_combinations(vocab_training_data_6)\n",
    "vocab_comb_training_data_7 = get_letter_combinations(vocab_training_data_7)\n",
    "vocab_comb_training_data_8 = get_letter_combinations(vocab_training_data_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df_training_data_1 = vocab_comb_training_data_1[nzv_features.columns]\n",
    "feature_df_training_data_2 = vocab_comb_training_data_2[nzv_features.columns]\n",
    "feature_df_training_data_3 = vocab_comb_training_data_3[nzv_features.columns]\n",
    "feature_df_training_data_4 = vocab_comb_training_data_4[nzv_features.columns]\n",
    "feature_df_training_data_5 = vocab_comb_training_data_5[nzv_features.columns]\n",
    "feature_df_training_data_6 = vocab_comb_training_data_6[nzv_features.columns]\n",
    "feature_df_training_data_7 = vocab_comb_training_data_7[nzv_features.columns]\n",
    "feature_df_training_data_8 = vocab_comb_training_data_8[nzv_features.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nsylb_training_data_1 = reg.predict(feature_df_training_data_1)\n",
    "pred_nsylb_training_data_2 = reg.predict(feature_df_training_data_2)\n",
    "pred_nsylb_training_data_3 = reg.predict(feature_df_training_data_3)\n",
    "pred_nsylb_training_data_4 = reg.predict(feature_df_training_data_4)\n",
    "pred_nsylb_training_data_5 = reg.predict(feature_df_training_data_5)\n",
    "pred_nsylb_training_data_6 = reg.predict(feature_df_training_data_6)\n",
    "pred_nsylb_training_data_7 = reg.predict(feature_df_training_data_7)\n",
    "pred_nsylb_training_data_8 = reg.predict(feature_df_training_data_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_syllables(vocabulary, pred_nsylb):\n",
    "\n",
    "    syllable_count = pd.DataFrame({\"words\": vocabulary[\"words\"], \"pred_nsylb\" : np.round(pred_nsylb)})\n",
    "    syllable_count[\"pred_nsylb\"] = syllable_count[\"pred_nsylb\"].astype(int).copy()\n",
    "\n",
    "    # convert to dictionary\n",
    "    syllables_dict = dict(zip(syllable_count.words, syllable_count.pred_nsylb))\n",
    "    return syllables_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_syllables_training_data_1 = get_nr_syllables(vocab_training_data_1, pred_nsylb_training_data_1)\n",
    "nr_syllables_training_data_2 = get_nr_syllables(vocab_training_data_2, pred_nsylb_training_data_2)\n",
    "nr_syllables_training_data_3 = get_nr_syllables(vocab_training_data_3, pred_nsylb_training_data_3)\n",
    "nr_syllables_training_data_4 = get_nr_syllables(vocab_training_data_4, pred_nsylb_training_data_4)\n",
    "nr_syllables_training_data_5 = get_nr_syllables(vocab_training_data_5, pred_nsylb_training_data_5)\n",
    "nr_syllables_training_data_6 = get_nr_syllables(vocab_training_data_6, pred_nsylb_training_data_6)\n",
    "nr_syllables_training_data_7 = get_nr_syllables(vocab_training_data_7, pred_nsylb_training_data_7)\n",
    "nr_syllables_training_data_8 = get_nr_syllables(vocab_training_data_8, pred_nsylb_training_data_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1[\"nr_syllables\"] = training_data_1[\"word_tok\"].apply(lambda token_list: [nr_syllables_training_data_1[token] for token in token_list])\n",
    "training_data_2[\"nr_syllables\"] = training_data_2[\"word_tok\"].apply(lambda token_list: [nr_syllables_training_data_2[token] for token in token_list])\n",
    "training_data_3[\"nr_syllables\"] = training_data_3[\"word_tok\"].apply(lambda token_list: [nr_syllables_training_data_3[token] for token in token_list])\n",
    "training_data_4[\"nr_syllables\"] = training_data_4[\"word_tok\"].apply(lambda token_list: [nr_syllables_training_data_4[token] for token in token_list])\n",
    "training_data_5[\"nr_syllables\"] = training_data_5[\"word_tok\"].apply(lambda token_list: [nr_syllables_training_data_5[token] for token in token_list])\n",
    "training_data_6[\"nr_syllables\"] = training_data_6[\"word_tok\"].apply(lambda token_list: [nr_syllables_training_data_6[token] for token in token_list])\n",
    "training_data_7[\"nr_syllables\"] = training_data_7[\"word_tok\"].apply(lambda token_list: [nr_syllables_training_data_7[token] for token in token_list])\n",
    "training_data_8[\"nr_syllables\"] = training_data_8[\"word_tok\"].apply(lambda token_list: [nr_syllables_training_data_8[token] for token in token_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [22, 7, 20, 18, 15, 42, 18, 4, 20, 3, 6, 18, 3...\n",
       "1      [23, 9, 6, 19, 19, 12, 10, 1, 2, 25, 2, 5, 10,...\n",
       "2      [6, 3, 10, 7, 4, 9, 9, 20, 15, 12, 12, 2, 6, 5...\n",
       "3      [8, 12, 12, 13, 17, 5, 1, 7, 18, 1, 49, 17, 16...\n",
       "4      [13, 17, 11, 11, 8, 9, 20, 18, 3, 2, 10, 19, 2...\n",
       "                             ...                        \n",
       "718    [11, 4, 14, 4, 3, 7, 4, 2, 12, 10, 11, 14, 3, ...\n",
       "719    [13, 4, 8, 2, 7, 12, 26, 10, 9, 6, 6, 15, 11, ...\n",
       "720    [3, 5, 4, 6, 11, 11, 6, 25, 2, 27, 5, 3, 3, 2,...\n",
       "721    [8, 6, 5, 3, 8, 7, 13, 11, 7, 4, 11, 6, 11, 1,...\n",
       "722    [9, 12, 10, 8, 10, 10, 4, 13, 4, 2, 11, 13, 6,...\n",
       "Name: sentence_len_words, Length: 723, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"sentence_len_words\"] = training_data_1[\"sentence_tok\"].apply(lambda sentence_list: [len(list(filter(None, sentence.split(\" \")))) for sentence in sentence_list])\n",
    "training_data_2[\"sentence_len_words\"] = training_data_2[\"sentence_tok\"].apply(lambda sentence_list: [len(list(filter(None, sentence.split(\" \")))) for sentence in sentence_list])\n",
    "training_data_3[\"sentence_len_words\"] = training_data_3[\"sentence_tok\"].apply(lambda sentence_list: [len(list(filter(None, sentence.split(\" \")))) for sentence in sentence_list])\n",
    "training_data_4[\"sentence_len_words\"] = training_data_4[\"sentence_tok\"].apply(lambda sentence_list: [len(list(filter(None, sentence.split(\" \")))) for sentence in sentence_list])\n",
    "training_data_5[\"sentence_len_words\"] = training_data_5[\"sentence_tok\"].apply(lambda sentence_list: [len(list(filter(None, sentence.split(\" \")))) for sentence in sentence_list])\n",
    "training_data_6[\"sentence_len_words\"] = training_data_6[\"sentence_tok\"].apply(lambda sentence_list: [len(list(filter(None, sentence.split(\" \")))) for sentence in sentence_list])\n",
    "training_data_7[\"sentence_len_words\"] = training_data_7[\"sentence_tok\"].apply(lambda sentence_list: [len(list(filter(None, sentence.split(\" \")))) for sentence in sentence_list])\n",
    "training_data_8[\"sentence_len_words\"] = training_data_8[\"sentence_tok\"].apply(lambda sentence_list: [len(list(filter(None, sentence.split(\" \")))) for sentence in sentence_list])\n",
    "training_data_8[\"sentence_len_words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      15.0\n",
       "1      12.0\n",
       "2      10.0\n",
       "3      15.0\n",
       "4      11.0\n",
       "       ... \n",
       "718     9.0\n",
       "719     9.0\n",
       "720     9.0\n",
       "721     7.0\n",
       "722     8.0\n",
       "Name: avg_sentence_len_words, Length: 723, dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"avg_sentence_len_words\"] = training_data_1[\"sentence_len_words\"].apply(lambda sentence_list: np.round([sum(sentence_len for sentence_len in sentence_list) / len(sentence_list)][0]))\n",
    "training_data_2[\"avg_sentence_len_words\"] = training_data_2[\"sentence_len_words\"].apply(lambda sentence_list: np.round([sum(sentence_len for sentence_len in sentence_list) / len(sentence_list)][0]))\n",
    "training_data_3[\"avg_sentence_len_words\"] = training_data_3[\"sentence_len_words\"].apply(lambda sentence_list: np.round([sum(sentence_len for sentence_len in sentence_list) / len(sentence_list)][0]))\n",
    "training_data_4[\"avg_sentence_len_words\"] = training_data_4[\"sentence_len_words\"].apply(lambda sentence_list: np.round([sum(sentence_len for sentence_len in sentence_list) / len(sentence_list)][0]))\n",
    "training_data_5[\"avg_sentence_len_words\"] = training_data_5[\"sentence_len_words\"].apply(lambda sentence_list: np.round([sum(sentence_len for sentence_len in sentence_list) / len(sentence_list)][0]))\n",
    "training_data_6[\"avg_sentence_len_words\"] = training_data_6[\"sentence_len_words\"].apply(lambda sentence_list: np.round([sum(sentence_len for sentence_len in sentence_list) / len(sentence_list)][0]))\n",
    "training_data_7[\"avg_sentence_len_words\"] = training_data_7[\"sentence_len_words\"].apply(lambda sentence_list: np.round([sum(sentence_len for sentence_len in sentence_list) / len(sentence_list)][0]))\n",
    "training_data_8[\"avg_sentence_len_words\"] = training_data_8[\"sentence_len_words\"].apply(lambda sentence_list: np.round([sum(sentence_len for sentence_len in sentence_list) / len(sentence_list)][0]))\n",
    "training_data_8[\"avg_sentence_len_words\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dale-Chall readability formula\n",
    "\n",
    "Wordlist: https://readabilityformulas.com/articles/dale-chall-readability-word-list.php\n",
    "\n",
    "Formula from Hussain et al. (2011) score = 0.1579 * pdw + 0.0496 * avgsentint + 3.6365\n",
    "\n",
    "Formula from (DuBay, 2007)\n",
    "1.\tSelect  100-¬‐ word  samples  throughout  the  text  (for  books,  every  tenth  page  is  recommended).\n",
    "2.\tCompute  the  average  sentence  length  in  words.\n",
    "3.\tCompute  the  percentage  of  words  not  in  the  Dale  list  of  3,000  words.\n",
    "4.\tCompute  this  equation: Score  =  .1579PDW  +  .0496ASL  +  3.6365\n",
    "Where:  Score  =  reading  grade  of  a  reader  who  can  answer  one-¬‐ half  of  the  test  questions  on  a  passage.PDW=  Percentage  of  Difficult  Words  (total  number  of  not  on  the  Dale-¬‐ Chall  word  list  divided  by  the  total  number  of  words  counted)  ASL  =  Average  Sentence  Length  in  words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_words_not_in_list(token_list, list2):\n",
    "    \"\"\"\n",
    "    Comparing the elements of two lists. The words that are not in list2 are returnd.\n",
    "    \"\"\"\n",
    "    # number of words not in dale chall list / total nr of words in the list\n",
    "    return (len([token for token in list(set(token_list)) if token not in list2]) / len(list(set(token_list)))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dale_chall_words = pd.read_excel(\"csv_files/features/Dale_chall_wordlist.xlsx\")\n",
    "dale_chall_wordlist = dale_chall_words[\"Dale-Chall words\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dale_chall_readability(x):\n",
    "    # getting a sample of 100 words\n",
    "    \n",
    "    if len(x.word_tok) > 100:\n",
    "        sample = random.sample(x.word_tok, 100) if len(x.word_tok) > 100 else x.word_tok    \n",
    "    else: \n",
    "        sample = x.word_tok\n",
    "\n",
    "    # calculate percentage of words not in Dale-Chall list\n",
    "    percentage_not_in_dale_chall = percentage_words_not_in_list(sample, dale_chall_wordlist)\n",
    "\n",
    "    # calculate Dale-Chall readability\n",
    "    return 0.1579 * percentage_not_in_dale_chall + 0.0496 * x.avg_sentence_len_words + 3.6365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      8.081281\n",
       "1      8.802489\n",
       "2      8.776618\n",
       "3      7.312929\n",
       "4      9.946703\n",
       "         ...   \n",
       "718    8.625229\n",
       "719    8.530787\n",
       "720    8.753182\n",
       "721    9.115450\n",
       "722    8.770300\n",
       "Name: dale_chall_readability, Length: 723, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"dale_chall_readability\"] = training_data_1.apply(lambda x: get_dale_chall_readability(x), axis = 1)\n",
    "training_data_2[\"dale_chall_readability\"] = training_data_2.apply(lambda x: get_dale_chall_readability(x), axis = 1)\n",
    "training_data_3[\"dale_chall_readability\"] = training_data_3.apply(lambda x: get_dale_chall_readability(x), axis = 1)\n",
    "training_data_4[\"dale_chall_readability\"] = training_data_4.apply(lambda x: get_dale_chall_readability(x), axis = 1)\n",
    "training_data_5[\"dale_chall_readability\"] = training_data_5.apply(lambda x: get_dale_chall_readability(x), axis = 1)\n",
    "training_data_6[\"dale_chall_readability\"] = training_data_6.apply(lambda x: get_dale_chall_readability(x), axis = 1)\n",
    "training_data_7[\"dale_chall_readability\"] = training_data_7.apply(lambda x: get_dale_chall_readability(x), axis = 1)\n",
    "training_data_8[\"dale_chall_readability\"] = training_data_8.apply(lambda x: get_dale_chall_readability(x), axis = 1)\n",
    "training_data_8[\"dale_chall_readability\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique (different) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      259\n",
       "1      308\n",
       "2      323\n",
       "3      255\n",
       "4      203\n",
       "      ... \n",
       "718    342\n",
       "719    218\n",
       "720    369\n",
       "721    250\n",
       "722    232\n",
       "Name: unique_words, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"unique_words\"] = training_data_1[\"word_tok\"].apply(lambda word_list: len(set(word_list)))\n",
    "training_data_2[\"unique_words\"] = training_data_2[\"word_tok\"].apply(lambda word_list: len(set(word_list)))\n",
    "training_data_3[\"unique_words\"] = training_data_3[\"word_tok\"].apply(lambda word_list: len(set(word_list)))\n",
    "training_data_4[\"unique_words\"] = training_data_4[\"word_tok\"].apply(lambda word_list: len(set(word_list)))\n",
    "training_data_5[\"unique_words\"] = training_data_5[\"word_tok\"].apply(lambda word_list: len(set(word_list)))\n",
    "training_data_6[\"unique_words\"] = training_data_6[\"word_tok\"].apply(lambda word_list: len(set(word_list)))\n",
    "training_data_7[\"unique_words\"] = training_data_7[\"word_tok\"].apply(lambda word_list: len(set(word_list)))\n",
    "training_data_8[\"unique_words\"] = training_data_8[\"word_tok\"].apply(lambda word_list: len(set(word_list)))\n",
    "training_data_8[\"unique_words\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      679\n",
       "1      785\n",
       "2      861\n",
       "3      712\n",
       "4      671\n",
       "      ... \n",
       "718    862\n",
       "719    556\n",
       "720    835\n",
       "721    576\n",
       "722    475\n",
       "Name: word_count, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"word_count\"] = training_data_1[\"word_tok\"].apply(lambda word_list: len(word_list))\n",
    "training_data_2[\"word_count\"] = training_data_2[\"word_tok\"].apply(lambda word_list: len(word_list))\n",
    "training_data_3[\"word_count\"] = training_data_3[\"word_tok\"].apply(lambda word_list: len(word_list))\n",
    "training_data_4[\"word_count\"] = training_data_4[\"word_tok\"].apply(lambda word_list: len(word_list))\n",
    "training_data_5[\"word_count\"] = training_data_5[\"word_tok\"].apply(lambda word_list: len(word_list))\n",
    "training_data_6[\"word_count\"] = training_data_6[\"word_tok\"].apply(lambda word_list: len(word_list))\n",
    "training_data_7[\"word_count\"] = training_data_7[\"word_tok\"].apply(lambda word_list: len(word_list))\n",
    "training_data_8[\"word_count\"] = training_data_8[\"word_tok\"].apply(lambda word_list: len(word_list))\n",
    "training_data_8[\"word_count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Guiraud\n",
    "\n",
    "Formula from (Mellor, 2010)\n",
    "\n",
    "* Va - number of advanced word types\n",
    "\n",
    "        Ag = Va / sqrt(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_list(token_list, list2):\n",
    "    \"\"\"\n",
    "    Number of words that are in both lists-\n",
    "    \"\"\"\n",
    "    return len([token for token in list(set(token_list)) if token in list2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lextutor.ca/freq/lists_download/jacet/jacet1000.txt\n",
    "# first 1000 words of the JAECT wordlist\n",
    "JAECT_words_df = pd.read_excel(\"csv_files/features/JAECT_wordlist.xlsx\")\n",
    "JAECT_wordlist = JAECT_words_df[\"JACET words\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adv_guiraude(x):\n",
    "\n",
    "    # frequent word types\n",
    "    unique_JACET_words = is_in_list(x.word_tok, JAECT_wordlist)\n",
    "\n",
    "    # total word types - frequent (JACET) word types\n",
    "    advanced_word_types = x.unique_words - unique_JACET_words\n",
    "\n",
    "    return advanced_word_types / np.sqrt(x.word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1[\"advanced_guiraud\"] = training_data_1.apply(lambda x: get_adv_guiraude(x), axis=1)\n",
    "training_data_2[\"advanced_guiraud\"] = training_data_2.apply(lambda x: get_adv_guiraude(x), axis=1)\n",
    "training_data_3[\"advanced_guiraud\"] = training_data_3.apply(lambda x: get_adv_guiraude(x), axis=1)\n",
    "training_data_4[\"advanced_guiraud\"] = training_data_4.apply(lambda x: get_adv_guiraude(x), axis=1)\n",
    "training_data_5[\"advanced_guiraud\"] = training_data_5.apply(lambda x: get_adv_guiraude(x), axis=1)\n",
    "training_data_6[\"advanced_guiraud\"] = training_data_6.apply(lambda x: get_adv_guiraude(x), axis=1)\n",
    "training_data_7[\"advanced_guiraud\"] = training_data_7.apply(lambda x: get_adv_guiraude(x), axis=1)\n",
    "training_data_8[\"advanced_guiraud\"] = training_data_8.apply(lambda x: get_adv_guiraude(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar and spelling errors\n",
    "\n",
    "\n",
    "Number of spellchecking errors\n",
    "\n",
    "http://pyenchant.github.io/pyenchant/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_errors(text, language):\n",
    "    \"\"\"\n",
    "    Find grammar errors in a text or sentence.\n",
    "    input: a string of text\n",
    "    output: a list of error types\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # check each sentence for grammatical errors\n",
    "        tool = language_tool_python.LanguageToolPublicAPI(language) # use the public API\n",
    "        matches = tool.check(text)\n",
    "        tool.close() # explicitly shut off the LanguageTool\n",
    "        \n",
    "    # handle exception\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    # list all found error types\n",
    "    specific_type_of_mistake = []\n",
    "    for rules in matches:\n",
    "        if len(rules.replacements) > 0:\n",
    "            specific_type_of_mistake.append([rules.ruleIssueType, rules.ruleId])\n",
    "\n",
    "    return list(filter(None, specific_type_of_mistake))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error type count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_type_count(dict, error_type):\n",
    "    \"\"\"\n",
    "    Applied to a column in a data frame where pos_tag_count_dict was applied before.\n",
    "    Input: \n",
    "        pos_dict: a dictionary with pos tag as key and frequency as value\n",
    "        pos_tag: a string\n",
    "    \"\"\"\n",
    "\n",
    "    for key, value in dict.items():\n",
    "        if key == error_type:\n",
    "            return value\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      2\n",
       "2      0\n",
       "3      4\n",
       "4      0\n",
       "      ..\n",
       "718    0\n",
       "719    0\n",
       "720    2\n",
       "721    0\n",
       "722    1\n",
       "Name: nr_grammar_errors, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"nr_grammar_errors\"] = training_data_1[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"grammar\"))\n",
    "training_data_2[\"nr_grammar_errors\"] = training_data_2[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"grammar\"))\n",
    "training_data_3[\"nr_grammar_errors\"] = training_data_3[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"grammar\"))\n",
    "training_data_4[\"nr_grammar_errors\"] = training_data_4[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"grammar\"))\n",
    "training_data_5[\"nr_grammar_errors\"] = training_data_5[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"grammar\"))\n",
    "training_data_6[\"nr_grammar_errors\"] = training_data_6[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"grammar\"))\n",
    "training_data_7[\"nr_grammar_errors\"] = training_data_7[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"grammar\"))\n",
    "training_data_8[\"nr_grammar_errors\"] = training_data_8[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"grammar\"))\n",
    "training_data_8[\"nr_grammar_errors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        6\n",
       "1       10\n",
       "2        2\n",
       "3       12\n",
       "4       10\n",
       "        ..\n",
       "1778     8\n",
       "1779     7\n",
       "1780     7\n",
       "1781     0\n",
       "1782     5\n",
       "Name: error_types, Length: 1783, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"misspelling\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      14\n",
       "1       7\n",
       "2       9\n",
       "3       5\n",
       "4      28\n",
       "       ..\n",
       "718     7\n",
       "719    11\n",
       "720     5\n",
       "721    10\n",
       "722     2\n",
       "Name: nr_spelling_errors, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"nr_spelling_errors\"] = training_data_1[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"misspelling\"))\n",
    "training_data_2[\"nr_spelling_errors\"] = training_data_2[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"misspelling\"))\n",
    "training_data_3[\"nr_spelling_errors\"] = training_data_3[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"misspelling\"))\n",
    "training_data_4[\"nr_spelling_errors\"] = training_data_4[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"misspelling\"))\n",
    "training_data_5[\"nr_spelling_errors\"] = training_data_5[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"misspelling\"))\n",
    "training_data_6[\"nr_spelling_errors\"] = training_data_6[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"misspelling\"))\n",
    "training_data_7[\"nr_spelling_errors\"] = training_data_7[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"misspelling\"))\n",
    "training_data_8[\"nr_spelling_errors\"] = training_data_8[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][0] for error in error_list]), \"misspelling\"))\n",
    "training_data_8[\"nr_spelling_errors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capitalization errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       4\n",
       "1      27\n",
       "2      34\n",
       "3      24\n",
       "4      19\n",
       "       ..\n",
       "718    48\n",
       "719    25\n",
       "720    34\n",
       "721    33\n",
       "722    18\n",
       "Name: nr_capitalization_errors, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"nr_capitalization_errors\"] = training_data_1[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][1] for error in error_list]), \"UPPERCASE_SENTENCE_START\"))\n",
    "training_data_2[\"nr_capitalization_errors\"] = training_data_2[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][1] for error in error_list]), \"UPPERCASE_SENTENCE_START\"))\n",
    "training_data_3[\"nr_capitalization_errors\"] = training_data_3[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][1] for error in error_list]), \"UPPERCASE_SENTENCE_START\"))\n",
    "training_data_4[\"nr_capitalization_errors\"] = training_data_4[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][1] for error in error_list]), \"UPPERCASE_SENTENCE_START\"))\n",
    "training_data_5[\"nr_capitalization_errors\"] = training_data_5[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][1] for error in error_list]), \"UPPERCASE_SENTENCE_START\"))\n",
    "training_data_6[\"nr_capitalization_errors\"] = training_data_6[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][1] for error in error_list]), \"UPPERCASE_SENTENCE_START\"))\n",
    "training_data_7[\"nr_capitalization_errors\"] = training_data_7[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][1] for error in error_list]), \"UPPERCASE_SENTENCE_START\"))\n",
    "training_data_8[\"nr_capitalization_errors\"] = training_data_8[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][1] for error in error_list]), \"UPPERCASE_SENTENCE_START\"))\n",
    "training_data_8[\"nr_capitalization_errors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      17\n",
       "1      13\n",
       "2       9\n",
       "3       4\n",
       "4       3\n",
       "       ..\n",
       "718     2\n",
       "719     4\n",
       "720     3\n",
       "721     1\n",
       "722     1\n",
       "Name: nr_punctuation_errors, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"nr_punctuation_errors\"] = training_data_1[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][1] for error in error_list]), \"AI_HYDRA_LEO_MISSING_COMMA\"))\n",
    "training_data_2[\"nr_punctuation_errors\"] = training_data_2[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][1] for error in error_list]), \"AI_HYDRA_LEO_MISSING_COMMA\"))\n",
    "training_data_3[\"nr_punctuation_errors\"] = training_data_3[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][1] for error in error_list]), \"AI_HYDRA_LEO_MISSING_COMMA\"))\n",
    "training_data_4[\"nr_punctuation_errors\"] = training_data_4[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][1] for error in error_list]), \"AI_HYDRA_LEO_MISSING_COMMA\"))\n",
    "training_data_5[\"nr_punctuation_errors\"] = training_data_5[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][1] for error in error_list]), \"AI_HYDRA_LEO_MISSING_COMMA\"))\n",
    "training_data_6[\"nr_punctuation_errors\"] = training_data_6[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][1] for error in error_list]), \"AI_HYDRA_LEO_MISSING_COMMA\"))\n",
    "training_data_7[\"nr_punctuation_errors\"] = training_data_7[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][1] for error in error_list]), \"AI_HYDRA_LEO_MISSING_COMMA\"))\n",
    "training_data_8[\"nr_punctuation_errors\"] = training_data_8[\"error_types\"].apply(lambda error_list: error_type_count(Counter([error[0][1] for error in error_list]), \"AI_HYDRA_LEO_MISSING_COMMA\"))\n",
    "training_data_8[\"nr_punctuation_errors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content features (cosine similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score point level for maximum cosine similarity over all score points\n",
    "\n",
    "Calculate the cosine similarity of one thesis with all other theses and chose the grade as feature for which the cosine similarity is highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_data_1[\"stemmed_no_stopwords_string\"] = training_data_1[\"stemmed_no_stopwords\"].apply(lambda word_list: \" \".join(word_list))\\ntraining_data_2[\"stemmed_no_stopwords_string\"] = training_data_2[\"stemmed_no_stopwords\"].apply(lambda word_list: \" \".join(word_list))\\ntraining_data_3[\"stemmed_no_stopwords_string\"] = training_data_3[\"stemmed_no_stopwords\"].apply(lambda word_list: \" \".join(word_list))\\ntraining_data_4[\"stemmed_no_stopwords_string\"] = training_data_4[\"stemmed_no_stopwords\"].apply(lambda word_list: \" \".join(word_list))\\ntraining_data_5[\"stemmed_no_stopwords_string\"] = training_data_5[\"stemmed_no_stopwords\"].apply(lambda word_list: \" \".join(word_list))\\ntraining_data_6[\"stemmed_no_stopwords_string\"] = training_data_6[\"stemmed_no_stopwords\"].apply(lambda word_list: \" \".join(word_list))\\ntraining_data_7[\"stemmed_no_stopwords_string\"] = training_data_7[\"stemmed_no_stopwords\"].apply(lambda word_list: \" \".join(word_list))\\ntraining_data_8[\"stemmed_no_stopwords_string\"] = training_data_8[\"stemmed_no_stopwords\"].apply(lambda word_list: \" \".join(word_list))\\ntraining_data_8[\"stemmed_no_stopwords_string\"].iloc[0][:100]'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"training_data_1[\"stemmed_no_stopwords_string\"] = training_data_1[\"stemmed_no_stopwords\"].apply(lambda word_list: \" \".join(word_list))\n",
    "training_data_2[\"stemmed_no_stopwords_string\"] = training_data_2[\"stemmed_no_stopwords\"].apply(lambda word_list: \" \".join(word_list))\n",
    "training_data_3[\"stemmed_no_stopwords_string\"] = training_data_3[\"stemmed_no_stopwords\"].apply(lambda word_list: \" \".join(word_list))\n",
    "training_data_4[\"stemmed_no_stopwords_string\"] = training_data_4[\"stemmed_no_stopwords\"].apply(lambda word_list: \" \".join(word_list))\n",
    "training_data_5[\"stemmed_no_stopwords_string\"] = training_data_5[\"stemmed_no_stopwords\"].apply(lambda word_list: \" \".join(word_list))\n",
    "training_data_6[\"stemmed_no_stopwords_string\"] = training_data_6[\"stemmed_no_stopwords\"].apply(lambda word_list: \" \".join(word_list))\n",
    "training_data_7[\"stemmed_no_stopwords_string\"] = training_data_7[\"stemmed_no_stopwords\"].apply(lambda word_list: \" \".join(word_list))\n",
    "training_data_8[\"stemmed_no_stopwords_string\"] = training_data_8[\"stemmed_no_stopwords\"].apply(lambda word_list: \" \".join(word_list))\n",
    "training_data_8[\"stemmed_no_stopwords_string\"].iloc[0][:100]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# code from: https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings\n",
    "\n",
    "WORD = re.compile(r\"\\w+\")\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)\n",
    "\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "\n",
    "    # select words in both vectors\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      {'a': 13, 'long': 1, 'time': 4, 'ago': 1, 'whe...\n",
       "1      {'softball': 7, 'has': 2, 'to': 24, 'be': 2, '...\n",
       "2      {'some': 4, 'people': 12, 'like': 5, 'making':...\n",
       "3      {'laughter': 1, 'caps1': 11, 'i': 27, 'hang': ...\n",
       "4      {'well': 3, 'ima': 3, 'tell': 1, 'a': 3, 'stor...\n",
       "                             ...                        \n",
       "718    {'in': 7, 'most': 1, 'stories': 1, 'mothers': ...\n",
       "719    {'i': 15, 'never': 1, 'understood': 1, 'the': ...\n",
       "720    {'when': 3, 'you': 6, 'laugh': 4, 'is': 6, 'ca...\n",
       "721    {'trippin': 1, 'on': 6, 'fences': 1, 'i': 32, ...\n",
       "722    {'many': 1, 'people': 9, 'believe': 1, 'that':...\n",
       "Name: text_vector, Length: 723, dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1['text_vector'] = training_data_1[\"normalised_docs\"].apply(lambda x: text_to_vector(x)) \n",
    "training_data_2['text_vector'] = training_data_2[\"normalised_docs\"].apply(lambda x: text_to_vector(x)) \n",
    "training_data_3['text_vector'] = training_data_3[\"normalised_docs\"].apply(lambda x: text_to_vector(x)) \n",
    "training_data_4['text_vector'] = training_data_4[\"normalised_docs\"].apply(lambda x: text_to_vector(x)) \n",
    "training_data_5['text_vector'] = training_data_5[\"normalised_docs\"].apply(lambda x: text_to_vector(x)) \n",
    "training_data_6['text_vector'] = training_data_6[\"normalised_docs\"].apply(lambda x: text_to_vector(x)) \n",
    "training_data_7['text_vector'] = training_data_7[\"normalised_docs\"].apply(lambda x: text_to_vector(x)) \n",
    "training_data_8['text_vector'] = training_data_8[\"normalised_docs\"].apply(lambda x: text_to_vector(x)) \n",
    "training_data_8['text_vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_sentence_len_words</th>\n",
       "      <th>dale_chall_readability</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>word_count</th>\n",
       "      <th>advanced_guiraud</th>\n",
       "      <th>nr_grammar_errors</th>\n",
       "      <th>nr_spelling_errors</th>\n",
       "      <th>nr_capitalization_errors</th>\n",
       "      <th>nr_punctuation_errors</th>\n",
       "      <th>text_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2978</td>\n",
       "      <td>2</td>\n",
       "      <td>Certain materials being removed from libraries...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.278400</td>\n",
       "      <td>206</td>\n",
       "      <td>485</td>\n",
       "      <td>3.678021</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>{'certain': 3, 'materials': 3, 'being': 2, 're...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2979</td>\n",
       "      <td>2</td>\n",
       "      <td>Write a persuasive essay to a newspaper reflec...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.298697</td>\n",
       "      <td>81</td>\n",
       "      <td>170</td>\n",
       "      <td>2.530984</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'write': 2, 'a': 3, 'persuasive': 1, 'essay':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2980</td>\n",
       "      <td>2</td>\n",
       "      <td>Do you think that libraries should remove cert...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.617494</td>\n",
       "      <td>118</td>\n",
       "      <td>229</td>\n",
       "      <td>2.577193</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>{'do': 3, 'you': 5, 'think': 2, 'that': 5, 'li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2981</td>\n",
       "      <td>2</td>\n",
       "      <td>In @DATE1's world, there are many things found...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.447982</td>\n",
       "      <td>186</td>\n",
       "      <td>470</td>\n",
       "      <td>3.459492</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'in': 9, 'date1': 1, 's': 4, 'world': 4, 'the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2982</td>\n",
       "      <td>2</td>\n",
       "      <td>In life you have the 'offensive things'. The l...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.008232</td>\n",
       "      <td>224</td>\n",
       "      <td>433</td>\n",
       "      <td>4.661523</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>{'in': 4, 'life': 6, 'you': 10, 'have': 3, 'th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>4773</td>\n",
       "      <td>2</td>\n",
       "      <td>The author is writting about taking books off ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.846269</td>\n",
       "      <td>104</td>\n",
       "      <td>271</td>\n",
       "      <td>2.612064</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'the': 18, 'author': 1, 'is': 1, 'writting': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>4774</td>\n",
       "      <td>2</td>\n",
       "      <td>I do not think that materials, such as books, ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.638717</td>\n",
       "      <td>129</td>\n",
       "      <td>248</td>\n",
       "      <td>3.175003</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>{'i': 3, 'do': 1, 'not': 3, 'think': 3, 'that'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>4775</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes we should keep the books,music,movies,an m...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.054034</td>\n",
       "      <td>68</td>\n",
       "      <td>118</td>\n",
       "      <td>1.841149</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>{'yes': 2, 'we': 5, 'should': 4, 'keep': 2, 't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>4776</td>\n",
       "      <td>2</td>\n",
       "      <td>I do believe that  book, magazines, music, mov...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.854254</td>\n",
       "      <td>174</td>\n",
       "      <td>507</td>\n",
       "      <td>3.242044</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>{'i': 6, 'do': 4, 'believe': 1, 'that': 4, 'bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>4777</td>\n",
       "      <td>2</td>\n",
       "      <td>Different Then Everyone Else     @CAPS1 do peo...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.794554</td>\n",
       "      <td>169</td>\n",
       "      <td>488</td>\n",
       "      <td>3.123483</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>{'different': 4, 'then': 4, 'everyone': 4, 'el...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id  essay_set                                              essay  \\\n",
       "0         2978          2  Certain materials being removed from libraries...   \n",
       "1         2979          2  Write a persuasive essay to a newspaper reflec...   \n",
       "2         2980          2  Do you think that libraries should remove cert...   \n",
       "3         2981          2  In @DATE1's world, there are many things found...   \n",
       "4         2982          2  In life you have the 'offensive things'. The l...   \n",
       "...        ...        ...                                                ...   \n",
       "1795      4773          2  The author is writting about taking books off ...   \n",
       "1796      4774          2  I do not think that materials, such as books, ...   \n",
       "1797      4775          2  Yes we should keep the books,music,movies,an m...   \n",
       "1798      4776          2  I do believe that  book, magazines, music, mov...   \n",
       "1799      4777          2  Different Then Everyone Else     @CAPS1 do peo...   \n",
       "\n",
       "      rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0                4.0             4.0             NaN            4.0   \n",
       "1                1.0             2.0             NaN            1.0   \n",
       "2                2.0             3.0             NaN            2.0   \n",
       "3                4.0             4.0             NaN            4.0   \n",
       "4                4.0             4.0             NaN            4.0   \n",
       "...              ...             ...             ...            ...   \n",
       "1795             3.0             2.0             NaN            3.0   \n",
       "1796             3.0             3.0             NaN            3.0   \n",
       "1797             2.0             2.0             NaN            2.0   \n",
       "1798             3.0             4.0             NaN            3.0   \n",
       "1799             3.0             3.0             NaN            3.0   \n",
       "\n",
       "      rater1_domain2  rater2_domain2  domain2_score  ...  \\\n",
       "0                4.0             4.0            4.0  ...   \n",
       "1                1.0             2.0            1.0  ...   \n",
       "2                3.0             3.0            3.0  ...   \n",
       "3                4.0             4.0            4.0  ...   \n",
       "4                4.0             4.0            4.0  ...   \n",
       "...              ...             ...            ...  ...   \n",
       "1795             3.0             2.0            3.0  ...   \n",
       "1796             3.0             3.0            3.0  ...   \n",
       "1797             2.0             2.0            2.0  ...   \n",
       "1798             4.0             3.0            4.0  ...   \n",
       "1799             2.0             3.0            2.0  ...   \n",
       "\n",
       "      avg_sentence_len_words  dale_chall_readability  unique_words  \\\n",
       "0                       14.0                8.278400           206   \n",
       "1                       15.0                9.298697            81   \n",
       "2                        8.0                8.617494           118   \n",
       "3                        9.0                7.447982           186   \n",
       "4                        8.0                9.008232           224   \n",
       "...                      ...                     ...           ...   \n",
       "1795                    15.0                9.846269           104   \n",
       "1796                     8.0                8.638717           129   \n",
       "1797                    16.0                8.054034            68   \n",
       "1798                    11.0                6.854254           174   \n",
       "1799                    15.0                7.794554           169   \n",
       "\n",
       "      word_count  advanced_guiraud  nr_grammar_errors  nr_spelling_errors  \\\n",
       "0            485          3.678021                  0                   7   \n",
       "1            170          2.530984                  0                   2   \n",
       "2            229          2.577193                  2                   5   \n",
       "3            470          3.459492                  0                   1   \n",
       "4            433          4.661523                  1                  15   \n",
       "...          ...               ...                ...                 ...   \n",
       "1795         271          2.612064                  1                   8   \n",
       "1796         248          3.175003                  1                   6   \n",
       "1797         118          1.841149                  1                   2   \n",
       "1798         507          3.242044                  2                   4   \n",
       "1799         488          3.123483                  6                   9   \n",
       "\n",
       "      nr_capitalization_errors  nr_punctuation_errors  \\\n",
       "0                           12                      2   \n",
       "1                            2                      1   \n",
       "2                            4                      0   \n",
       "3                           20                      1   \n",
       "4                           15                      3   \n",
       "...                        ...                    ...   \n",
       "1795                         3                      0   \n",
       "1796                         7                      3   \n",
       "1797                         2                      0   \n",
       "1798                        13                      6   \n",
       "1799                         8                      0   \n",
       "\n",
       "                                            text_vector  \n",
       "0     {'certain': 3, 'materials': 3, 'being': 2, 're...  \n",
       "1     {'write': 2, 'a': 3, 'persuasive': 1, 'essay':...  \n",
       "2     {'do': 3, 'you': 5, 'think': 2, 'that': 5, 'li...  \n",
       "3     {'in': 9, 'date1': 1, 's': 4, 'world': 4, 'the...  \n",
       "4     {'in': 4, 'life': 6, 'you': 10, 'have': 3, 'th...  \n",
       "...                                                 ...  \n",
       "1795  {'the': 18, 'author': 1, 'is': 1, 'writting': ...  \n",
       "1796  {'i': 3, 'do': 1, 'not': 3, 'think': 3, 'that'...  \n",
       "1797  {'yes': 2, 'we': 5, 'should': 4, 'keep': 2, 't...  \n",
       "1798  {'i': 6, 'do': 4, 'believe': 1, 'that': 4, 'bo...  \n",
       "1799  {'different': 4, 'then': 4, 'everyone': 4, 'el...  \n",
       "\n",
       "[1800 rows x 60 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two grades to predict\n",
    "training_data_2_2 = training_data_2.copy()\n",
    "training_data_2_2[\"grade\"] = training_data_2_2[\"domain2_score\"].copy()\n",
    "training_data_2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grade_feature(df):\n",
    "\n",
    "    grade_as_feature = []\n",
    "\n",
    "    for current_row_index, current_row in df.iterrows():\n",
    "\n",
    "        max_cosine_sim = 0\n",
    "        corresponding_grade = 0\n",
    "    \n",
    "        for i, val in df.iterrows():\n",
    "\n",
    "            # do not calculate cosine similarity of the same thesis\n",
    "            if i != current_row_index:\n",
    "                cosine_sim = get_cosine(current_row.text_vector, val.text_vector)\n",
    "\n",
    "                # update max cosine similarity\n",
    "                if max_cosine_sim < cosine_sim:\n",
    "                    max_cosine_sim = cosine_sim\n",
    "                    corresponding_grade = val.grade\n",
    "\n",
    "        grade_as_feature.append(corresponding_grade)\n",
    "\n",
    "    return grade_as_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      40.0\n",
       "1      36.0\n",
       "2      30.0\n",
       "3      34.0\n",
       "4      36.0\n",
       "       ... \n",
       "718    40.0\n",
       "719    35.0\n",
       "720    32.0\n",
       "721    45.0\n",
       "722    40.0\n",
       "Name: grade_as_feature, Length: 723, dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"grade_as_feature\"] = get_grade_feature(training_data_1)\n",
    "training_data_2[\"grade_as_feature\"] = get_grade_feature(training_data_2)\n",
    "training_data_3[\"grade_as_feature\"] = get_grade_feature(training_data_3)\n",
    "training_data_4[\"grade_as_feature\"] = get_grade_feature(training_data_4)\n",
    "training_data_5[\"grade_as_feature\"] = get_grade_feature(training_data_5)\n",
    "training_data_6[\"grade_as_feature\"] = get_grade_feature(training_data_6)\n",
    "training_data_7[\"grade_as_feature\"] = get_grade_feature(training_data_7)\n",
    "training_data_8[\"grade_as_feature\"] = get_grade_feature(training_data_8)\n",
    "training_data_8[\"grade_as_feature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_2_2[\"grade_as_feature\"] = get_grade_feature(training_data_2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity with essays that have highest score point level\n",
    "\n",
    "Calculate cosine similarity with theses of grade 9.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_cosine_similarity_feature(df):\n",
    "    \"\"\"\n",
    "    The cosine similarity of each thesis is calculated with all the theses of the highest grade. \n",
    "    The cosine simularity for theses with the highest grade is calculated with itself and other theses with the highest grade.\n",
    "    Code from: https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings\n",
    "    \"\"\"\n",
    "\n",
    "    # save highest grade\n",
    "    max_grade = df.grade.max()\n",
    "\n",
    "    # seperate highest grade from other grades\n",
    "    grouped_grade_df = df.groupby(\"grade\").get_group(max_grade)\n",
    "    cosine_sim_all = []\n",
    "\n",
    "    # loop through df\n",
    "    for i, val in df.iterrows():\n",
    "\n",
    "        cosine_sim_inbetween = []\n",
    "\n",
    "        # calculate cosine similarity of each row in df with all high grade theses\n",
    "        for j, high_grade_val in grouped_grade_df.iterrows():\n",
    "            cosine_sim = get_cosine(high_grade_val.text_vector, val.text_vector)\n",
    "            cosine_sim_inbetween.append(cosine_sim)\n",
    "\n",
    "        cosine_sim_all.append(cosine_sim_inbetween)\n",
    "\n",
    "    return np.array(cosine_sim_all).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.674637\n",
       "1      0.674637\n",
       "2      0.674637\n",
       "3      0.674637\n",
       "4      0.674637\n",
       "         ...   \n",
       "718    0.674637\n",
       "719    0.674637\n",
       "720    0.674637\n",
       "721    0.674637\n",
       "722    0.674637\n",
       "Name: avg_cosine_similariy_high_grade, Length: 723, dtype: float64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"avg_cosine_similariy_high_grade\"] = avg_cosine_similarity_feature(training_data_1)\n",
    "training_data_2[\"avg_cosine_similariy_high_grade\"] = avg_cosine_similarity_feature(training_data_2)\n",
    "training_data_3[\"avg_cosine_similariy_high_grade\"] = avg_cosine_similarity_feature(training_data_3)\n",
    "training_data_4[\"avg_cosine_similariy_high_grade\"] = avg_cosine_similarity_feature(training_data_4)\n",
    "training_data_5[\"avg_cosine_similariy_high_grade\"] = avg_cosine_similarity_feature(training_data_5)\n",
    "training_data_6[\"avg_cosine_similariy_high_grade\"] = avg_cosine_similarity_feature(training_data_6)\n",
    "training_data_7[\"avg_cosine_similariy_high_grade\"] = avg_cosine_similarity_feature(training_data_7)\n",
    "training_data_8[\"avg_cosine_similariy_high_grade\"] = avg_cosine_similarity_feature(training_data_8)\n",
    "training_data_8[\"avg_cosine_similariy_high_grade\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_2_2[\"avg_cosine_similariy_high_grade\"] = avg_cosine_similarity_feature(training_data_2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pattern cosine\n",
    "\n",
    "(Attali, 2011)\n",
    "\n",
    "1. Divid k grades\n",
    "1. Calculate cosine similarity with other theses and rank from highest to lowest for each grade\n",
    "1. pattern_cosine_feature = sum_i_to_k(grade_i * ranking_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_cosine_feature(df):\n",
    "\n",
    "    pattern_cosine_feature = []\n",
    "    #grades = [6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0, 9.5]\n",
    "    grades = np.sort(df.grade.unique()).tolist()\n",
    "    df_grouped = df.groupby(\"grade\")\n",
    "\n",
    "    # grouping by grades\n",
    "    for j, elem in enumerate(grades):\n",
    "\n",
    "        cosine_sim = []\n",
    "\n",
    "        # calculate cosine similarity\n",
    "        for current_row_index, current_row in df_grouped.get_group(elem).iterrows():\n",
    "\n",
    "            for i, val in df_grouped.get_group(elem).iterrows():\n",
    "                \n",
    "                # do not calculate cosine similarity of the same thesis\n",
    "                if i != current_row_index:\n",
    "                    cosine_sim.append(get_cosine(current_row.text_vector, val.text_vector))\n",
    "\n",
    "            sorted_cosine = sorted(cosine_sim, key = float, reverse = True)\n",
    "            pattern_cosine_feature.append(sum(elem * np.array(sorted_cosine)))\n",
    "    \n",
    "    return pattern_cosine_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.000000\n",
       "1         0.000000\n",
       "2        18.908606\n",
       "3        31.268074\n",
       "4        50.430460\n",
       "          ...     \n",
       "718    5194.687136\n",
       "719    5626.579700\n",
       "720      43.200876\n",
       "721      86.401752\n",
       "722       0.000000\n",
       "Name: pattern_cosine, Length: 723, dtype: float64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"pattern_cosine\"] = pattern_cosine_feature(training_data_1)\n",
    "training_data_2[\"pattern_cosine\"] = pattern_cosine_feature(training_data_2)\n",
    "training_data_3[\"pattern_cosine\"] = pattern_cosine_feature(training_data_3)\n",
    "training_data_4[\"pattern_cosine\"] = pattern_cosine_feature(training_data_4)\n",
    "training_data_5[\"pattern_cosine\"] = pattern_cosine_feature(training_data_5)\n",
    "training_data_6[\"pattern_cosine\"] = pattern_cosine_feature(training_data_6)\n",
    "training_data_7[\"pattern_cosine\"] = pattern_cosine_feature(training_data_7)\n",
    "training_data_8[\"pattern_cosine\"] = pattern_cosine_feature(training_data_8)\n",
    "training_data_8[\"pattern_cosine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_2_2[\"pattern_cosine\"] = pattern_cosine_feature(training_data_2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted sum of all cosine correlation values\n",
    "\n",
    "(Attali, 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  1.,  1.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math \n",
    "#grades = np.sort(english_theses.grade.unique()).tolist()\n",
    "grades = [6.0, 6.5, 7.0]\n",
    "\n",
    "#len_1 = int(math.ceil(len(grades)/2))\n",
    "np.concatenate([(np.ones((1, int(len(grades)/2))) * - 1), np.ones((1, int(math.ceil(len(grades)/2))))], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_cosine_feature(df):\n",
    "\n",
    "    weighted_cosine_feature = []\n",
    "    #grades = [6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0, 9.5]\n",
    "    #weights = [-1, -1, -1, -1, 1, 1, 1, 1]\n",
    "    grades = np.sort(df.grade.unique()).tolist()\n",
    "    weights = np.concatenate([(np.ones((1, int(len(grades)/2))) * - 1), np.ones((1, int(math.ceil(len(grades)/2))))], axis = 1).tolist()[0]\n",
    "\n",
    "    df_grouped = df.groupby(\"grade\")\n",
    "\n",
    "    # grouping by grades\n",
    "    for i, elem in enumerate(grades):\n",
    "\n",
    "        cosine_sim = []\n",
    "\n",
    "        # calculate cosine similarity\n",
    "        for current_row_index, current_row in df_grouped.get_group(elem).iterrows():\n",
    "\n",
    "            for j, val in df_grouped.get_group(elem).iterrows():\n",
    "                # do not calculate cosine similarity of the same thesis\n",
    "                if j != current_row_index:\n",
    "                    cosine_sim.append(weights[i] * get_cosine(current_row.text_vector, val.text_vector))\n",
    "\n",
    "            sorted_cosine = sorted(cosine_sim, key = float, reverse = True)\n",
    "            weighted_cosine_feature.append(sum(elem * np.array(sorted_cosine)))\n",
    "    \n",
    "    return weighted_cosine_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.000000\n",
       "1         0.000000\n",
       "2       -18.908606\n",
       "3       -31.268074\n",
       "4       -50.430460\n",
       "          ...     \n",
       "718    5194.687136\n",
       "719    5626.579700\n",
       "720      43.200876\n",
       "721      86.401752\n",
       "722       0.000000\n",
       "Name: weighted_cosine, Length: 723, dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"weighted_cosine\"] = weighted_cosine_feature(training_data_1)\n",
    "training_data_2[\"weighted_cosine\"] = weighted_cosine_feature(training_data_2)\n",
    "training_data_3[\"weighted_cosine\"] = weighted_cosine_feature(training_data_3)\n",
    "training_data_4[\"weighted_cosine\"] = weighted_cosine_feature(training_data_4)\n",
    "training_data_5[\"weighted_cosine\"] = weighted_cosine_feature(training_data_5)\n",
    "training_data_6[\"weighted_cosine\"] = weighted_cosine_feature(training_data_6)\n",
    "training_data_7[\"weighted_cosine\"] = weighted_cosine_feature(training_data_7)\n",
    "training_data_8[\"weighted_cosine\"] = weighted_cosine_feature(training_data_8)\n",
    "training_data_8[\"weighted_cosine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_2_2[\"weighted_cosine\"] = weighted_cosine_feature(training_data_2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical sophistication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_count(text, no_spaces = True):\n",
    "    \"\"\" \n",
    "    Number of characters with or without spaces\n",
    "    \"\"\"\n",
    "    if no_spaces:\n",
    "        # chars without spaces\n",
    "        return len([ele for ele in text if not ele == \" \"])\n",
    "    else:\n",
    "        # chars with spaces\n",
    "        return len([ele for ele in text])  \n",
    "    #return len([ele for ele in text if ele.isalpha()]) <- only letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2741\n",
       "1      3180\n",
       "2      3547\n",
       "3      2692\n",
       "4      2299\n",
       "       ... \n",
       "718    3612\n",
       "719    2269\n",
       "720    3780\n",
       "721    2492\n",
       "722    2059\n",
       "Name: char_count, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"char_count\"] = training_data_1[\"normalised_docs\"].apply(char_count)\n",
    "training_data_2[\"char_count\"] = training_data_2[\"normalised_docs\"].apply(char_count)\n",
    "training_data_3[\"char_count\"] = training_data_3[\"normalised_docs\"].apply(char_count)\n",
    "training_data_4[\"char_count\"] = training_data_4[\"normalised_docs\"].apply(char_count)\n",
    "training_data_5[\"char_count\"] = training_data_5[\"normalised_docs\"].apply(char_count)\n",
    "training_data_6[\"char_count\"] = training_data_6[\"normalised_docs\"].apply(char_count)\n",
    "training_data_7[\"char_count\"] = training_data_7[\"normalised_docs\"].apply(char_count)\n",
    "training_data_8[\"char_count\"] = training_data_8[\"normalised_docs\"].apply(char_count)\n",
    "training_data_8[\"char_count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [1, 4, 4, 3, 4, 1, 3, 2, 5, 5, 1, 3, 1, 6, 7, ...\n",
       "1      [8, 3, 2, 2, 3, 2, 3, 6, 4, 8, 6, 5, 7, 8, 2, ...\n",
       "2      [4, 6, 4, 6, 6, 5, 1, 4, 2, 1, 4, 2, 4, 8, 2, ...\n",
       "3      [8, 5, 1, 4, 3, 4, 2, 7, 3, 3, 5, 2, 2, 4, 2, ...\n",
       "4      [4, 1, 1, 1, 4, 1, 5, 5, 3, 4, 1, 3, 5, 4, 4, ...\n",
       "                             ...                        \n",
       "718    [2, 4, 7, 7, 3, 9, 3, 6, 7, 2, 7, 2, 6, 5, 7, ...\n",
       "719    [1, 5, 10, 3, 7, 8, 2, 3, 8, 8, 7, 3, 6, 4, 1,...\n",
       "720    [4, 3, 5, 2, 5, 3, 2, 5, 2, 2, 5, 5, 4, 3, 3, ...\n",
       "721    [7, 2, 6, 1, 2, 4, 5, 5, 3, 2, 5, 5, 4, 5, 3, ...\n",
       "722    [4, 6, 7, 4, 8, 3, 7, 4, 4, 8, 4, 3, 10, 4, 4,...\n",
       "Name: word_length, Length: 723, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"word_length\"] = training_data_1[\"word_tok\"].apply(lambda word_list: [len(word) for word in word_list])\n",
    "training_data_2[\"word_length\"] = training_data_2[\"word_tok\"].apply(lambda word_list: [len(word) for word in word_list])\n",
    "training_data_3[\"word_length\"] = training_data_3[\"word_tok\"].apply(lambda word_list: [len(word) for word in word_list])\n",
    "training_data_4[\"word_length\"] = training_data_4[\"word_tok\"].apply(lambda word_list: [len(word) for word in word_list])\n",
    "training_data_5[\"word_length\"] = training_data_5[\"word_tok\"].apply(lambda word_list: [len(word) for word in word_list])\n",
    "training_data_6[\"word_length\"] = training_data_6[\"word_tok\"].apply(lambda word_list: [len(word) for word in word_list])\n",
    "training_data_7[\"word_length\"] = training_data_7[\"word_tok\"].apply(lambda word_list: [len(word) for word in word_list])\n",
    "training_data_8[\"word_length\"] = training_data_8[\"word_tok\"].apply(lambda word_list: [len(word) for word in word_list])\n",
    "training_data_8[\"word_length\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_length(tok_words):\n",
    "    \"\"\" \n",
    "    input: str (token)\n",
    "    output: int (avg word length of text)\n",
    "    Works on tokenized_text column after tokenization() was applied to the data frame. \n",
    "    Empty reviews in tokenized_sentences have to be removed otherwise division by zero \"\"\"\n",
    "    if len(tok_words) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return [sum(len(word) for word in tok_words) / len(tok_words)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      3.826215\n",
       "1      3.945223\n",
       "2      3.979094\n",
       "3      3.693820\n",
       "4      3.329359\n",
       "         ...   \n",
       "718    4.002320\n",
       "719    3.865108\n",
       "720    4.294611\n",
       "721    4.109375\n",
       "722    4.181053\n",
       "Name: avg_word_len, Length: 723, dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"avg_word_len\"] = training_data_1[\"word_tok\"].apply(average_word_length)\n",
    "training_data_2[\"avg_word_len\"] = training_data_2[\"word_tok\"].apply(average_word_length)\n",
    "training_data_3[\"avg_word_len\"] = training_data_3[\"word_tok\"].apply(average_word_length)\n",
    "training_data_4[\"avg_word_len\"] = training_data_4[\"word_tok\"].apply(average_word_length)\n",
    "training_data_5[\"avg_word_len\"] = training_data_5[\"word_tok\"].apply(average_word_length)\n",
    "training_data_6[\"avg_word_len\"] = training_data_6[\"word_tok\"].apply(average_word_length)\n",
    "training_data_7[\"avg_word_len\"] = training_data_7[\"word_tok\"].apply(average_word_length)\n",
    "training_data_8[\"avg_word_len\"] = training_data_8[\"word_tok\"].apply(average_word_length)\n",
    "training_data_8[\"avg_word_len\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of long words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the words based on char length\n",
    "\n",
    "def get_long_words(words, avg_word_len):\n",
    "    \n",
    "    counter = 0\n",
    "    for word in words:\n",
    "        if len(word) > avg_word_len:\n",
    "            counter = counter + 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      335\n",
       "1      444\n",
       "2      447\n",
       "3      349\n",
       "4      313\n",
       "      ... \n",
       "718    296\n",
       "719    304\n",
       "720    308\n",
       "721    188\n",
       "722    163\n",
       "Name: nr_long_words, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"nr_long_words\"] = training_data_1.apply(lambda x: get_long_words(x.word_tok, x.avg_word_len), axis=1)\n",
    "training_data_2[\"nr_long_words\"] = training_data_2.apply(lambda x: get_long_words(x.word_tok, x.avg_word_len), axis=1)\n",
    "training_data_3[\"nr_long_words\"] = training_data_3.apply(lambda x: get_long_words(x.word_tok, x.avg_word_len), axis=1)\n",
    "training_data_4[\"nr_long_words\"] = training_data_4.apply(lambda x: get_long_words(x.word_tok, x.avg_word_len), axis=1)\n",
    "training_data_5[\"nr_long_words\"] = training_data_5.apply(lambda x: get_long_words(x.word_tok, x.avg_word_len), axis=1)\n",
    "training_data_6[\"nr_long_words\"] = training_data_6.apply(lambda x: get_long_words(x.word_tok, x.avg_word_len), axis=1)\n",
    "training_data_7[\"nr_long_words\"] = training_data_7.apply(lambda x: get_long_words(x.word_tok, x.avg_word_len), axis=1)\n",
    "training_data_8[\"nr_long_words\"] = training_data_8.apply(lambda x: get_long_words(x.word_tok, x.avg_word_len), axis=1)\n",
    "training_data_8[\"nr_long_words\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of short words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_short_words(words, avg_word_len):\n",
    "    \n",
    "    counter = 0\n",
    "    for word in words:\n",
    "        if len(word) < avg_word_len:\n",
    "            counter = counter + 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      344\n",
       "1      341\n",
       "2      414\n",
       "3      363\n",
       "4      358\n",
       "      ... \n",
       "718    566\n",
       "719    252\n",
       "720    527\n",
       "721    388\n",
       "722    312\n",
       "Name: nr_short_words, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"nr_short_words\"] = training_data_1.apply(lambda x: get_short_words(x.word_tok, x.avg_word_len), axis=1)\n",
    "training_data_2[\"nr_short_words\"] = training_data_2.apply(lambda x: get_short_words(x.word_tok, x.avg_word_len), axis=1)\n",
    "training_data_3[\"nr_short_words\"] = training_data_3.apply(lambda x: get_short_words(x.word_tok, x.avg_word_len), axis=1)\n",
    "training_data_4[\"nr_short_words\"] = training_data_4.apply(lambda x: get_short_words(x.word_tok, x.avg_word_len), axis=1)\n",
    "training_data_5[\"nr_short_words\"] = training_data_5.apply(lambda x: get_short_words(x.word_tok, x.avg_word_len), axis=1)\n",
    "training_data_6[\"nr_short_words\"] = training_data_6.apply(lambda x: get_short_words(x.word_tok, x.avg_word_len), axis=1)\n",
    "training_data_7[\"nr_short_words\"] = training_data_7.apply(lambda x: get_short_words(x.word_tok, x.avg_word_len), axis=1)\n",
    "training_data_8[\"nr_short_words\"] = training_data_8.apply(lambda x: get_short_words(x.word_tok, x.avg_word_len), axis=1)\n",
    "training_data_8[\"nr_short_words\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most frequent word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      3\n",
       "1      4\n",
       "2      3\n",
       "3      2\n",
       "4      4\n",
       "      ..\n",
       "718    3\n",
       "719    4\n",
       "720    3\n",
       "721    3\n",
       "722    4\n",
       "Name: most_freq_word_length, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"most_freq_word_length\"] = training_data_1[\"word_length\"].apply(lambda word_len_list: max(Counter(word_len_list), key = lambda x: Counter(word_len_list)[x]))\n",
    "training_data_2[\"most_freq_word_length\"] = training_data_2[\"word_length\"].apply(lambda word_len_list: max(Counter(word_len_list), key = lambda x: Counter(word_len_list)[x]))\n",
    "training_data_3[\"most_freq_word_length\"] = training_data_3[\"word_length\"].apply(lambda word_len_list: max(Counter(word_len_list), key = lambda x: Counter(word_len_list)[x]))\n",
    "training_data_4[\"most_freq_word_length\"] = training_data_4[\"word_length\"].apply(lambda word_len_list: max(Counter(word_len_list), key = lambda x: Counter(word_len_list)[x]))\n",
    "training_data_5[\"most_freq_word_length\"] = training_data_5[\"word_length\"].apply(lambda word_len_list: max(Counter(word_len_list), key = lambda x: Counter(word_len_list)[x]))\n",
    "training_data_6[\"most_freq_word_length\"] = training_data_6[\"word_length\"].apply(lambda word_len_list: max(Counter(word_len_list), key = lambda x: Counter(word_len_list)[x]))\n",
    "training_data_7[\"most_freq_word_length\"] = training_data_7[\"word_length\"].apply(lambda word_len_list: max(Counter(word_len_list), key = lambda x: Counter(word_len_list)[x]))\n",
    "training_data_8[\"most_freq_word_length\"] = training_data_8[\"word_length\"].apply(lambda word_len_list: max(Counter(word_len_list), key = lambda x: Counter(word_len_list)[x]))\n",
    "training_data_8[\"most_freq_word_length\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      45\n",
       "1      67\n",
       "2      81\n",
       "3      48\n",
       "4      57\n",
       "       ..\n",
       "718    90\n",
       "719    63\n",
       "720    90\n",
       "721    79\n",
       "722    55\n",
       "Name: nr_sentences, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"nr_sentences\"] = training_data_1[\"sentence_tok\"].apply(lambda sentence_list: len(sentence_list))\n",
    "training_data_2[\"nr_sentences\"] = training_data_2[\"sentence_tok\"].apply(lambda sentence_list: len(sentence_list))\n",
    "training_data_3[\"nr_sentences\"] = training_data_3[\"sentence_tok\"].apply(lambda sentence_list: len(sentence_list))\n",
    "training_data_4[\"nr_sentences\"] = training_data_4[\"sentence_tok\"].apply(lambda sentence_list: len(sentence_list))\n",
    "training_data_5[\"nr_sentences\"] = training_data_5[\"sentence_tok\"].apply(lambda sentence_list: len(sentence_list))\n",
    "training_data_6[\"nr_sentences\"] = training_data_6[\"sentence_tok\"].apply(lambda sentence_list: len(sentence_list))\n",
    "training_data_7[\"nr_sentences\"] = training_data_7[\"sentence_tok\"].apply(lambda sentence_list: len(sentence_list))\n",
    "training_data_8[\"nr_sentences\"] = training_data_8[\"sentence_tok\"].apply(lambda sentence_list: len(sentence_list))\n",
    "training_data_8[\"nr_sentences\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of long sentences (based on words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_long_sentence(sentence_lengths, avg_sentence_len):\n",
    "    \n",
    "    counter = 0\n",
    "    for sentence_len in sentence_lengths:\n",
    "        if sentence_len > avg_sentence_len:\n",
    "            counter = counter + 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       16\n",
       "1       13\n",
       "2        8\n",
       "3       18\n",
       "4       16\n",
       "        ..\n",
       "1778    13\n",
       "1779     8\n",
       "1780    10\n",
       "1781     1\n",
       "1782     4\n",
       "Name: nr_long_sentences, Length: 1783, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"nr_long_sentences\"] = training_data_1.apply(lambda x: get_long_sentence(x.sentence_len_words, x.avg_sentence_len_words), axis=1)\n",
    "training_data_2[\"nr_long_sentences\"] = training_data_2.apply(lambda x: get_long_sentence(x.sentence_len_words, x.avg_sentence_len_words), axis=1)\n",
    "training_data_3[\"nr_long_sentences\"] = training_data_3.apply(lambda x: get_long_sentence(x.sentence_len_words, x.avg_sentence_len_words), axis=1)\n",
    "training_data_4[\"nr_long_sentences\"] = training_data_4.apply(lambda x: get_long_sentence(x.sentence_len_words, x.avg_sentence_len_words), axis=1)\n",
    "training_data_5[\"nr_long_sentences\"] = training_data_5.apply(lambda x: get_long_sentence(x.sentence_len_words, x.avg_sentence_len_words), axis=1)\n",
    "training_data_6[\"nr_long_sentences\"] = training_data_6.apply(lambda x: get_long_sentence(x.sentence_len_words, x.avg_sentence_len_words), axis=1)\n",
    "training_data_7[\"nr_long_sentences\"] = training_data_7.apply(lambda x: get_long_sentence(x.sentence_len_words, x.avg_sentence_len_words), axis=1)\n",
    "training_data_8[\"nr_long_sentences\"] = training_data_8.apply(lambda x: get_long_sentence(x.sentence_len_words, x.avg_sentence_len_words), axis=1)\n",
    "training_data_1[\"nr_long_sentences\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of short sentences (based on words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_short_sentence(sentence_lengths, avg_sentence_len):\n",
    "    \n",
    "    counter = 0\n",
    "    for sentence_len in sentence_lengths:\n",
    "        if sentence_len < avg_sentence_len:\n",
    "            counter = counter + 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      21\n",
       "1      38\n",
       "2      43\n",
       "3      26\n",
       "4      28\n",
       "       ..\n",
       "718    39\n",
       "719    35\n",
       "720    53\n",
       "721    43\n",
       "722    26\n",
       "Name: nr_short_sentences, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"nr_short_sentences\"] = training_data_1.apply(lambda x: get_short_sentence(x.sentence_len_words, x.avg_sentence_len_words), axis=1)\n",
    "training_data_2[\"nr_short_sentences\"] = training_data_2.apply(lambda x: get_short_sentence(x.sentence_len_words, x.avg_sentence_len_words), axis=1)\n",
    "training_data_3[\"nr_short_sentences\"] = training_data_3.apply(lambda x: get_short_sentence(x.sentence_len_words, x.avg_sentence_len_words), axis=1)\n",
    "training_data_4[\"nr_short_sentences\"] = training_data_4.apply(lambda x: get_short_sentence(x.sentence_len_words, x.avg_sentence_len_words), axis=1)\n",
    "training_data_5[\"nr_short_sentences\"] = training_data_5.apply(lambda x: get_short_sentence(x.sentence_len_words, x.avg_sentence_len_words), axis=1)\n",
    "training_data_6[\"nr_short_sentences\"] = training_data_6.apply(lambda x: get_short_sentence(x.sentence_len_words, x.avg_sentence_len_words), axis=1)\n",
    "training_data_7[\"nr_short_sentences\"] = training_data_7.apply(lambda x: get_short_sentence(x.sentence_len_words, x.avg_sentence_len_words), axis=1)\n",
    "training_data_8[\"nr_short_sentences\"] = training_data_8.apply(lambda x: get_short_sentence(x.sentence_len_words, x.avg_sentence_len_words), axis=1)\n",
    "training_data_8[\"nr_short_sentences\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most frequent sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      18\n",
       "1       8\n",
       "2       4\n",
       "3       8\n",
       "4      10\n",
       "       ..\n",
       "718     4\n",
       "719     7\n",
       "720     3\n",
       "721     6\n",
       "722    11\n",
       "Name: most_freq_sentence_length, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"most_freq_sentence_length\"] = training_data_1[\"sentence_len_words\"].apply(lambda sentence_len_list: max(Counter(sentence_len_list), key = lambda x: Counter(sentence_len_list)[x]))\n",
    "training_data_2[\"most_freq_sentence_length\"] = training_data_2[\"sentence_len_words\"].apply(lambda sentence_len_list: max(Counter(sentence_len_list), key = lambda x: Counter(sentence_len_list)[x]))\n",
    "training_data_3[\"most_freq_sentence_length\"] = training_data_3[\"sentence_len_words\"].apply(lambda sentence_len_list: max(Counter(sentence_len_list), key = lambda x: Counter(sentence_len_list)[x]))\n",
    "training_data_4[\"most_freq_sentence_length\"] = training_data_4[\"sentence_len_words\"].apply(lambda sentence_len_list: max(Counter(sentence_len_list), key = lambda x: Counter(sentence_len_list)[x]))\n",
    "training_data_5[\"most_freq_sentence_length\"] = training_data_5[\"sentence_len_words\"].apply(lambda sentence_len_list: max(Counter(sentence_len_list), key = lambda x: Counter(sentence_len_list)[x]))\n",
    "training_data_6[\"most_freq_sentence_length\"] = training_data_6[\"sentence_len_words\"].apply(lambda sentence_len_list: max(Counter(sentence_len_list), key = lambda x: Counter(sentence_len_list)[x]))\n",
    "training_data_7[\"most_freq_sentence_length\"] = training_data_7[\"sentence_len_words\"].apply(lambda sentence_len_list: max(Counter(sentence_len_list), key = lambda x: Counter(sentence_len_list)[x]))\n",
    "training_data_8[\"most_freq_sentence_length\"] = training_data_8[\"sentence_len_words\"].apply(lambda sentence_len_list: max(Counter(sentence_len_list), key = lambda x: Counter(sentence_len_list)[x]))\n",
    "training_data_8[\"most_freq_sentence_length\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tags\n",
    "\n",
    "English: https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/\n",
    "\n",
    "Dutch: https://spacy.io/models/nl --> check label scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### coordinating conjunction\n",
    "\n",
    "* a conjunction that connects words, phrases, and clauses that are coordinate, or equal to each other. \n",
    "* There are seven coordinating conjunctions: for, and, nor, but, or, yet, so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      14\n",
       "1      18\n",
       "2      16\n",
       "3      17\n",
       "4      16\n",
       "       ..\n",
       "718    11\n",
       "719    23\n",
       "720    25\n",
       "721     6\n",
       "722     9\n",
       "Name: CCONJ, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"CCONJ\"] = training_data_1[\"pos_dict\"].apply(pos_tag_count, args = (\"CCONJ\",))\n",
    "training_data_2[\"CCONJ\"] = training_data_2[\"pos_dict\"].apply(pos_tag_count, args = (\"CCONJ\",))\n",
    "training_data_3[\"CCONJ\"] = training_data_3[\"pos_dict\"].apply(pos_tag_count, args = (\"CCONJ\",))\n",
    "training_data_4[\"CCONJ\"] = training_data_4[\"pos_dict\"].apply(pos_tag_count, args = (\"CCONJ\",))\n",
    "training_data_5[\"CCONJ\"] = training_data_5[\"pos_dict\"].apply(pos_tag_count, args = (\"CCONJ\",))\n",
    "training_data_6[\"CCONJ\"] = training_data_6[\"pos_dict\"].apply(pos_tag_count, args = (\"CCONJ\",))\n",
    "training_data_7[\"CCONJ\"] = training_data_7[\"pos_dict\"].apply(pos_tag_count, args = (\"CCONJ\",))\n",
    "training_data_8[\"CCONJ\"] = training_data_8[\"pos_dict\"].apply(pos_tag_count, args = (\"CCONJ\",))\n",
    "training_data_8[\"CCONJ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subordinating conjunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      13\n",
       "1      15\n",
       "2       9\n",
       "3      19\n",
       "4      17\n",
       "       ..\n",
       "718    14\n",
       "719    18\n",
       "720    14\n",
       "721     4\n",
       "722     5\n",
       "Name: SCONJ, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"SCONJ\"] = training_data_1[\"pos_dict\"].apply(pos_tag_count, args = (\"SCONJ\",))\n",
    "training_data_2[\"SCONJ\"] = training_data_2[\"pos_dict\"].apply(pos_tag_count, args = (\"SCONJ\",))\n",
    "training_data_3[\"SCONJ\"] = training_data_3[\"pos_dict\"].apply(pos_tag_count, args = (\"SCONJ\",))\n",
    "training_data_4[\"SCONJ\"] = training_data_4[\"pos_dict\"].apply(pos_tag_count, args = (\"SCONJ\",))\n",
    "training_data_5[\"SCONJ\"] = training_data_5[\"pos_dict\"].apply(pos_tag_count, args = (\"SCONJ\",))\n",
    "training_data_6[\"SCONJ\"] = training_data_6[\"pos_dict\"].apply(pos_tag_count, args = (\"SCONJ\",))\n",
    "training_data_7[\"SCONJ\"] = training_data_7[\"pos_dict\"].apply(pos_tag_count, args = (\"SCONJ\",))\n",
    "training_data_8[\"SCONJ\"] = training_data_8[\"pos_dict\"].apply(pos_tag_count, args = (\"SCONJ\",))\n",
    "training_data_8[\"SCONJ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeral\n",
    "\n",
    "* example: 1, 2017, one, seventy-seven, IV, MMXIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      6\n",
       "2      3\n",
       "3      0\n",
       "4      4\n",
       "      ..\n",
       "718    0\n",
       "719    5\n",
       "720    4\n",
       "721    2\n",
       "722    1\n",
       "Name: NUM, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"NUM\"] = training_data_1[\"pos_dict\"].apply(pos_tag_count, args = (\"NUM\",))\n",
    "training_data_2[\"NUM\"] = training_data_2[\"pos_dict\"].apply(pos_tag_count, args = (\"NUM\",))\n",
    "training_data_3[\"NUM\"] = training_data_3[\"pos_dict\"].apply(pos_tag_count, args = (\"NUM\",))\n",
    "training_data_4[\"NUM\"] = training_data_4[\"pos_dict\"].apply(pos_tag_count, args = (\"NUM\",))\n",
    "training_data_5[\"NUM\"] = training_data_5[\"pos_dict\"].apply(pos_tag_count, args = (\"NUM\",))\n",
    "training_data_6[\"NUM\"] = training_data_6[\"pos_dict\"].apply(pos_tag_count, args = (\"NUM\",))\n",
    "training_data_7[\"NUM\"] = training_data_7[\"pos_dict\"].apply(pos_tag_count, args = (\"NUM\",))\n",
    "training_data_8[\"NUM\"] = training_data_8[\"pos_dict\"].apply(pos_tag_count, args = (\"NUM\",))\n",
    "training_data_8[\"NUM\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determiner\n",
    "* examples: a, an, the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      20\n",
       "1      30\n",
       "2      25\n",
       "3      42\n",
       "4      50\n",
       "       ..\n",
       "718    23\n",
       "719    36\n",
       "720    34\n",
       "721    21\n",
       "722    12\n",
       "Name: DET, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"DET\"] = training_data_1[\"pos_dict\"].apply(pos_tag_count, args = (\"DET\",))\n",
    "training_data_2[\"DET\"] = training_data_2[\"pos_dict\"].apply(pos_tag_count, args = (\"DET\",))\n",
    "training_data_3[\"DET\"] = training_data_3[\"pos_dict\"].apply(pos_tag_count, args = (\"DET\",))\n",
    "training_data_4[\"DET\"] = training_data_4[\"pos_dict\"].apply(pos_tag_count, args = (\"DET\",))\n",
    "training_data_5[\"DET\"] = training_data_5[\"pos_dict\"].apply(pos_tag_count, args = (\"DET\",))\n",
    "training_data_6[\"DET\"] = training_data_6[\"pos_dict\"].apply(pos_tag_count, args = (\"DET\",))\n",
    "training_data_7[\"DET\"] = training_data_7[\"pos_dict\"].apply(pos_tag_count, args = (\"DET\",))\n",
    "training_data_8[\"DET\"] = training_data_8[\"pos_dict\"].apply(pos_tag_count, args = (\"DET\",))\n",
    "training_data_8[\"DET\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preposition\n",
    "* esample: It arrived **in** a box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      49\n",
       "1      53\n",
       "2      30\n",
       "3      54\n",
       "4      41\n",
       "       ..\n",
       "718    41\n",
       "719    65\n",
       "720    75\n",
       "721    22\n",
       "722    14\n",
       "Name: ADP, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"ADP\"] = training_data_1[\"pos_dict\"].apply(pos_tag_count, args = (\"ADP\",))\n",
    "training_data_2[\"ADP\"] = training_data_2[\"pos_dict\"].apply(pos_tag_count, args = (\"ADP\",))\n",
    "training_data_3[\"ADP\"] = training_data_3[\"pos_dict\"].apply(pos_tag_count, args = (\"ADP\",))\n",
    "training_data_4[\"ADP\"] = training_data_4[\"pos_dict\"].apply(pos_tag_count, args = (\"ADP\",))\n",
    "training_data_5[\"ADP\"] = training_data_5[\"pos_dict\"].apply(pos_tag_count, args = (\"ADP\",))\n",
    "training_data_6[\"ADP\"] = training_data_6[\"pos_dict\"].apply(pos_tag_count, args = (\"ADP\",))\n",
    "training_data_7[\"ADP\"] = training_data_7[\"pos_dict\"].apply(pos_tag_count, args = (\"ADP\",))\n",
    "training_data_8[\"ADP\"] = training_data_8[\"pos_dict\"].apply(pos_tag_count, args = (\"ADP\",))\n",
    "training_data_8[\"ADP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      15\n",
       "1      18\n",
       "2      19\n",
       "3      39\n",
       "4      29\n",
       "       ..\n",
       "718    38\n",
       "719    40\n",
       "720    31\n",
       "721     6\n",
       "722     9\n",
       "Name: ADJ, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"ADJ\"] = training_data_1[\"pos_dict\"].apply(pos_tag_count, args = (\"ADJ\",))\n",
    "training_data_2[\"ADJ\"] = training_data_2[\"pos_dict\"].apply(pos_tag_count, args = (\"ADJ\",))\n",
    "training_data_3[\"ADJ\"] = training_data_3[\"pos_dict\"].apply(pos_tag_count, args = (\"ADJ\",))\n",
    "training_data_4[\"ADJ\"] = training_data_4[\"pos_dict\"].apply(pos_tag_count, args = (\"ADJ\",))\n",
    "training_data_5[\"ADJ\"] = training_data_5[\"pos_dict\"].apply(pos_tag_count, args = (\"ADJ\",))\n",
    "training_data_6[\"ADJ\"] = training_data_6[\"pos_dict\"].apply(pos_tag_count, args = (\"ADJ\",))\n",
    "training_data_7[\"ADJ\"] = training_data_7[\"pos_dict\"].apply(pos_tag_count, args = (\"ADJ\",))\n",
    "training_data_8[\"ADJ\"] = training_data_8[\"pos_dict\"].apply(pos_tag_count, args = (\"ADJ\",))\n",
    "training_data_8[\"ADJ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal adjective or numeral\n",
    "* I cannot destinguish it from ADJ (adjective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       80\n",
       "1       99\n",
       "2       72\n",
       "3      132\n",
       "4      108\n",
       "      ... \n",
       "718     95\n",
       "719    121\n",
       "720    147\n",
       "721     70\n",
       "722     38\n",
       "Name: NOUN, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"NOUN\"] = training_data_1[\"pos_dict\"].apply(pos_tag_count, args = (\"NOUN\",))\n",
    "training_data_2[\"NOUN\"] = training_data_2[\"pos_dict\"].apply(pos_tag_count, args = (\"NOUN\",))\n",
    "training_data_3[\"NOUN\"] = training_data_3[\"pos_dict\"].apply(pos_tag_count, args = (\"NOUN\",))\n",
    "training_data_4[\"NOUN\"] = training_data_4[\"pos_dict\"].apply(pos_tag_count, args = (\"NOUN\",))\n",
    "training_data_5[\"NOUN\"] = training_data_5[\"pos_dict\"].apply(pos_tag_count, args = (\"NOUN\",))\n",
    "training_data_6[\"NOUN\"] = training_data_6[\"pos_dict\"].apply(pos_tag_count, args = (\"NOUN\",))\n",
    "training_data_7[\"NOUN\"] = training_data_7[\"pos_dict\"].apply(pos_tag_count, args = (\"NOUN\",))\n",
    "training_data_8[\"NOUN\"] = training_data_8[\"pos_dict\"].apply(pos_tag_count, args = (\"NOUN\",))\n",
    "training_data_8[\"NOUN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Singular & plural proper nouns\n",
    "\n",
    "* A proper noun is a specific (i.e., not generic) name for a particular person, place, or thing.\n",
    "* In the dutch tags I cannot identify which ones are singular and which plural tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       7\n",
       "1       5\n",
       "2       4\n",
       "3      38\n",
       "4       3\n",
       "       ..\n",
       "718     4\n",
       "719     7\n",
       "720     4\n",
       "721    14\n",
       "722    13\n",
       "Name: PROPN, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"PROPN\"] = training_data_1[\"pos_dict\"].apply(pos_tag_count, args = (\"PROPN\",))\n",
    "training_data_2[\"PROPN\"] = training_data_2[\"pos_dict\"].apply(pos_tag_count, args = (\"PROPN\",))\n",
    "training_data_3[\"PROPN\"] = training_data_3[\"pos_dict\"].apply(pos_tag_count, args = (\"PROPN\",))\n",
    "training_data_4[\"PROPN\"] = training_data_4[\"pos_dict\"].apply(pos_tag_count, args = (\"PROPN\",))\n",
    "training_data_5[\"PROPN\"] = training_data_5[\"pos_dict\"].apply(pos_tag_count, args = (\"PROPN\",))\n",
    "training_data_6[\"PROPN\"] = training_data_6[\"pos_dict\"].apply(pos_tag_count, args = (\"PROPN\",))\n",
    "training_data_7[\"PROPN\"] = training_data_7[\"pos_dict\"].apply(pos_tag_count, args = (\"PROPN\",))\n",
    "training_data_8[\"PROPN\"] = training_data_8[\"pos_dict\"].apply(pos_tag_count, args = (\"PROPN\",))\n",
    "training_data_8[\"PROPN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pronouns (personal & posessive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      47\n",
       "1      49\n",
       "2      25\n",
       "3      32\n",
       "4      41\n",
       "       ..\n",
       "718    38\n",
       "719    79\n",
       "720    54\n",
       "721    27\n",
       "722    23\n",
       "Name: PRON, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"PRON\"] = training_data_1[\"pos_dict\"].apply(pos_tag_count, args = (\"PRON\",))\n",
    "training_data_2[\"PRON\"] = training_data_2[\"pos_dict\"].apply(pos_tag_count, args = (\"PRON\",))\n",
    "training_data_3[\"PRON\"] = training_data_3[\"pos_dict\"].apply(pos_tag_count, args = (\"PRON\",))\n",
    "training_data_4[\"PRON\"] = training_data_4[\"pos_dict\"].apply(pos_tag_count, args = (\"PRON\",))\n",
    "training_data_5[\"PRON\"] = training_data_5[\"pos_dict\"].apply(pos_tag_count, args = (\"PRON\",))\n",
    "training_data_6[\"PRON\"] = training_data_6[\"pos_dict\"].apply(pos_tag_count, args = (\"PRON\",))\n",
    "training_data_7[\"PRON\"] = training_data_7[\"pos_dict\"].apply(pos_tag_count, args = (\"PRON\",))\n",
    "training_data_8[\"PRON\"] = training_data_8[\"pos_dict\"].apply(pos_tag_count, args = (\"PRON\",))\n",
    "training_data_8[\"PRON\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      15\n",
       "1      18\n",
       "2      11\n",
       "3      25\n",
       "4      36\n",
       "       ..\n",
       "718    20\n",
       "719    46\n",
       "720    39\n",
       "721     9\n",
       "722     8\n",
       "Name: ADV, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"ADV\"] = training_data_1[\"pos_dict\"].apply(pos_tag_count, args = (\"ADV\",))\n",
    "training_data_2[\"ADV\"] = training_data_2[\"pos_dict\"].apply(pos_tag_count, args = (\"ADV\",))\n",
    "training_data_3[\"ADV\"] = training_data_3[\"pos_dict\"].apply(pos_tag_count, args = (\"ADV\",))\n",
    "training_data_4[\"ADV\"] = training_data_4[\"pos_dict\"].apply(pos_tag_count, args = (\"ADV\",))\n",
    "training_data_5[\"ADV\"] = training_data_5[\"pos_dict\"].apply(pos_tag_count, args = (\"ADV\",))\n",
    "training_data_6[\"ADV\"] = training_data_6[\"pos_dict\"].apply(pos_tag_count, args = (\"ADV\",))\n",
    "training_data_7[\"ADV\"] = training_data_7[\"pos_dict\"].apply(pos_tag_count, args = (\"ADV\",))\n",
    "training_data_8[\"ADV\"] = training_data_8[\"pos_dict\"].apply(pos_tag_count, args = (\"ADV\",))\n",
    "training_data_8[\"ADV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      51\n",
       "1      73\n",
       "2      39\n",
       "3      71\n",
       "4      59\n",
       "       ..\n",
       "718    60\n",
       "719    83\n",
       "720    75\n",
       "721    44\n",
       "722    24\n",
       "Name: VERB, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no feature but needed for calculation\n",
    "training_data_1[\"VERB\"] = training_data_1[\"pos_dict\"].apply(pos_tag_count, args = (\"VERB\",))\n",
    "training_data_1[\"PRON\"] = training_data_1[\"pos_dict\"].apply(pos_tag_count, args = (\"PRON\",))\n",
    "\n",
    "training_data_2[\"VERB\"] = training_data_2[\"pos_dict\"].apply(pos_tag_count, args = (\"VERB\",))\n",
    "training_data_2[\"PRON\"] = training_data_2[\"pos_dict\"].apply(pos_tag_count, args = (\"PRON\",))\n",
    "\n",
    "training_data_3[\"VERB\"] = training_data_3[\"pos_dict\"].apply(pos_tag_count, args = (\"VERB\",))\n",
    "training_data_3[\"PRON\"] = training_data_3[\"pos_dict\"].apply(pos_tag_count, args = (\"PRON\",))\n",
    "\n",
    "training_data_4[\"VERB\"] = training_data_4[\"pos_dict\"].apply(pos_tag_count, args = (\"VERB\",))\n",
    "training_data_4[\"PRON\"] = training_data_4[\"pos_dict\"].apply(pos_tag_count, args = (\"PRON\",))\n",
    "\n",
    "training_data_5[\"VERB\"] = training_data_5[\"pos_dict\"].apply(pos_tag_count, args = (\"VERB\",))\n",
    "training_data_5[\"PRON\"] = training_data_5[\"pos_dict\"].apply(pos_tag_count, args = (\"PRON\",))\n",
    "\n",
    "training_data_6[\"VERB\"] = training_data_6[\"pos_dict\"].apply(pos_tag_count, args = (\"VERB\",))\n",
    "training_data_6[\"PRON\"] = training_data_6[\"pos_dict\"].apply(pos_tag_count, args = (\"PRON\",))\n",
    "\n",
    "training_data_7[\"VERB\"] = training_data_7[\"pos_dict\"].apply(pos_tag_count, args = (\"VERB\",))\n",
    "training_data_7[\"PRON\"] = training_data_7[\"pos_dict\"].apply(pos_tag_count, args = (\"PRON\",))\n",
    "\n",
    "training_data_8[\"VERB\"] = training_data_8[\"pos_dict\"].apply(pos_tag_count, args = (\"VERB\",))\n",
    "training_data_8[\"PRON\"] = training_data_8[\"pos_dict\"].apply(pos_tag_count, args = (\"PRON\",))\n",
    "training_data_8[\"VERB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability measures\n",
    "\n",
    "Abbreviations\n",
    "\n",
    "* nchar - number of characters\n",
    "* nw - number of words\n",
    "* nuw - number of unique words\n",
    "* ns - number of sentences\n",
    "* nsb - number of syllables\n",
    "* nn - number or nouns\n",
    "* nprep - number of prepositions\n",
    "* npar - number of participles\n",
    "* npro - number of pronouns\n",
    "* nadv - number of adverbs\n",
    "* nv - number of verbs\n",
    "* npolysyb - number of words with 3 or more syllables\n",
    "\n",
    "* pdw - % of difficult word\n",
    "\n",
    "* avgsentint - average sentence length in words\n",
    "* avgsent - average sentence length (nw / ns)\n",
    "\n",
    "one of them\n",
    "* hw -  Hard  words  =  number  of  words  of  more  than  two  syllables\n",
    "* ncw - number of \"complex\" words consisting of three or more syllables. Do not include proper nouns, familiar jargon, or compound words. Do not include common suffixes (such as -es, -ed, or -ing) as a syllable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gunning Fog Index\n",
    "\n",
    "1. Formula from DuBay (2007)\n",
    "        \n",
    "        Grade  Level  =  0.4  (average  sentence  length  +  hard  words) Where:  Hard  words  =  number  of  words  of  more  than  two  syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      24\n",
       "1      53\n",
       "2      78\n",
       "3      36\n",
       "4       9\n",
       "       ..\n",
       "718    42\n",
       "719    20\n",
       "720    82\n",
       "721    43\n",
       "722    55\n",
       "Name: nr_syllables_bigger_two, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"nr_syllables_bigger_two\"] = training_data_1[\"nr_syllables\"].apply(lambda nsylb_list: sum([nsylb > 2 for nsylb in nsylb_list]))\n",
    "training_data_2[\"nr_syllables_bigger_two\"] = training_data_2[\"nr_syllables\"].apply(lambda nsylb_list: sum([nsylb > 2 for nsylb in nsylb_list]))\n",
    "training_data_3[\"nr_syllables_bigger_two\"] = training_data_3[\"nr_syllables\"].apply(lambda nsylb_list: sum([nsylb > 2 for nsylb in nsylb_list]))\n",
    "training_data_4[\"nr_syllables_bigger_two\"] = training_data_4[\"nr_syllables\"].apply(lambda nsylb_list: sum([nsylb > 2 for nsylb in nsylb_list]))\n",
    "training_data_5[\"nr_syllables_bigger_two\"] = training_data_5[\"nr_syllables\"].apply(lambda nsylb_list: sum([nsylb > 2 for nsylb in nsylb_list]))\n",
    "training_data_6[\"nr_syllables_bigger_two\"] = training_data_6[\"nr_syllables\"].apply(lambda nsylb_list: sum([nsylb > 2 for nsylb in nsylb_list]))\n",
    "training_data_7[\"nr_syllables_bigger_two\"] = training_data_7[\"nr_syllables\"].apply(lambda nsylb_list: sum([nsylb > 2 for nsylb in nsylb_list]))\n",
    "training_data_8[\"nr_syllables_bigger_two\"] = training_data_8[\"nr_syllables\"].apply(lambda nsylb_list: sum([nsylb > 2 for nsylb in nsylb_list]))\n",
    "training_data_8[\"nr_syllables_bigger_two\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      15.6\n",
       "1      26.0\n",
       "2      35.2\n",
       "3      20.4\n",
       "4       8.0\n",
       "       ... \n",
       "718    20.4\n",
       "719    11.6\n",
       "720    36.4\n",
       "721    20.0\n",
       "722    25.2\n",
       "Name: gunning_fox_index, Length: 723, dtype: float64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"gunning_fox_index\"] = training_data_1.apply(lambda x: 0.4 * (x.avg_sentence_len_words + x.nr_syllables_bigger_two), axis=1)\n",
    "training_data_2[\"gunning_fox_index\"] = training_data_2.apply(lambda x: 0.4 * (x.avg_sentence_len_words + x.nr_syllables_bigger_two), axis=1)\n",
    "training_data_3[\"gunning_fox_index\"] = training_data_3.apply(lambda x: 0.4 * (x.avg_sentence_len_words + x.nr_syllables_bigger_two), axis=1)\n",
    "training_data_4[\"gunning_fox_index\"] = training_data_4.apply(lambda x: 0.4 * (x.avg_sentence_len_words + x.nr_syllables_bigger_two), axis=1)\n",
    "training_data_5[\"gunning_fox_index\"] = training_data_5.apply(lambda x: 0.4 * (x.avg_sentence_len_words + x.nr_syllables_bigger_two), axis=1)\n",
    "training_data_6[\"gunning_fox_index\"] = training_data_6.apply(lambda x: 0.4 * (x.avg_sentence_len_words + x.nr_syllables_bigger_two), axis=1)\n",
    "training_data_7[\"gunning_fox_index\"] = training_data_7.apply(lambda x: 0.4 * (x.avg_sentence_len_words + x.nr_syllables_bigger_two), axis=1)\n",
    "training_data_8[\"gunning_fox_index\"] = training_data_8.apply(lambda x: 0.4 * (x.avg_sentence_len_words + x.nr_syllables_bigger_two), axis=1)\n",
    "training_data_8[\"gunning_fox_index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flesch reading ease\n",
    "\n",
    "Flesch reading Easy score between 70 and 80 is ”Fairly Easy” which means that the text can easily be understood by a (U.S.)7th grade student (Smith & Jönsson, 2011).\n",
    "\n",
    "        Score = 206.835 − 1.015 * avg nr of words per sentence (nw / ns) − 84.6 * avg nr of syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.069219\n",
       "1      1.277707\n",
       "2      1.313589\n",
       "3      1.158708\n",
       "4      1.077496\n",
       "         ...   \n",
       "718    1.069606\n",
       "719    1.111511\n",
       "720    1.261078\n",
       "721    1.204861\n",
       "722    1.330526\n",
       "Name: avg_nr_syllables, Length: 723, dtype: float64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"avg_nr_syllables\"] = training_data_1.apply(lambda x: sum(x.nr_syllables)/x.word_count, axis = 1)\n",
    "training_data_2[\"avg_nr_syllables\"] = training_data_2.apply(lambda x: sum(x.nr_syllables)/x.word_count, axis = 1)\n",
    "training_data_3[\"avg_nr_syllables\"] = training_data_3.apply(lambda x: sum(x.nr_syllables)/x.word_count, axis = 1)\n",
    "training_data_4[\"avg_nr_syllables\"] = training_data_4.apply(lambda x: sum(x.nr_syllables)/x.word_count, axis = 1)\n",
    "training_data_5[\"avg_nr_syllables\"] = training_data_5.apply(lambda x: sum(x.nr_syllables)/x.word_count, axis = 1)\n",
    "training_data_6[\"avg_nr_syllables\"] = training_data_6.apply(lambda x: sum(x.nr_syllables)/x.word_count, axis = 1)\n",
    "training_data_7[\"avg_nr_syllables\"] = training_data_7.apply(lambda x: sum(x.nr_syllables)/x.word_count, axis = 1)\n",
    "training_data_8[\"avg_nr_syllables\"] = training_data_8.apply(lambda x: sum(x.nr_syllables)/x.word_count, axis = 1)\n",
    "training_data_8[\"avg_nr_syllables\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      101.154035\n",
       "1       86.560987\n",
       "2       85.555383\n",
       "3       93.583315\n",
       "4      104.513815\n",
       "          ...    \n",
       "718    107.211369\n",
       "719    103.666187\n",
       "720     91.012814\n",
       "721     97.798750\n",
       "722     86.152474\n",
       "Name: flesh_reading_ease, Length: 723, dtype: float64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"flesh_reading_ease\"] = training_data_1.apply(lambda x: 206.835 - 1.015 * x.avg_sentence_len_words - 84.6 * x.avg_nr_syllables, axis=1)\n",
    "training_data_2[\"flesh_reading_ease\"] = training_data_2.apply(lambda x: 206.835 - 1.015 * x.avg_sentence_len_words - 84.6 * x.avg_nr_syllables, axis=1)\n",
    "training_data_3[\"flesh_reading_ease\"] = training_data_3.apply(lambda x: 206.835 - 1.015 * x.avg_sentence_len_words - 84.6 * x.avg_nr_syllables, axis=1)\n",
    "training_data_4[\"flesh_reading_ease\"] = training_data_4.apply(lambda x: 206.835 - 1.015 * x.avg_sentence_len_words - 84.6 * x.avg_nr_syllables, axis=1)\n",
    "training_data_5[\"flesh_reading_ease\"] = training_data_5.apply(lambda x: 206.835 - 1.015 * x.avg_sentence_len_words - 84.6 * x.avg_nr_syllables, axis=1)\n",
    "training_data_6[\"flesh_reading_ease\"] = training_data_6.apply(lambda x: 206.835 - 1.015 * x.avg_sentence_len_words - 84.6 * x.avg_nr_syllables, axis=1)\n",
    "training_data_7[\"flesh_reading_ease\"] = training_data_7.apply(lambda x: 206.835 - 1.015 * x.avg_sentence_len_words - 84.6 * x.avg_nr_syllables, axis=1)\n",
    "training_data_8[\"flesh_reading_ease\"] = training_data_8.apply(lambda x: 206.835 - 1.015 * x.avg_sentence_len_words - 84.6 * x.avg_nr_syllables, axis=1)\n",
    "training_data_8[\"flesh_reading_ease\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flesch Kincaid grade level\n",
    "\n",
    " The Flesch-Kincaid Grade level is a U.S. grade level version that normalises the Flesch reading easy score to correspond  to  readability  for  students  in  various grades (Smith & Jönsson, 2011). Formula from Hussain et al. (2011)\n",
    "\n",
    "        score = 0.39 * (nw / ns) + 11.8 (nbs / nw) - 15.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2.876789\n",
       "1      4.166943\n",
       "2      3.810348\n",
       "3      3.932753\n",
       "4      1.414456\n",
       "         ...   \n",
       "718    0.541346\n",
       "719    1.035827\n",
       "720    2.800719\n",
       "721    1.357361\n",
       "722    3.230211\n",
       "Name: flesh_kincaid_grade_level, Length: 723, dtype: float64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"flesh_kincaid_grade_level\"] = training_data_1.apply(lambda x: 0.39 * x.avg_sentence_len_words + 11.8 * x.avg_nr_syllables - 15.59, axis=1)\n",
    "training_data_2[\"flesh_kincaid_grade_level\"] = training_data_2.apply(lambda x: 0.39 * x.avg_sentence_len_words + 11.8 * x.avg_nr_syllables - 15.59, axis=1)\n",
    "training_data_3[\"flesh_kincaid_grade_level\"] = training_data_3.apply(lambda x: 0.39 * x.avg_sentence_len_words + 11.8 * x.avg_nr_syllables - 15.59, axis=1)\n",
    "training_data_4[\"flesh_kincaid_grade_level\"] = training_data_4.apply(lambda x: 0.39 * x.avg_sentence_len_words + 11.8 * x.avg_nr_syllables - 15.59, axis=1)\n",
    "training_data_5[\"flesh_kincaid_grade_level\"] = training_data_5.apply(lambda x: 0.39 * x.avg_sentence_len_words + 11.8 * x.avg_nr_syllables - 15.59, axis=1)\n",
    "training_data_6[\"flesh_kincaid_grade_level\"] = training_data_6.apply(lambda x: 0.39 * x.avg_sentence_len_words + 11.8 * x.avg_nr_syllables - 15.59, axis=1)\n",
    "training_data_7[\"flesh_kincaid_grade_level\"] = training_data_7.apply(lambda x: 0.39 * x.avg_sentence_len_words + 11.8 * x.avg_nr_syllables - 15.59, axis=1)\n",
    "training_data_8[\"flesh_kincaid_grade_level\"] = training_data_8.apply(lambda x: 0.39 * x.avg_sentence_len_words + 11.8 * x.avg_nr_syllables - 15.59, axis=1)\n",
    "training_data_8[\"flesh_kincaid_grade_level\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated readability index\n",
    "\n",
    "Formula from \n",
    "\n",
    "* (DuBay, 2007) GL  =  0.50  (avg words  per  sentence)  +  4.71  (avg strokes  per  word (chars per word))  –21.43.\n",
    "\n",
    "\n",
    "Score ranges from 1(Kindergarten) to 14(College)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    723.000000\n",
       "mean       3.353877\n",
       "std        2.298699\n",
       "min       -2.474000\n",
       "25%        1.968942\n",
       "50%        2.917861\n",
       "75%        4.103674\n",
       "max       19.168736\n",
       "Name: automated_readability_index, dtype: float64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"automated_readability_index\"] = training_data_1.apply(lambda x: 0.5 * x.avg_sentence_len_words + 4.71 * x.avg_word_len - 21.43, axis = 1)\n",
    "training_data_2[\"automated_readability_index\"] = training_data_2.apply(lambda x: 0.5 * x.avg_sentence_len_words + 4.71 * x.avg_word_len - 21.43, axis = 1)\n",
    "training_data_3[\"automated_readability_index\"] = training_data_3.apply(lambda x: 0.5 * x.avg_sentence_len_words + 4.71 * x.avg_word_len - 21.43, axis = 1)\n",
    "training_data_4[\"automated_readability_index\"] = training_data_4.apply(lambda x: 0.5 * x.avg_sentence_len_words + 4.71 * x.avg_word_len - 21.43, axis = 1)\n",
    "training_data_5[\"automated_readability_index\"] = training_data_5.apply(lambda x: 0.5 * x.avg_sentence_len_words + 4.71 * x.avg_word_len - 21.43, axis = 1)\n",
    "training_data_6[\"automated_readability_index\"] = training_data_6.apply(lambda x: 0.5 * x.avg_sentence_len_words + 4.71 * x.avg_word_len - 21.43, axis = 1)\n",
    "training_data_7[\"automated_readability_index\"] = training_data_7.apply(lambda x: 0.5 * x.avg_sentence_len_words + 4.71 * x.avg_word_len - 21.43, axis = 1)\n",
    "training_data_8[\"automated_readability_index\"] = training_data_8.apply(lambda x: 0.5 * x.avg_sentence_len_words + 4.71 * x.avg_word_len - 21.43, axis = 1)\n",
    "pd.Series(training_data_8[\"automated_readability_index\"]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple measure of Gobbledygook (SMOG)\n",
    "\n",
    "1. Formula invented by G.  HarryMcLaughlin  (1969)  published  his  SMOG  formula  (DuBay, 2007)\n",
    "\n",
    "        SMOG  grading  =  3  +  sqrt(npolysyb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       5\n",
       "1      12\n",
       "2      21\n",
       "3       4\n",
       "4       0\n",
       "       ..\n",
       "718    10\n",
       "719     4\n",
       "720    28\n",
       "721     8\n",
       "722    21\n",
       "Name: nr_syllables_bigger_three, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"nr_syllables_bigger_three\"] = training_data_1[\"nr_syllables\"].apply(lambda nsylb_list: sum([nsylb > 3 for nsylb in nsylb_list]))\n",
    "training_data_2[\"nr_syllables_bigger_three\"] = training_data_2[\"nr_syllables\"].apply(lambda nsylb_list: sum([nsylb > 3 for nsylb in nsylb_list]))\n",
    "training_data_3[\"nr_syllables_bigger_three\"] = training_data_3[\"nr_syllables\"].apply(lambda nsylb_list: sum([nsylb > 3 for nsylb in nsylb_list]))\n",
    "training_data_4[\"nr_syllables_bigger_three\"] = training_data_4[\"nr_syllables\"].apply(lambda nsylb_list: sum([nsylb > 3 for nsylb in nsylb_list]))\n",
    "training_data_5[\"nr_syllables_bigger_three\"] = training_data_5[\"nr_syllables\"].apply(lambda nsylb_list: sum([nsylb > 3 for nsylb in nsylb_list]))\n",
    "training_data_6[\"nr_syllables_bigger_three\"] = training_data_6[\"nr_syllables\"].apply(lambda nsylb_list: sum([nsylb > 3 for nsylb in nsylb_list]))\n",
    "training_data_7[\"nr_syllables_bigger_three\"] = training_data_7[\"nr_syllables\"].apply(lambda nsylb_list: sum([nsylb > 3 for nsylb in nsylb_list]))\n",
    "training_data_8[\"nr_syllables_bigger_three\"] = training_data_8[\"nr_syllables\"].apply(lambda nsylb_list: sum([nsylb > 3 for nsylb in nsylb_list]))\n",
    "training_data_8[\"nr_syllables_bigger_three\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      5.236068\n",
       "1      6.464102\n",
       "2      7.582576\n",
       "3      5.000000\n",
       "4      3.000000\n",
       "         ...   \n",
       "718    6.162278\n",
       "719    5.000000\n",
       "720    8.291503\n",
       "721    5.828427\n",
       "722    7.582576\n",
       "Name: SMOG, Length: 723, dtype: float64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"SMOG\"] = training_data_1[\"nr_syllables_bigger_three\"].apply(lambda nsylb_bigger_three: 3 + np.sqrt(nsylb_bigger_three))\n",
    "training_data_2[\"SMOG\"] = training_data_2[\"nr_syllables_bigger_three\"].apply(lambda nsylb_bigger_three: 3 + np.sqrt(nsylb_bigger_three))\n",
    "training_data_3[\"SMOG\"] = training_data_3[\"nr_syllables_bigger_three\"].apply(lambda nsylb_bigger_three: 3 + np.sqrt(nsylb_bigger_three))\n",
    "training_data_4[\"SMOG\"] = training_data_4[\"nr_syllables_bigger_three\"].apply(lambda nsylb_bigger_three: 3 + np.sqrt(nsylb_bigger_three))\n",
    "training_data_5[\"SMOG\"] = training_data_5[\"nr_syllables_bigger_three\"].apply(lambda nsylb_bigger_three: 3 + np.sqrt(nsylb_bigger_three))\n",
    "training_data_6[\"SMOG\"] = training_data_6[\"nr_syllables_bigger_three\"].apply(lambda nsylb_bigger_three: 3 + np.sqrt(nsylb_bigger_three))\n",
    "training_data_7[\"SMOG\"] = training_data_7[\"nr_syllables_bigger_three\"].apply(lambda nsylb_bigger_three: 3 + np.sqrt(nsylb_bigger_three))\n",
    "training_data_8[\"SMOG\"] = training_data_8[\"nr_syllables_bigger_three\"].apply(lambda nsylb_bigger_three: 3 + np.sqrt(nsylb_bigger_three))\n",
    "training_data_8[\"SMOG\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIX\n",
    "\n",
    "LIX  measures  the  number  of  words  per sentence and also the number of long words (>6 characters) in the text (Smith & Jönsson, 2011).\n",
    "\n",
    "        LIX = (nw / ns) + ((nw > 6 chars) / nw) * 100\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "LIX value Text genre\n",
    "\n",
    "* -25 Children’s books\n",
    "* 25-30 Easy texts\n",
    "* 30-40 Normal text/fiction\n",
    "* 40-50 Informative text\n",
    "* 50-60 Specialist literature\n",
    "* more than 60 Research, dissertations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    723.000000\n",
       "mean      24.848527\n",
       "std        4.919789\n",
       "min       13.668016\n",
       "25%       21.773124\n",
       "50%       24.121212\n",
       "75%       26.928150\n",
       "max       51.776173\n",
       "Name: LIX, dtype: float64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"nr_long_words_char6\"] = training_data_1[\"word_length\"].apply(lambda word_len_list: len([word_len for word_len in word_len_list if word_len > 6])/len(word_len_list))\n",
    "training_data_1[\"LIX\"] = training_data_1.apply(lambda x: x.avg_sentence_len_words + x.nr_long_words_char6 * 100, axis = 1)\n",
    "\n",
    "training_data_2[\"nr_long_words_char6\"] = training_data_2[\"word_length\"].apply(lambda word_len_list: len([word_len for word_len in word_len_list if word_len > 6])/len(word_len_list))\n",
    "training_data_2[\"LIX\"] = training_data_2.apply(lambda x: x.avg_sentence_len_words + x.nr_long_words_char6 * 100, axis = 1)\n",
    "\n",
    "training_data_3[\"nr_long_words_char6\"] = training_data_3[\"word_length\"].apply(lambda word_len_list: len([word_len for word_len in word_len_list if word_len > 6])/len(word_len_list))\n",
    "training_data_3[\"LIX\"] = training_data_3.apply(lambda x: x.avg_sentence_len_words + x.nr_long_words_char6 * 100, axis = 1)\n",
    "\n",
    "training_data_4[\"nr_long_words_char6\"] = training_data_4[\"word_length\"].apply(lambda word_len_list: len([word_len for word_len in word_len_list if word_len > 6])/len(word_len_list))\n",
    "training_data_4[\"LIX\"] = training_data_4.apply(lambda x: x.avg_sentence_len_words + x.nr_long_words_char6 * 100, axis = 1)\n",
    "\n",
    "training_data_5[\"nr_long_words_char6\"] = training_data_5[\"word_length\"].apply(lambda word_len_list: len([word_len for word_len in word_len_list if word_len > 6])/len(word_len_list))\n",
    "training_data_5[\"LIX\"] = training_data_5.apply(lambda x: x.avg_sentence_len_words + x.nr_long_words_char6 * 100, axis = 1)\n",
    "\n",
    "training_data_6[\"nr_long_words_char6\"] = training_data_6[\"word_length\"].apply(lambda word_len_list: len([word_len for word_len in word_len_list if word_len > 6])/len(word_len_list))\n",
    "training_data_6[\"LIX\"] = training_data_6.apply(lambda x: x.avg_sentence_len_words + x.nr_long_words_char6 * 100, axis = 1)\n",
    "\n",
    "training_data_7[\"nr_long_words_char6\"] = training_data_7[\"word_length\"].apply(lambda word_len_list: len([word_len for word_len in word_len_list if word_len > 6])/len(word_len_list))\n",
    "training_data_7[\"LIX\"] = training_data_7.apply(lambda x: x.avg_sentence_len_words + x.nr_long_words_char6 * 100, axis = 1)\n",
    "\n",
    "training_data_8[\"nr_long_words_char6\"] = training_data_8[\"word_length\"].apply(lambda word_len_list: len([word_len for word_len in word_len_list if word_len > 6])/len(word_len_list))\n",
    "training_data_8[\"LIX\"] = training_data_8.apply(lambda x: x.avg_sentence_len_words + x.nr_long_words_char6 * 100, axis = 1)\n",
    "training_data_8[\"LIX\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word variation index (OVIX)\n",
    "\n",
    "Formula from (Smith & Jönsson, 2011)\n",
    "\n",
    "        OVIX = log(nw) / log( 2 - (log(nuw) / log(nw)))\n",
    "\n",
    "High values are associates with lower readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    723.000000\n",
       "mean      51.870492\n",
       "std        6.946805\n",
       "min        0.000000\n",
       "25%       47.275474\n",
       "50%       51.413588\n",
       "75%       55.827506\n",
       "max       83.745824\n",
       "Name: OVIX, dtype: float64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"OVIX\"] = training_data_1.apply(lambda x: np.log(x.word_count) / np.log(2 - (np.log(x.unique_words) / np.log(x.word_count))) if np.log(2 - (np.log(x.unique_words) / np.log(x.word_count))) != 0 else 0, axis = 1)\n",
    "training_data_2[\"OVIX\"] = training_data_2.apply(lambda x: np.log(x.word_count) / np.log(2 - (np.log(x.unique_words) / np.log(x.word_count))) if np.log(2 - (np.log(x.unique_words) / np.log(x.word_count))) != 0 else 0, axis = 1)\n",
    "training_data_3[\"OVIX\"] = training_data_3.apply(lambda x: np.log(x.word_count) / np.log(2 - (np.log(x.unique_words) / np.log(x.word_count))) if np.log(2 - (np.log(x.unique_words) / np.log(x.word_count))) != 0 else 0, axis = 1)\n",
    "training_data_4[\"OVIX\"] = training_data_4.apply(lambda x: np.log(x.word_count) / np.log(2 - (np.log(x.unique_words) / np.log(x.word_count))) if np.log(2 - (np.log(x.unique_words) / np.log(x.word_count))) != 0 else 0, axis = 1)\n",
    "training_data_5[\"OVIX\"] = training_data_5.apply(lambda x: np.log(x.word_count) / np.log(2 - (np.log(x.unique_words) / np.log(x.word_count))) if np.log(2 - (np.log(x.unique_words) / np.log(x.word_count))) != 0 else 0, axis = 1)\n",
    "training_data_6[\"OVIX\"] = training_data_6.apply(lambda x: np.log(x.word_count) / np.log(2 - (np.log(x.unique_words) / np.log(x.word_count))) if np.log(2 - (np.log(x.unique_words) / np.log(x.word_count))) != 0 else 0, axis = 1)\n",
    "training_data_7[\"OVIX\"] = training_data_7.apply(lambda x: np.log(x.word_count) / np.log(2 - (np.log(x.unique_words) / np.log(x.word_count))) if np.log(2 - (np.log(x.unique_words) / np.log(x.word_count))) != 0 else 0, axis = 1)\n",
    "training_data_8[\"OVIX\"] = training_data_8.apply(lambda x: np.log(x.word_count) / np.log(2 - (np.log(x.unique_words) / np.log(x.word_count))) if np.log(2 - (np.log(x.unique_words) / np.log(x.word_count))) != 0 else 0, axis = 1)\n",
    "training_data_8[\"OVIX\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nominal ratio\n",
    "\n",
    "A higher NR indicates a more professional and stylistically  developed  text,  while  a  lower  value indicates more simple and informal language (Smith & Jönsson, 2011).\n",
    "\n",
    "        NR = (nn + nprep + npar) / (npro + nadv + nv)\n",
    "\n",
    "nprep - number or prepositions\n",
    "npar - number of participles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.256637\n",
       "1      1.278571\n",
       "2      1.773333\n",
       "3      1.664062\n",
       "4      1.227941\n",
       "         ...   \n",
       "718    1.313559\n",
       "719    0.956731\n",
       "720    1.625000\n",
       "721    1.462500\n",
       "722    1.272727\n",
       "Name: nominal_ratio, Length: 723, dtype: float64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"nominal_ratio\"] = training_data_1.apply(lambda x: ((x.NOUN + x.ADP + x.participle) / (x.PRON + x.ADV + x.VERB)) if x.PRON + x.ADV + x.VERB > 0 else 0, axis = 1)\n",
    "training_data_2[\"nominal_ratio\"] = training_data_2.apply(lambda x: ((x.NOUN + x.ADP + x.participle) / (x.PRON + x.ADV + x.VERB)) if x.PRON + x.ADV + x.VERB > 0 else 0, axis = 1)\n",
    "training_data_3[\"nominal_ratio\"] = training_data_3.apply(lambda x: ((x.NOUN + x.ADP + x.participle) / (x.PRON + x.ADV + x.VERB)) if x.PRON + x.ADV + x.VERB > 0 else 0, axis = 1)\n",
    "training_data_4[\"nominal_ratio\"] = training_data_4.apply(lambda x: ((x.NOUN + x.ADP + x.participle) / (x.PRON + x.ADV + x.VERB)) if x.PRON + x.ADV + x.VERB > 0 else 0, axis = 1)\n",
    "training_data_5[\"nominal_ratio\"] = training_data_5.apply(lambda x: ((x.NOUN + x.ADP + x.participle) / (x.PRON + x.ADV + x.VERB)) if x.PRON + x.ADV + x.VERB > 0 else 0, axis = 1)\n",
    "training_data_6[\"nominal_ratio\"] = training_data_6.apply(lambda x: ((x.NOUN + x.ADP + x.participle) / (x.PRON + x.ADV + x.VERB)) if x.PRON + x.ADV + x.VERB > 0 else 0, axis = 1)\n",
    "training_data_7[\"nominal_ratio\"] = training_data_7.apply(lambda x: ((x.NOUN + x.ADP + x.participle) / (x.PRON + x.ADV + x.VERB)) if x.PRON + x.ADV + x.VERB > 0 else 0, axis = 1)\n",
    "training_data_8[\"nominal_ratio\"] = training_data_8.apply(lambda x: ((x.NOUN + x.ADP + x.participle) / (x.PRON + x.ADV + x.VERB)) if x.PRON + x.ADV + x.VERB > 0 else 0, axis = 1)\n",
    "training_data_8[\"nominal_ratio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hapax legomena\n",
    "\n",
    "The number of words occurring only once in a text (Mellor, 2010). --> measure may also be affected by length, it was calculated for a hundred word sample\n",
    "\n",
    "Do I check the whole text or take a sample of 100 words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      140\n",
       "1      190\n",
       "2      195\n",
       "3      135\n",
       "4      107\n",
       "      ... \n",
       "718    220\n",
       "719    111\n",
       "720    247\n",
       "721    153\n",
       "722    152\n",
       "Name: hapax_legomena, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"hapax_legomena\"] = training_data_1[\"word_tok\"].apply(lambda token_list: sum([token_list.count(token) == 1 for token in token_list]))\n",
    "training_data_2[\"hapax_legomena\"] = training_data_2[\"word_tok\"].apply(lambda token_list: sum([token_list.count(token) == 1 for token in token_list]))\n",
    "training_data_3[\"hapax_legomena\"] = training_data_3[\"word_tok\"].apply(lambda token_list: sum([token_list.count(token) == 1 for token in token_list]))\n",
    "training_data_4[\"hapax_legomena\"] = training_data_4[\"word_tok\"].apply(lambda token_list: sum([token_list.count(token) == 1 for token in token_list]))\n",
    "training_data_5[\"hapax_legomena\"] = training_data_5[\"word_tok\"].apply(lambda token_list: sum([token_list.count(token) == 1 for token in token_list]))\n",
    "training_data_6[\"hapax_legomena\"] = training_data_6[\"word_tok\"].apply(lambda token_list: sum([token_list.count(token) == 1 for token in token_list]))\n",
    "training_data_7[\"hapax_legomena\"] = training_data_7[\"word_tok\"].apply(lambda token_list: sum([token_list.count(token) == 1 for token in token_list]))\n",
    "training_data_8[\"hapax_legomena\"] = training_data_8[\"word_tok\"].apply(lambda token_list: sum([token_list.count(token) == 1 for token in token_list]))\n",
    "training_data_8[\"hapax_legomena\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.349043\n",
       "1      0.350318\n",
       "2      0.335656\n",
       "3      0.318820\n",
       "4      0.272727\n",
       "         ...   \n",
       "718    0.356148\n",
       "719    0.350719\n",
       "720    0.402395\n",
       "721    0.399306\n",
       "722    0.440000\n",
       "Name: TTR, Length: 723, dtype: float64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on lemmatized tokens but I use stemmed tokens\n",
    "training_data_1[\"TTR\"] = training_data_1[\"stemmed_word_token\"].apply(ld.ttr)\n",
    "training_data_2[\"TTR\"] = training_data_2[\"stemmed_word_token\"].apply(ld.ttr)\n",
    "training_data_3[\"TTR\"] = training_data_3[\"stemmed_word_token\"].apply(ld.ttr)\n",
    "training_data_4[\"TTR\"] = training_data_4[\"stemmed_word_token\"].apply(ld.ttr)\n",
    "training_data_5[\"TTR\"] = training_data_5[\"stemmed_word_token\"].apply(ld.ttr)\n",
    "training_data_6[\"TTR\"] = training_data_6[\"stemmed_word_token\"].apply(ld.ttr)\n",
    "training_data_7[\"TTR\"] = training_data_7[\"stemmed_word_token\"].apply(ld.ttr)\n",
    "training_data_8[\"TTR\"] = training_data_8[\"stemmed_word_token\"].apply(ld.ttr)\n",
    "training_data_8[\"TTR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guiraud;s index\n",
    "\n",
    "* v - number of word types (unique_words)\n",
    "* N - number of word tokens\n",
    "\n",
    "From (Mellor, 2010)\n",
    "\n",
    "        GI = v/sqrt(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    723.000000\n",
       "mean      10.384390\n",
       "std        1.617807\n",
       "min        2.236068\n",
       "25%        9.418159\n",
       "50%       10.488751\n",
       "75%       11.387789\n",
       "max       16.709799\n",
       "Name: guirauds_index, dtype: float64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"guirauds_index\"] = training_data_1.apply(lambda x: x.unique_words / np.sqrt(len(x.word_tok)), axis = 1)\n",
    "training_data_2[\"guirauds_index\"] = training_data_2.apply(lambda x: x.unique_words / np.sqrt(len(x.word_tok)), axis = 1)\n",
    "training_data_3[\"guirauds_index\"] = training_data_3.apply(lambda x: x.unique_words / np.sqrt(len(x.word_tok)), axis = 1)\n",
    "training_data_4[\"guirauds_index\"] = training_data_4.apply(lambda x: x.unique_words / np.sqrt(len(x.word_tok)), axis = 1)\n",
    "training_data_5[\"guirauds_index\"] = training_data_5.apply(lambda x: x.unique_words / np.sqrt(len(x.word_tok)), axis = 1)\n",
    "training_data_6[\"guirauds_index\"] = training_data_6.apply(lambda x: x.unique_words / np.sqrt(len(x.word_tok)), axis = 1)\n",
    "training_data_7[\"guirauds_index\"] = training_data_7.apply(lambda x: x.unique_words / np.sqrt(len(x.word_tok)), axis = 1)\n",
    "training_data_8[\"guirauds_index\"] = training_data_8.apply(lambda x: x.unique_words / np.sqrt(len(x.word_tok)), axis = 1)\n",
    "training_data_8[\"guirauds_index\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yule;sK\n",
    "\n",
    "From (Mellor, 2010)\n",
    "\n",
    "* Vr - number of word types occuring r times in a text consisting of N token words\n",
    "\n",
    "        K = 10^4 * sum(r^2*Vr - N) / N^2  (r = 1, 2, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from: https://gist.github.com/magnusnissel/d9521cb78b9ae0b2c7d6\n",
    "\n",
    "def get_yules(tokens):\n",
    "    \"\"\" \n",
    "    Returns a tuple with Yule's K and Yule's I.\n",
    "    (cf. Oakes, M.P. 1998. Statistics for Corpus Linguistics.\n",
    "    International Journal of Applied Linguistics, Vol 10 Issue 2)\n",
    "    In production this needs exception handling.\n",
    "    \"\"\"\n",
    "\n",
    "    token_counter = collections.Counter(tok.upper() for tok in tokens)\n",
    "    m1 = sum(token_counter.values())                            # N - total number of words\n",
    "    m2 = sum([freq ** 2 for freq in token_counter.values()])    # sum(r^2*Vr)\n",
    "    if m2-m1 == 0:\n",
    "        i = m1*m1\n",
    "    else:\n",
    "        i = (m1*m1) / (m2-m1)                                       # N^2 / sum(r^2*Vr) - N\n",
    "    k = 1/i * 10000\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    723.000000\n",
       "mean     115.084929\n",
       "std       27.240115\n",
       "min       67.061224\n",
       "25%       97.553037\n",
       "50%      110.927322\n",
       "75%      126.762081\n",
       "max      400.000000\n",
       "Name: yules_k, dtype: float64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"yules_k\"] = training_data_1[\"word_tok\"].apply(get_yules)\n",
    "training_data_2[\"yules_k\"] = training_data_2[\"word_tok\"].apply(get_yules)\n",
    "training_data_3[\"yules_k\"] = training_data_3[\"word_tok\"].apply(get_yules)\n",
    "training_data_4[\"yules_k\"] = training_data_4[\"word_tok\"].apply(get_yules)\n",
    "training_data_5[\"yules_k\"] = training_data_5[\"word_tok\"].apply(get_yules)\n",
    "training_data_6[\"yules_k\"] = training_data_6[\"word_tok\"].apply(get_yules)\n",
    "training_data_7[\"yules_k\"] = training_data_7[\"word_tok\"].apply(get_yules)\n",
    "training_data_8[\"yules_k\"] = training_data_8[\"word_tok\"].apply(get_yules)\n",
    "training_data_8[\"yules_k\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The D estimate\n",
    "\n",
    "https://pypi.org/project/lexical-diversity/\n",
    "\n",
    "* Hypergeometric distribution D (HDD)\n",
    "* A more straightforward and reliable implementation of vocD (Malvern, Richards, Chipere, & Duran, 2004) as per McCarthy and Jarvis (2007, 2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.838588\n",
       "1      0.828133\n",
       "2      0.843323\n",
       "3      0.849304\n",
       "4      0.750694\n",
       "         ...   \n",
       "718    0.846849\n",
       "719    0.853738\n",
       "720    0.866497\n",
       "721    0.852410\n",
       "722    0.856804\n",
       "Name: d_estimate, Length: 723, dtype: float64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"d_estimate\"] = training_data_1[\"word_tok\"].apply(ld.hdd)\n",
    "training_data_2[\"d_estimate\"] = training_data_2[\"word_tok\"].apply(ld.hdd)\n",
    "training_data_3[\"d_estimate\"] = training_data_3[\"word_tok\"].apply(ld.hdd)\n",
    "training_data_4[\"d_estimate\"] = training_data_4[\"word_tok\"].apply(ld.hdd)\n",
    "training_data_5[\"d_estimate\"] = training_data_5[\"word_tok\"].apply(ld.hdd)\n",
    "training_data_6[\"d_estimate\"] = training_data_6[\"word_tok\"].apply(ld.hdd)\n",
    "training_data_7[\"d_estimate\"] = training_data_7[\"word_tok\"].apply(ld.hdd)\n",
    "training_data_8[\"d_estimate\"] = training_data_8[\"word_tok\"].apply(ld.hdd)\n",
    "training_data_8[\"d_estimate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of different PoS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nr_diff_pos_tags(pos_dict):\n",
    "    \"\"\"\n",
    "    Input: a dictionaty with POS tags as key and counts as value\n",
    "    Output: integer\n",
    "\n",
    "    This function returns the number of unique POS tags of a text.\n",
    "    \"\"\"\n",
    "    \n",
    "    return len(pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      15\n",
       "1      14\n",
       "2      14\n",
       "3      13\n",
       "4      15\n",
       "       ..\n",
       "718    15\n",
       "719    15\n",
       "720    14\n",
       "721    14\n",
       "722    14\n",
       "Name: nr_unique_pos, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"nr_unique_pos\"] = training_data_1[\"pos_dict\"].apply(nr_diff_pos_tags)\n",
    "training_data_2[\"nr_unique_pos\"] = training_data_2[\"pos_dict\"].apply(nr_diff_pos_tags)\n",
    "training_data_3[\"nr_unique_pos\"] = training_data_3[\"pos_dict\"].apply(nr_diff_pos_tags)\n",
    "training_data_4[\"nr_unique_pos\"] = training_data_4[\"pos_dict\"].apply(nr_diff_pos_tags)\n",
    "training_data_5[\"nr_unique_pos\"] = training_data_5[\"pos_dict\"].apply(nr_diff_pos_tags)\n",
    "training_data_6[\"nr_unique_pos\"] = training_data_6[\"pos_dict\"].apply(nr_diff_pos_tags)\n",
    "training_data_7[\"nr_unique_pos\"] = training_data_7[\"pos_dict\"].apply(nr_diff_pos_tags)\n",
    "training_data_8[\"nr_unique_pos\"] = training_data_8[\"pos_dict\"].apply(nr_diff_pos_tags)\n",
    "training_data_8[\"nr_unique_pos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct Verb form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      51\n",
       "1      71\n",
       "2      39\n",
       "3      67\n",
       "4      59\n",
       "       ..\n",
       "718    60\n",
       "719    83\n",
       "720    73\n",
       "721    44\n",
       "722    23\n",
       "Name: correct_verb_form, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"correct_verb_form\"] = training_data_1.apply(lambda x: x.VERB - x.nr_grammar_errors, axis = 1)\n",
    "training_data_2[\"correct_verb_form\"] = training_data_2.apply(lambda x: x.VERB - x.nr_grammar_errors, axis = 1)\n",
    "training_data_3[\"correct_verb_form\"] = training_data_3.apply(lambda x: x.VERB - x.nr_grammar_errors, axis = 1)\n",
    "training_data_4[\"correct_verb_form\"] = training_data_4.apply(lambda x: x.VERB - x.nr_grammar_errors, axis = 1)\n",
    "training_data_5[\"correct_verb_form\"] = training_data_5.apply(lambda x: x.VERB - x.nr_grammar_errors, axis = 1)\n",
    "training_data_6[\"correct_verb_form\"] = training_data_6.apply(lambda x: x.VERB - x.nr_grammar_errors, axis = 1)\n",
    "training_data_7[\"correct_verb_form\"] = training_data_7.apply(lambda x: x.VERB - x.nr_grammar_errors, axis = 1)\n",
    "training_data_8[\"correct_verb_form\"] = training_data_8.apply(lambda x: x.VERB - x.nr_grammar_errors, axis = 1)\n",
    "training_data_8[\"correct_verb_form\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>LIX</th>\n",
       "      <th>OVIX</th>\n",
       "      <th>nominal_ratio</th>\n",
       "      <th>hapax_legomena</th>\n",
       "      <th>TTR</th>\n",
       "      <th>guirauds_index</th>\n",
       "      <th>yules_k</th>\n",
       "      <th>d_estimate</th>\n",
       "      <th>nr_unique_pos</th>\n",
       "      <th>correct_verb_form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2978</td>\n",
       "      <td>2</td>\n",
       "      <td>Certain materials being removed from libraries...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25.340206</td>\n",
       "      <td>47.688251</td>\n",
       "      <td>0.945455</td>\n",
       "      <td>114</td>\n",
       "      <td>0.371134</td>\n",
       "      <td>9.353978</td>\n",
       "      <td>97.438623</td>\n",
       "      <td>0.849459</td>\n",
       "      <td>15</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2979</td>\n",
       "      <td>2</td>\n",
       "      <td>Write a persuasive essay to a newspaper reflec...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27.941176</td>\n",
       "      <td>38.089154</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>54</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>6.212416</td>\n",
       "      <td>193.079585</td>\n",
       "      <td>0.735807</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0      2978          2  Certain materials being removed from libraries...   \n",
       "1      2979          2  Write a persuasive essay to a newspaper reflec...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0             4.0             4.0             NaN            4.0   \n",
       "1             1.0             2.0             NaN            1.0   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...        LIX       OVIX  \\\n",
       "0             4.0             4.0            4.0  ...  25.340206  47.688251   \n",
       "1             1.0             2.0            1.0  ...  27.941176  38.089154   \n",
       "\n",
       "   nominal_ratio  hapax_legomena       TTR  guirauds_index     yules_k  \\\n",
       "0       0.945455             114  0.371134        9.353978   97.438623   \n",
       "1       0.612903              54  0.458824        6.212416  193.079585   \n",
       "\n",
       "   d_estimate  nr_unique_pos  correct_verb_form  \n",
       "0    0.849459             15                 79  \n",
       "1    0.735807             15                 28  \n",
       "\n",
       "[2 rows x 100 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two grades to predict\n",
    "training_data_2_copy = training_data_2.drop(['grade_as_feature','avg_cosine_similariy_high_grade', 'pattern_cosine', 'weighted_cosine'], axis = 1).copy()\n",
    "training_data_2_copy.head(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade_as_feature</th>\n",
       "      <th>avg_cosine_similariy_high_grade</th>\n",
       "      <th>pattern_cosine</th>\n",
       "      <th>weighted_cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>9.891167e+00</td>\n",
       "      <td>-9.891167e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>2.056349e+01</td>\n",
       "      <td>-2.056349e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>3.141830e+01</td>\n",
       "      <td>-3.141830e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>4.233766e+01</td>\n",
       "      <td>-4.233766e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>5.059815e+01</td>\n",
       "      <td>-5.059815e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>1.859834e+06</td>\n",
       "      <td>1.859834e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>1.861941e+06</td>\n",
       "      <td>1.861941e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>1.864128e+06</td>\n",
       "      <td>1.864128e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>1.866421e+06</td>\n",
       "      <td>1.866421e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>1.868618e+06</td>\n",
       "      <td>1.868618e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      grade_as_feature  avg_cosine_similariy_high_grade  pattern_cosine  \\\n",
       "0                  2.0                         0.623354    9.891167e+00   \n",
       "1                  2.0                         0.623354    2.056349e+01   \n",
       "2                  4.0                         0.623354    3.141830e+01   \n",
       "3                  4.0                         0.623354    4.233766e+01   \n",
       "4                  3.0                         0.623354    5.059815e+01   \n",
       "...                ...                              ...             ...   \n",
       "1795               3.0                         0.623354    1.859834e+06   \n",
       "1796               4.0                         0.623354    1.861941e+06   \n",
       "1797               4.0                         0.623354    1.864128e+06   \n",
       "1798               3.0                         0.623354    1.866421e+06   \n",
       "1799               4.0                         0.623354    1.868618e+06   \n",
       "\n",
       "      weighted_cosine  \n",
       "0       -9.891167e+00  \n",
       "1       -2.056349e+01  \n",
       "2       -3.141830e+01  \n",
       "3       -4.233766e+01  \n",
       "4       -5.059815e+01  \n",
       "...               ...  \n",
       "1795     1.859834e+06  \n",
       "1796     1.861941e+06  \n",
       "1797     1.864128e+06  \n",
       "1798     1.866421e+06  \n",
       "1799     1.868618e+06  \n",
       "\n",
       "[1800 rows x 4 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_2_2[['grade_as_feature','avg_cosine_similariy_high_grade', 'pattern_cosine', 'weighted_cosine']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>TTR</th>\n",
       "      <th>guirauds_index</th>\n",
       "      <th>yules_k</th>\n",
       "      <th>d_estimate</th>\n",
       "      <th>nr_unique_pos</th>\n",
       "      <th>correct_verb_form</th>\n",
       "      <th>grade_as_feature</th>\n",
       "      <th>avg_cosine_similariy_high_grade</th>\n",
       "      <th>pattern_cosine</th>\n",
       "      <th>weighted_cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2978</td>\n",
       "      <td>2</td>\n",
       "      <td>Certain materials being removed from libraries...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371134</td>\n",
       "      <td>9.353978</td>\n",
       "      <td>97.438623</td>\n",
       "      <td>0.849459</td>\n",
       "      <td>15</td>\n",
       "      <td>79</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>9.891167e+00</td>\n",
       "      <td>-9.891167e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2979</td>\n",
       "      <td>2</td>\n",
       "      <td>Write a persuasive essay to a newspaper reflec...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>6.212416</td>\n",
       "      <td>193.079585</td>\n",
       "      <td>0.735807</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>2.056349e+01</td>\n",
       "      <td>-2.056349e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2980</td>\n",
       "      <td>2</td>\n",
       "      <td>Do you think that libraries should remove cert...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475983</td>\n",
       "      <td>7.797659</td>\n",
       "      <td>112.126008</td>\n",
       "      <td>0.828178</td>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>3.141830e+01</td>\n",
       "      <td>-3.141830e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2981</td>\n",
       "      <td>2</td>\n",
       "      <td>In @DATE1's world, there are many things found...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342553</td>\n",
       "      <td>8.579540</td>\n",
       "      <td>129.651426</td>\n",
       "      <td>0.826450</td>\n",
       "      <td>14</td>\n",
       "      <td>78</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>4.233766e+01</td>\n",
       "      <td>-4.233766e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2982</td>\n",
       "      <td>2</td>\n",
       "      <td>In life you have the 'offensive things'. The l...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429561</td>\n",
       "      <td>10.764753</td>\n",
       "      <td>91.525369</td>\n",
       "      <td>0.879302</td>\n",
       "      <td>16</td>\n",
       "      <td>74</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>5.059815e+01</td>\n",
       "      <td>-5.059815e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>4773</td>\n",
       "      <td>2</td>\n",
       "      <td>The author is writting about taking books off ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335793</td>\n",
       "      <td>6.317550</td>\n",
       "      <td>224.670143</td>\n",
       "      <td>0.737169</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>1.859834e+06</td>\n",
       "      <td>1.859834e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>4774</td>\n",
       "      <td>2</td>\n",
       "      <td>I do not think that materials, such as books, ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475806</td>\n",
       "      <td>8.191508</td>\n",
       "      <td>100.156087</td>\n",
       "      <td>0.842815</td>\n",
       "      <td>15</td>\n",
       "      <td>33</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>1.861941e+06</td>\n",
       "      <td>1.861941e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>4775</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes we should keep the books,music,movies,an m...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516949</td>\n",
       "      <td>6.259907</td>\n",
       "      <td>149.382361</td>\n",
       "      <td>0.781812</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>1.864128e+06</td>\n",
       "      <td>1.864128e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>4776</td>\n",
       "      <td>2</td>\n",
       "      <td>I do believe that  book, magazines, music, mov...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285996</td>\n",
       "      <td>7.727611</td>\n",
       "      <td>129.391672</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>15</td>\n",
       "      <td>93</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>1.866421e+06</td>\n",
       "      <td>1.866421e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>4777</td>\n",
       "      <td>2</td>\n",
       "      <td>Different Then Everyone Else     @CAPS1 do peo...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293033</td>\n",
       "      <td>7.650271</td>\n",
       "      <td>127.485891</td>\n",
       "      <td>0.818042</td>\n",
       "      <td>15</td>\n",
       "      <td>67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.623354</td>\n",
       "      <td>1.868618e+06</td>\n",
       "      <td>1.868618e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id  essay_set                                              essay  \\\n",
       "0         2978          2  Certain materials being removed from libraries...   \n",
       "1         2979          2  Write a persuasive essay to a newspaper reflec...   \n",
       "2         2980          2  Do you think that libraries should remove cert...   \n",
       "3         2981          2  In @DATE1's world, there are many things found...   \n",
       "4         2982          2  In life you have the 'offensive things'. The l...   \n",
       "...        ...        ...                                                ...   \n",
       "1795      4773          2  The author is writting about taking books off ...   \n",
       "1796      4774          2  I do not think that materials, such as books, ...   \n",
       "1797      4775          2  Yes we should keep the books,music,movies,an m...   \n",
       "1798      4776          2  I do believe that  book, magazines, music, mov...   \n",
       "1799      4777          2  Different Then Everyone Else     @CAPS1 do peo...   \n",
       "\n",
       "      rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0                4.0             4.0             NaN            4.0   \n",
       "1                1.0             2.0             NaN            1.0   \n",
       "2                2.0             3.0             NaN            2.0   \n",
       "3                4.0             4.0             NaN            4.0   \n",
       "4                4.0             4.0             NaN            4.0   \n",
       "...              ...             ...             ...            ...   \n",
       "1795             3.0             2.0             NaN            3.0   \n",
       "1796             3.0             3.0             NaN            3.0   \n",
       "1797             2.0             2.0             NaN            2.0   \n",
       "1798             3.0             4.0             NaN            3.0   \n",
       "1799             3.0             3.0             NaN            3.0   \n",
       "\n",
       "      rater1_domain2  rater2_domain2  domain2_score  ...       TTR  \\\n",
       "0                4.0             4.0            4.0  ...  0.371134   \n",
       "1                1.0             2.0            1.0  ...  0.458824   \n",
       "2                3.0             3.0            3.0  ...  0.475983   \n",
       "3                4.0             4.0            4.0  ...  0.342553   \n",
       "4                4.0             4.0            4.0  ...  0.429561   \n",
       "...              ...             ...            ...  ...       ...   \n",
       "1795             3.0             2.0            3.0  ...  0.335793   \n",
       "1796             3.0             3.0            3.0  ...  0.475806   \n",
       "1797             2.0             2.0            2.0  ...  0.516949   \n",
       "1798             4.0             3.0            4.0  ...  0.285996   \n",
       "1799             2.0             3.0            2.0  ...  0.293033   \n",
       "\n",
       "      guirauds_index     yules_k  d_estimate  nr_unique_pos  \\\n",
       "0           9.353978   97.438623    0.849459             15   \n",
       "1           6.212416  193.079585    0.735807             15   \n",
       "2           7.797659  112.126008    0.828178             15   \n",
       "3           8.579540  129.651426    0.826450             14   \n",
       "4          10.764753   91.525369    0.879302             16   \n",
       "...              ...         ...         ...            ...   \n",
       "1795        6.317550  224.670143    0.737169             15   \n",
       "1796        8.191508  100.156087    0.842815             15   \n",
       "1797        6.259907  149.382361    0.781812             15   \n",
       "1798        7.727611  129.391672    0.816000             15   \n",
       "1799        7.650271  127.485891    0.818042             15   \n",
       "\n",
       "      correct_verb_form  grade_as_feature  avg_cosine_similariy_high_grade  \\\n",
       "0                    79               2.0                         0.623354   \n",
       "1                    28               2.0                         0.623354   \n",
       "2                    38               4.0                         0.623354   \n",
       "3                    78               4.0                         0.623354   \n",
       "4                    74               3.0                         0.623354   \n",
       "...                 ...               ...                              ...   \n",
       "1795                 35               3.0                         0.623354   \n",
       "1796                 33               4.0                         0.623354   \n",
       "1797                 19               4.0                         0.623354   \n",
       "1798                 93               3.0                         0.623354   \n",
       "1799                 67               4.0                         0.623354   \n",
       "\n",
       "      pattern_cosine  weighted_cosine  \n",
       "0       9.891167e+00    -9.891167e+00  \n",
       "1       2.056349e+01    -2.056349e+01  \n",
       "2       3.141830e+01    -3.141830e+01  \n",
       "3       4.233766e+01    -4.233766e+01  \n",
       "4       5.059815e+01    -5.059815e+01  \n",
       "...              ...              ...  \n",
       "1795    1.859834e+06     1.859834e+06  \n",
       "1796    1.861941e+06     1.861941e+06  \n",
       "1797    1.864128e+06     1.864128e+06  \n",
       "1798    1.866421e+06     1.866421e+06  \n",
       "1799    1.868618e+06     1.868618e+06  \n",
       "\n",
       "[1800 rows x 104 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_2_2_complete = pd.concat([training_data_2_copy, training_data_2_2[['grade_as_feature','avg_cosine_similariy_high_grade', 'pattern_cosine', 'weighted_cosine']]], axis = 1)#.copy()\n",
    "training_data_2_2_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rater1_trait1', 'rater1_trait2', 'rater1_trait3', 'rater1_trait4',\n",
       "       'rater1_trait5', 'rater1_trait6', 'rater2_trait1', 'rater2_trait2',\n",
       "       'rater2_trait3', 'rater2_trait4', 'rater2_trait5', 'rater2_trait6',\n",
       "       'rater3_trait1', 'rater3_trait2', 'rater3_trait3', 'rater3_trait4',\n",
       "       'rater3_trait5', 'rater3_trait6', 'normalised_docs', 'word_tok',\n",
       "       'sentence_tok', 'stemmed_word_token', 'stemmed_no_stopwords',\n",
       "       'error_types', 'grade', 'nr_stopwords', 'token_pos', 'pos_dict',\n",
       "       'token_tag', 'tag_dict', 'comparative_adj', 'superlative_adj',\n",
       "       'modal_aux', 'participle', 'infinitive_marker', 'verb_baseform',\n",
       "       'verb_past_tense', 'avg_tree_height', 'nr_syllables',\n",
       "       'sentence_len_words', 'avg_sentence_len_words',\n",
       "       'dale_chall_readability', 'unique_words', 'word_count',\n",
       "       'advanced_guiraud', 'nr_grammar_errors', 'nr_spelling_errors',\n",
       "       'nr_capitalization_errors', 'nr_punctuation_errors', 'text_vector',\n",
       "       'grade_as_feature', 'avg_cosine_similariy_high_grade', 'pattern_cosine',\n",
       "       'weighted_cosine', 'char_count', 'word_length', 'avg_word_len',\n",
       "       'nr_long_words', 'nr_short_words', 'most_freq_word_length',\n",
       "       'nr_sentences', 'nr_long_sentences', 'nr_short_sentences',\n",
       "       'most_freq_sentence_length', 'CCONJ', 'SCONJ', 'NUM', 'DET', 'ADP',\n",
       "       'ADJ', 'NOUN', 'PROPN', 'PRON', 'ADV', 'VERB',\n",
       "       'nr_syllables_bigger_two', 'gunning_fox_index', 'avg_nr_syllables',\n",
       "       'flesh_reading_ease', 'flesh_kincaid_grade_level',\n",
       "       'automated_readability_index', 'nr_syllables_bigger_three', 'SMOG',\n",
       "       'nr_long_words_char6', 'LIX', 'OVIX', 'nominal_ratio', 'hapax_legomena',\n",
       "       'TTR', 'guirauds_index', 'yules_k', 'd_estimate', 'nr_unique_pos',\n",
       "       'correct_verb_form'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1.columns[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\n",
    "    \n",
    "'grade',\n",
    "\n",
    "## preprocessed text\n",
    "'normalised_docs', 'word_tok', 'sentence_tok', 'stemmed_word_token', 'stemmed_no_stopwords', \n",
    "       \n",
    "## Lexical sophistication\n",
    "'nr_stopwords', 'avg_sentence_len_words', 'unique_words', 'word_count', 'char_count', 'avg_word_len', 'nr_long_words', 'nr_short_words',\n",
    "'most_freq_word_length', 'nr_sentences', 'nr_long_sentences','nr_short_sentences', 'most_freq_sentence_length', \n",
    "\n",
    "## Readability measures\n",
    "'dale_chall_readability', 'gunning_fox_index', 'flesh_reading_ease', 'flesh_kincaid_grade_level','automated_readability_index', 'SMOG', 'LIX', 'OVIX', 'nominal_ratio',\n",
    "\n",
    "## Lexical diversity\n",
    "'TTR', 'guirauds_index', 'yules_k', 'd_estimate', 'hapax_legomena', 'advanced_guiraud',\n",
    "\n",
    "## Grammar\n",
    "'nr_unique_pos', 'avg_tree_height', 'nr_grammar_errors', \"correct_verb_form\",\n",
    "\n",
    "## POS tag\n",
    "'comparative_adj', 'superlative_adj', 'modal_aux', 'participle', 'infinitive_marker', 'verb_baseform', 'verb_past_tense', \n",
    "# coordinating conjunction\n",
    "'CCONJ', \n",
    "# subordinating conjunction\n",
    "\"SCONJ\", \n",
    "# numeral\n",
    "'NUM',\n",
    "# determiner\n",
    "'DET', \n",
    "# preposition\n",
    "'ADP',\n",
    "# adjektiv\n",
    "'ADJ', \n",
    "# adverb \n",
    "'ADV', \n",
    "# pronoun\n",
    "\"PRON\",\n",
    "# proper noun\n",
    "'PROPN', \n",
    "# common noun\n",
    "'NOUN',\n",
    "\n",
    "## Mechanics\n",
    "'nr_spelling_errors', 'nr_capitalization_errors', 'nr_punctuation_errors',\n",
    "\n",
    "## Content\n",
    "'grade_as_feature','avg_cosine_similariy_high_grade', 'pattern_cosine', 'weighted_cosine']\n",
    "\n",
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_data_1 = training_data_1[columns].copy()\n",
    "final_training_data_2 = training_data_2[columns].copy()\n",
    "final_training_data_2_2  = training_data_2_2_complete[columns].copy()\n",
    "final_training_data_3 = training_data_3[columns].copy()\n",
    "final_training_data_4 = training_data_4[columns].copy()\n",
    "final_training_data_5 = training_data_5[columns].copy()\n",
    "final_training_data_6 = training_data_6[columns].copy()\n",
    "final_training_data_7 = training_data_7[columns].copy()\n",
    "final_training_data_8 = training_data_8[columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade</th>\n",
       "      <th>normalised_docs</th>\n",
       "      <th>word_tok</th>\n",
       "      <th>sentence_tok</th>\n",
       "      <th>stemmed_word_token</th>\n",
       "      <th>stemmed_no_stopwords</th>\n",
       "      <th>nr_stopwords</th>\n",
       "      <th>avg_sentence_len_words</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>nr_spelling_errors</th>\n",
       "      <th>nr_capitalization_errors</th>\n",
       "      <th>nr_punctuation_errors</th>\n",
       "      <th>grade_as_feature</th>\n",
       "      <th>avg_cosine_similariy_high_grade</th>\n",
       "      <th>pattern_cosine</th>\n",
       "      <th>weighted_cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>dear local newspaper, i think effects computer...</td>\n",
       "      <td>[Dear, local, newspaper, I, think, effects, co...</td>\n",
       "      <td>[Dear local newspaper, I think effects compute...</td>\n",
       "      <td>[dear, local, newspap, i, think, effect, compu...</td>\n",
       "      <td>[dear, local, newspap, think, effect, comput, ...</td>\n",
       "      <td>174</td>\n",
       "      <td>10.0</td>\n",
       "      <td>172</td>\n",
       "      <td>351</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>7</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.624099</td>\n",
       "      <td>5.536037</td>\n",
       "      <td>-5.536037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>dear @caps1 @caps2, i believe that using compu...</td>\n",
       "      <td>[Dear, CAPS1, CAPS2, I, believe, that, using, ...</td>\n",
       "      <td>[Dear @CAPS1, @CAPS2, I believe that using com...</td>\n",
       "      <td>[dear, caps1, caps2, i, believ, that, use, com...</td>\n",
       "      <td>[dear, caps1, caps2, believ, use, comput, bene...</td>\n",
       "      <td>181</td>\n",
       "      <td>13.0</td>\n",
       "      <td>205</td>\n",
       "      <td>423</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>99</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.624099</td>\n",
       "      <td>5.686812</td>\n",
       "      <td>-5.686812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>dear, @caps1 @caps2 @caps3 more and more peopl...</td>\n",
       "      <td>[Dear, CAPS1, CAPS2, CAPS3, More, and, more, p...</td>\n",
       "      <td>[Dear, @CAPS1 @CAPS2 @CAPS3, More and more peo...</td>\n",
       "      <td>[dear, caps1, caps2, caps3, more, and, more, p...</td>\n",
       "      <td>[dear, caps1, caps2, caps3, peopl, use, comput...</td>\n",
       "      <td>133</td>\n",
       "      <td>11.0</td>\n",
       "      <td>158</td>\n",
       "      <td>283</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.624099</td>\n",
       "      <td>9.249741</td>\n",
       "      <td>-9.249741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>dear local newspaper, @caps1 i have found that...</td>\n",
       "      <td>[Dear, Local, Newspaper, CAPS1, I, have, found...</td>\n",
       "      <td>[Dear Local Newspaper, @CAPS1, I have found th...</td>\n",
       "      <td>[dear, local, newspap, caps1, i, have, found, ...</td>\n",
       "      <td>[dear, local, newspap, caps1, found, mani, exp...</td>\n",
       "      <td>210</td>\n",
       "      <td>13.0</td>\n",
       "      <td>260</td>\n",
       "      <td>530</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>132</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.624099</td>\n",
       "      <td>12.575098</td>\n",
       "      <td>-12.575098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>dear @location1, i know having computers has a...</td>\n",
       "      <td>[Dear, LOCATION1, I, know, having, computers, ...</td>\n",
       "      <td>[Dear @LOCATION1, I know having computers has ...</td>\n",
       "      <td>[dear, location1, i, know, have, comput, has, ...</td>\n",
       "      <td>[dear, location1, know, comput, posit, effect,...</td>\n",
       "      <td>216</td>\n",
       "      <td>11.0</td>\n",
       "      <td>209</td>\n",
       "      <td>472</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>108</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.624099</td>\n",
       "      <td>15.922914</td>\n",
       "      <td>-15.922914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>8.0</td>\n",
       "      <td>dear @caps1, @caps2 several reasons on way i t...</td>\n",
       "      <td>[Dear, CAPS1, CAPS2, several, reasons, on, way...</td>\n",
       "      <td>[Dear @CAPS1, @CAPS2 several reasons on way I ...</td>\n",
       "      <td>[dear, caps1, caps2, sever, reason, on, way, i...</td>\n",
       "      <td>[dear, caps1, caps2, sever, reason, way, advan...</td>\n",
       "      <td>254</td>\n",
       "      <td>18.0</td>\n",
       "      <td>217</td>\n",
       "      <td>516</td>\n",
       "      <td>...</td>\n",
       "      <td>85</td>\n",
       "      <td>9</td>\n",
       "      <td>84</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.624099</td>\n",
       "      <td>16244.243582</td>\n",
       "      <td>16244.243582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>7.0</td>\n",
       "      <td>do a adults and kids spend to much time on the...</td>\n",
       "      <td>[Do, a, adults, and, kids, spend, to, much, ti...</td>\n",
       "      <td>[Do a adults and kids spend to much time on th...</td>\n",
       "      <td>[do, a, adult, and, kid, spend, to, much, time...</td>\n",
       "      <td>[adult, kid, spend, much, time, comput, well, ...</td>\n",
       "      <td>103</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115</td>\n",
       "      <td>214</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.624099</td>\n",
       "      <td>16597.240329</td>\n",
       "      <td>16597.240329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>8.0</td>\n",
       "      <td>my opinion is that people should have computer...</td>\n",
       "      <td>[My, opinion, is, that, people, should, have, ...</td>\n",
       "      <td>[My opinion is that people should have compute...</td>\n",
       "      <td>[my, opinion, is, that, peopl, should, have, c...</td>\n",
       "      <td>[opinion, peopl, comput, home, comput, import,...</td>\n",
       "      <td>147</td>\n",
       "      <td>16.0</td>\n",
       "      <td>104</td>\n",
       "      <td>296</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.624099</td>\n",
       "      <td>16991.053492</td>\n",
       "      <td>16991.053492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>2.0</td>\n",
       "      <td>dear readers, i think that its good and bad to...</td>\n",
       "      <td>[Dear, readers, I, think, that, its, good, and...</td>\n",
       "      <td>[Dear readers, I think that its good and bad t...</td>\n",
       "      <td>[dear, reader, i, think, that, it, good, and, ...</td>\n",
       "      <td>[dear, reader, think, good, bad, use, comput, ...</td>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.624099</td>\n",
       "      <td>17383.118759</td>\n",
       "      <td>17383.118759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>7.0</td>\n",
       "      <td>dear - local newspaper i agree thats computers...</td>\n",
       "      <td>[Dear, Local, Newspaper, I, agree, that, s, co...</td>\n",
       "      <td>[Dear - Local Newspaper I agree thats computer...</td>\n",
       "      <td>[dear, local, newspap, i, agre, that, s, compu...</td>\n",
       "      <td>[dear, local, newspap, agre, comput, good, soc...</td>\n",
       "      <td>100</td>\n",
       "      <td>12.0</td>\n",
       "      <td>129</td>\n",
       "      <td>222</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.624099</td>\n",
       "      <td>17728.389524</td>\n",
       "      <td>17728.389524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1783 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      grade                                    normalised_docs  \\\n",
       "0       8.0  dear local newspaper, i think effects computer...   \n",
       "1       9.0  dear @caps1 @caps2, i believe that using compu...   \n",
       "2       7.0  dear, @caps1 @caps2 @caps3 more and more peopl...   \n",
       "3      10.0  dear local newspaper, @caps1 i have found that...   \n",
       "4       8.0  dear @location1, i know having computers has a...   \n",
       "...     ...                                                ...   \n",
       "1778    8.0  dear @caps1, @caps2 several reasons on way i t...   \n",
       "1779    7.0  do a adults and kids spend to much time on the...   \n",
       "1780    8.0  my opinion is that people should have computer...   \n",
       "1781    2.0  dear readers, i think that its good and bad to...   \n",
       "1782    7.0  dear - local newspaper i agree thats computers...   \n",
       "\n",
       "                                               word_tok  \\\n",
       "0     [Dear, local, newspaper, I, think, effects, co...   \n",
       "1     [Dear, CAPS1, CAPS2, I, believe, that, using, ...   \n",
       "2     [Dear, CAPS1, CAPS2, CAPS3, More, and, more, p...   \n",
       "3     [Dear, Local, Newspaper, CAPS1, I, have, found...   \n",
       "4     [Dear, LOCATION1, I, know, having, computers, ...   \n",
       "...                                                 ...   \n",
       "1778  [Dear, CAPS1, CAPS2, several, reasons, on, way...   \n",
       "1779  [Do, a, adults, and, kids, spend, to, much, ti...   \n",
       "1780  [My, opinion, is, that, people, should, have, ...   \n",
       "1781  [Dear, readers, I, think, that, its, good, and...   \n",
       "1782  [Dear, Local, Newspaper, I, agree, that, s, co...   \n",
       "\n",
       "                                           sentence_tok  \\\n",
       "0     [Dear local newspaper, I think effects compute...   \n",
       "1     [Dear @CAPS1, @CAPS2, I believe that using com...   \n",
       "2     [Dear, @CAPS1 @CAPS2 @CAPS3, More and more peo...   \n",
       "3     [Dear Local Newspaper, @CAPS1, I have found th...   \n",
       "4     [Dear @LOCATION1, I know having computers has ...   \n",
       "...                                                 ...   \n",
       "1778  [Dear @CAPS1, @CAPS2 several reasons on way I ...   \n",
       "1779  [Do a adults and kids spend to much time on th...   \n",
       "1780  [My opinion is that people should have compute...   \n",
       "1781  [Dear readers, I think that its good and bad t...   \n",
       "1782  [Dear - Local Newspaper I agree thats computer...   \n",
       "\n",
       "                                     stemmed_word_token  \\\n",
       "0     [dear, local, newspap, i, think, effect, compu...   \n",
       "1     [dear, caps1, caps2, i, believ, that, use, com...   \n",
       "2     [dear, caps1, caps2, caps3, more, and, more, p...   \n",
       "3     [dear, local, newspap, caps1, i, have, found, ...   \n",
       "4     [dear, location1, i, know, have, comput, has, ...   \n",
       "...                                                 ...   \n",
       "1778  [dear, caps1, caps2, sever, reason, on, way, i...   \n",
       "1779  [do, a, adult, and, kid, spend, to, much, time...   \n",
       "1780  [my, opinion, is, that, peopl, should, have, c...   \n",
       "1781  [dear, reader, i, think, that, it, good, and, ...   \n",
       "1782  [dear, local, newspap, i, agre, that, s, compu...   \n",
       "\n",
       "                                   stemmed_no_stopwords  nr_stopwords  \\\n",
       "0     [dear, local, newspap, think, effect, comput, ...           174   \n",
       "1     [dear, caps1, caps2, believ, use, comput, bene...           181   \n",
       "2     [dear, caps1, caps2, caps3, peopl, use, comput...           133   \n",
       "3     [dear, local, newspap, caps1, found, mani, exp...           210   \n",
       "4     [dear, location1, know, comput, posit, effect,...           216   \n",
       "...                                                 ...           ...   \n",
       "1778  [dear, caps1, caps2, sever, reason, way, advan...           254   \n",
       "1779  [adult, kid, spend, much, time, comput, well, ...           103   \n",
       "1780  [opinion, peopl, comput, home, comput, import,...           147   \n",
       "1781  [dear, reader, think, good, bad, use, comput, ...             6   \n",
       "1782  [dear, local, newspap, agre, comput, good, soc...           100   \n",
       "\n",
       "      avg_sentence_len_words  unique_words  word_count  ...  PRON  PROPN  \\\n",
       "0                       10.0           172         351  ...    47      7   \n",
       "1                       13.0           205         423  ...    49      5   \n",
       "2                       11.0           158         283  ...    25      4   \n",
       "3                       13.0           260         530  ...    32     38   \n",
       "4                       11.0           209         472  ...    41      3   \n",
       "...                      ...           ...         ...  ...   ...    ...   \n",
       "1778                    18.0           217         516  ...    85      9   \n",
       "1779                     8.0           115         214  ...    28      0   \n",
       "1780                    16.0           104         296  ...    39      0   \n",
       "1781                     8.0            14          15  ...     2      0   \n",
       "1782                    12.0           129         222  ...    33      4   \n",
       "\n",
       "      NOUN  nr_spelling_errors  nr_capitalization_errors  \\\n",
       "0       80                   6                        12   \n",
       "1       99                  10                         7   \n",
       "2       72                   2                         8   \n",
       "3      132                  12                        10   \n",
       "4      108                  10                        10   \n",
       "...    ...                 ...                       ...   \n",
       "1778    84                   8                         6   \n",
       "1779    47                   7                         6   \n",
       "1780    78                   7                         0   \n",
       "1781     2                   0                         0   \n",
       "1782    35                   5                         0   \n",
       "\n",
       "      nr_punctuation_errors  grade_as_feature  \\\n",
       "0                         2               9.0   \n",
       "1                         1               9.0   \n",
       "2                         1              12.0   \n",
       "3                         6               9.0   \n",
       "4                        10              10.0   \n",
       "...                     ...               ...   \n",
       "1778                      2               8.0   \n",
       "1779                      4              10.0   \n",
       "1780                      1              10.0   \n",
       "1781                      0               9.0   \n",
       "1782                      4               9.0   \n",
       "\n",
       "      avg_cosine_similariy_high_grade  pattern_cosine  weighted_cosine  \n",
       "0                            0.624099        5.536037        -5.536037  \n",
       "1                            0.624099        5.686812        -5.686812  \n",
       "2                            0.624099        9.249741        -9.249741  \n",
       "3                            0.624099       12.575098       -12.575098  \n",
       "4                            0.624099       15.922914       -15.922914  \n",
       "...                               ...             ...              ...  \n",
       "1778                         0.624099    16244.243582     16244.243582  \n",
       "1779                         0.624099    16597.240329     16597.240329  \n",
       "1780                         0.624099    16991.053492     16991.053492  \n",
       "1781                         0.624099    17383.118759     17383.118759  \n",
       "1782                         0.624099    17728.389524     17728.389524  \n",
       "\n",
       "[1783 rows x 62 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_training_data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_data_1.to_csv(\"csv_files/features_1_training_set_1.csv\", index = False)\n",
    "final_training_data_2.to_csv(\"csv_files/features_1_training_set_2.csv\", index = False)\n",
    "final_training_data_2_2.to_csv(\"csv_files/features_1_training_set_2_2.csv\", index = False)\n",
    "final_training_data_3.to_csv(\"csv_files/features_1_training_set_3.csv\", index = False)\n",
    "final_training_data_4.to_csv(\"csv_files/features_1_training_set_4.csv\", index = False)\n",
    "final_training_data_5.to_csv(\"csv_files/features_1_training_set_5.csv\", index = False)\n",
    "final_training_data_6.to_csv(\"csv_files/features_1_training_set_6.csv\", index = False)\n",
    "final_training_data_7.to_csv(\"csv_files/features_1_training_set_7.csv\", index = False)\n",
    "final_training_data_8.to_csv(\"csv_files/features_1_training_set_8.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grade\n",
       "8.0     687\n",
       "9.0     334\n",
       "10.0    316\n",
       "7.0     135\n",
       "6.0     110\n",
       "11.0    109\n",
       "12.0     47\n",
       "4.0      17\n",
       "5.0      17\n",
       "2.0      10\n",
       "3.0       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1.value_counts(\"grade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "td = training_data_2_2.copy()\n",
    "n = \"2B\"\n",
    "\n",
    "td.groupby(\"grade\").size().plot(kind='bar',stacked=True, color = \"grey\")\n",
    "plt.title(\"DS\" + str(n))\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Grade\")\n",
    "plt.ylim(0, 900)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"kaggle_data_\" + str(n) + \"_grade_count.png\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "454b10af5dccce86783b9f8c85ade064919a97f3a1663be32e61664f18b345a3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
