{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jessi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# tokenization\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# punctuation removal\n",
    "import re\n",
    "import string \n",
    "\n",
    "# removing stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Stemming\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# tokenization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "* https://www.kaggle.com/competitions/asap-aes/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12973</th>\n",
       "      <td>21626</td>\n",
       "      <td>8</td>\n",
       "      <td>In most stories mothers and daughters are eit...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>21628</td>\n",
       "      <td>8</td>\n",
       "      <td>I never understood the meaning laughter is th...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12976</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>Trippin' on fen...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12977</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>Many people believe that laughter can improve...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12978 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                              essay  \\\n",
       "0             1          1  Dear local newspaper, I think effects computer...   \n",
       "1             2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2             3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3             4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4             5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "...         ...        ...                                                ...   \n",
       "12973     21626          8   In most stories mothers and daughters are eit...   \n",
       "12974     21628          8   I never understood the meaning laughter is th...   \n",
       "12975     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
       "12976     21630          8                                 Trippin' on fen...   \n",
       "12977     21633          8   Many people believe that laughter can improve...   \n",
       "\n",
       "       rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0                 4.0             4.0             NaN            8.0   \n",
       "1                 5.0             4.0             NaN            9.0   \n",
       "2                 4.0             3.0             NaN            7.0   \n",
       "3                 5.0             5.0             NaN           10.0   \n",
       "4                 4.0             4.0             NaN            8.0   \n",
       "...               ...             ...             ...            ...   \n",
       "12973            17.0            18.0             NaN           35.0   \n",
       "12974            15.0            17.0             NaN           32.0   \n",
       "12975            20.0            26.0            40.0           40.0   \n",
       "12976            20.0            20.0             NaN           40.0   \n",
       "12977            20.0            20.0             NaN           40.0   \n",
       "\n",
       "       rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n",
       "0                 NaN             NaN            NaN  ...            NaN   \n",
       "1                 NaN             NaN            NaN  ...            NaN   \n",
       "2                 NaN             NaN            NaN  ...            NaN   \n",
       "3                 NaN             NaN            NaN  ...            NaN   \n",
       "4                 NaN             NaN            NaN  ...            NaN   \n",
       "...               ...             ...            ...  ...            ...   \n",
       "12973             NaN             NaN            NaN  ...            4.0   \n",
       "12974             NaN             NaN            NaN  ...            4.0   \n",
       "12975             NaN             NaN            NaN  ...            5.0   \n",
       "12976             NaN             NaN            NaN  ...            4.0   \n",
       "12977             NaN             NaN            NaN  ...            4.0   \n",
       "\n",
       "       rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  \\\n",
       "0                NaN            NaN            NaN            NaN   \n",
       "1                NaN            NaN            NaN            NaN   \n",
       "2                NaN            NaN            NaN            NaN   \n",
       "3                NaN            NaN            NaN            NaN   \n",
       "4                NaN            NaN            NaN            NaN   \n",
       "...              ...            ...            ...            ...   \n",
       "12973            4.0            4.0            3.0            NaN   \n",
       "12974            4.0            4.0            3.0            NaN   \n",
       "12975            5.0            5.0            5.0            4.0   \n",
       "12976            4.0            4.0            4.0            NaN   \n",
       "12977            4.0            4.0            4.0            NaN   \n",
       "\n",
       "       rater3_trait2  rater3_trait3  rater3_trait4  rater3_trait5  \\\n",
       "0                NaN            NaN            NaN            NaN   \n",
       "1                NaN            NaN            NaN            NaN   \n",
       "2                NaN            NaN            NaN            NaN   \n",
       "3                NaN            NaN            NaN            NaN   \n",
       "4                NaN            NaN            NaN            NaN   \n",
       "...              ...            ...            ...            ...   \n",
       "12973            NaN            NaN            NaN            NaN   \n",
       "12974            NaN            NaN            NaN            NaN   \n",
       "12975            4.0            4.0            4.0            4.0   \n",
       "12976            NaN            NaN            NaN            NaN   \n",
       "12977            NaN            NaN            NaN            NaN   \n",
       "\n",
       "       rater3_trait6  \n",
       "0                NaN  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  \n",
       "...              ...  \n",
       "12973            NaN  \n",
       "12974            NaN  \n",
       "12975            4.0  \n",
       "12976            NaN  \n",
       "12977            NaN  \n",
       "\n",
       "[12978 rows x 28 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pd.read_excel(\"csv_files/Kaggle_ASAP_training_set.xlsx\")#.sample(100)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1783, 28)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1 = training_data[training_data[\"essay_set\"] == 1]\n",
    "training_data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 28)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_2 = training_data[training_data[\"essay_set\"] == 2]\n",
    "training_data_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1726, 28)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_3 = training_data[training_data[\"essay_set\"] == 3]\n",
    "training_data_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1772, 28)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_4 = training_data[training_data[\"essay_set\"] == 4]\n",
    "training_data_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1805, 28)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_5 = training_data[training_data[\"essay_set\"] == 5]\n",
    "training_data_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 28)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_6 = training_data[training_data[\"essay_set\"] == 6]\n",
    "training_data_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1569, 28)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_7 = training_data[training_data[\"essay_set\"] == 7]\n",
    "training_data_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(723, 28)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_8 = training_data[training_data[\"essay_set\"] == 8]\n",
    "training_data_8.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/2382297556.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_1[\"normalised_docs\"] = [doc.lower() for doc in training_data_1[\"essay\"]]\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/2382297556.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_2[\"normalised_docs\"] = [doc.lower() for doc in training_data_2[\"essay\"]]\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/2382297556.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_3[\"normalised_docs\"] = [doc.lower() for doc in training_data_3[\"essay\"]]\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/2382297556.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_4[\"normalised_docs\"] = [doc.lower() for doc in training_data_4[\"essay\"]]\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/2382297556.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_5[\"normalised_docs\"] = [doc.lower() for doc in training_data_5[\"essay\"]]\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/2382297556.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_6[\"normalised_docs\"] = [doc.lower() for doc in training_data_6[\"essay\"]]\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/2382297556.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_7[\"normalised_docs\"] = [doc.lower() for doc in training_data_7[\"essay\"]]\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/2382297556.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_8[\"normalised_docs\"] = [doc.lower() for doc in training_data_8[\"essay\"]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" a long time ago when i was in third grade i had a friend @person2 who's mom was in a bad mood. she never laughed and she never smiled. every time i saw her i would smile at her and all she would do was frown and keep walking. at first i didn't know she was a grouch i just thought she didn't like me or something.when @person2 told me his mom was a grouch i started to laugh and laugh. he asked me what was so funny i told him that i thought his mom didn't like me or something because every time i see his mom i would smile at her and all she will do is frown and walk away. that made my friend laugh we were cracking up so hard that we got in trouble in class.   the next day @person2 and i were eating lunch at school when he says to me &lt;hey your pretty good at making people laugh&gt;. i said &lt;no i am not my jokesare horrible&gt;. he said &lt;@caps1 lets put them to the test go up to some one new to this school&gt;. i said @caps1 so we went around the whole school looking for a new student unfortunately we couldn't find one we heard the bell ring and we ran to our class. we sat in the back of the classroom its only @person2 and i and anempty seat between us. we were excited because our teacher @person1 was going to show us a movie. @person1 got the front of the room andclass today i have an announcement we have a new student in our class say hello to @location1. @organization1 walked through the door. @person1 told @organization1 shecould sit in the back in between @person2 and i. she sat down turned to the both of us and said hello. @person2 gave me a look that said tell her the joke and @caps3 him a thumbs up. i turned to @location1 and said hi i'm @caps4 do you want to hear a joke. @organization1 said yeah sure. i started &lt;@caps5 knockshe said &lt;who's therei said &lt;booshe said &lt;boo who?i said &lt;oh don't cry i am right here&gt;. at first she didn't laugh because she didn't get it but duringthe middle of the movie she said &lt;ohhhh i get itand she started to laugh. @person2 turned to me and said &lt;i told you so&gt;.   @person2 got this crazy idea that if i spent the night at his house that i could make his mom laugh or at least make her smile. i said &lt;@caps1 sounds like a plan&gt;. i asked mt mom if it was @caps1 if i could spend the night at @person2 house she said &lt;yeah just make sure its @caps1 with his mom&gt;. @person2 asked his mom she said it was @caps1. when i got to @person2 house the first thing we did was play video games. when it was dinner time we all sat down at the table to eat @person2 and i were on one side and his parents on the other. when we started eating @person2 told me to tell the joke to his parents i said @caps1. so i said to them &lt;@caps5 @caps5&gt;they replied &lt;who's therei said &lt;boothey said &lt;boo who?i said &lt;oh don't cry i am right here&gt;. his parents started to laugh and laugh and they keptlaughing for like five minutes. @person2 turned to me and yelled it worked his mom asked what worked. @person2 explained everything to them. his mom told usthat her mom had recently died and that's why she was in a bad mood. after dinner we went to bed and fell asleep. the next morning when my mom came to pick me up she asked me how the sleep over went. i said it was fun i made people laugh. she said @caps1 but laughter isn't going to help you clean your roomall i could say was gosh darn it.             fin\""
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"normalised_docs\"] = [doc.lower() for doc in training_data_1[\"essay\"]]\n",
    "training_data_2[\"normalised_docs\"] = [doc.lower() for doc in training_data_2[\"essay\"]]\n",
    "training_data_3[\"normalised_docs\"] = [doc.lower() for doc in training_data_3[\"essay\"]]\n",
    "training_data_4[\"normalised_docs\"] = [doc.lower() for doc in training_data_4[\"essay\"]]\n",
    "training_data_5[\"normalised_docs\"] = [doc.lower() for doc in training_data_5[\"essay\"]]\n",
    "training_data_6[\"normalised_docs\"] = [doc.lower() for doc in training_data_6[\"essay\"]]\n",
    "training_data_7[\"normalised_docs\"] = [doc.lower() for doc in training_data_7[\"essay\"]]\n",
    "training_data_8[\"normalised_docs\"] = [doc.lower() for doc in training_data_8[\"essay\"]]\n",
    "\n",
    "training_data_8[\"normalised_docs\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_english = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(tok):\n",
    "    return tok.translate(str.maketrans('', '', string.punctuation + \"’\"))\n",
    "\n",
    "remove_punctuation(\"..hi...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/755971060.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_1[\"word_tok\"] = training_data_1[\"essay\"].apply(lambda text: list(filter(None, [remove_punctuation(word.text) for word in nlp_english(text) if len(word.text.strip()) > 0 and not word.text.isdigit()])))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/755971060.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_2[\"word_tok\"] = training_data_2[\"essay\"].apply(lambda text: list(filter(None, [remove_punctuation(word.text) for word in nlp_english(text) if len(word.text.strip()) > 0 and not word.text.isdigit()])))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/755971060.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_3[\"word_tok\"] = training_data_3[\"essay\"].apply(lambda text: list(filter(None, [remove_punctuation(word.text) for word in nlp_english(text) if len(word.text.strip()) > 0 and not word.text.isdigit()])))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/755971060.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_4[\"word_tok\"] = training_data_4[\"essay\"].apply(lambda text: list(filter(None, [remove_punctuation(word.text) for word in nlp_english(text) if len(word.text.strip()) > 0 and not word.text.isdigit()])))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/755971060.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_5[\"word_tok\"] = training_data_5[\"essay\"].apply(lambda text: list(filter(None, [remove_punctuation(word.text) for word in nlp_english(text) if len(word.text.strip()) > 0 and not word.text.isdigit()])))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/755971060.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_6[\"word_tok\"] = training_data_6[\"essay\"].apply(lambda text: list(filter(None, [remove_punctuation(word.text) for word in nlp_english(text) if len(word.text.strip()) > 0 and not word.text.isdigit()])))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/755971060.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_7[\"word_tok\"] = training_data_7[\"essay\"].apply(lambda text: list(filter(None, [remove_punctuation(word.text) for word in nlp_english(text) if len(word.text.strip()) > 0 and not word.text.isdigit()])))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/755971060.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_8[\"word_tok\"] = training_data_8[\"essay\"].apply(lambda text: list(filter(None, [remove_punctuation(word.text) for word in nlp_english(text) if len(word.text.strip()) > 0 and not word.text.isdigit()])))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'long',\n",
       " 'time',\n",
       " 'ago',\n",
       " 'when',\n",
       " 'I',\n",
       " 'was',\n",
       " 'in',\n",
       " 'third',\n",
       " 'grade',\n",
       " 'I',\n",
       " 'had',\n",
       " 'a',\n",
       " 'friend',\n",
       " 'PERSON2',\n",
       " 'who',\n",
       " 's',\n",
       " 'mom',\n",
       " 'was',\n",
       " 'in',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'mood',\n",
       " 'She',\n",
       " 'never',\n",
       " 'laughed',\n",
       " 'and',\n",
       " 'she',\n",
       " 'never',\n",
       " 'smiled',\n",
       " 'Every',\n",
       " 'time',\n",
       " 'I',\n",
       " 'saw',\n",
       " 'her',\n",
       " 'I',\n",
       " 'would',\n",
       " 'smile',\n",
       " 'at',\n",
       " 'her',\n",
       " 'and',\n",
       " 'all',\n",
       " 'she',\n",
       " 'would',\n",
       " 'do',\n",
       " 'was',\n",
       " 'frown',\n",
       " 'and',\n",
       " 'keep',\n",
       " 'walking',\n",
       " 'At',\n",
       " 'first',\n",
       " 'I',\n",
       " 'did',\n",
       " 'nt',\n",
       " 'know',\n",
       " 'she',\n",
       " 'was',\n",
       " 'a',\n",
       " 'grouch',\n",
       " 'i',\n",
       " 'just',\n",
       " 'thought',\n",
       " 'she',\n",
       " 'did',\n",
       " 'nt',\n",
       " 'like',\n",
       " 'me',\n",
       " 'or',\n",
       " 'something',\n",
       " 'When',\n",
       " 'PERSON2',\n",
       " 'told',\n",
       " 'me',\n",
       " 'his',\n",
       " 'mom',\n",
       " 'was',\n",
       " 'a',\n",
       " 'grouch',\n",
       " 'I',\n",
       " 'started',\n",
       " 'to',\n",
       " 'laugh',\n",
       " 'and',\n",
       " 'laugh',\n",
       " 'He',\n",
       " 'asked',\n",
       " 'me',\n",
       " 'what',\n",
       " 'was',\n",
       " 'so',\n",
       " 'funny',\n",
       " 'i',\n",
       " 'told',\n",
       " 'him',\n",
       " 'that',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'his',\n",
       " 'mom',\n",
       " 'did',\n",
       " 'nt',\n",
       " 'like',\n",
       " 'me',\n",
       " 'or',\n",
       " 'something',\n",
       " 'because',\n",
       " 'every',\n",
       " 'time',\n",
       " 'I',\n",
       " 'see',\n",
       " 'his',\n",
       " 'mom',\n",
       " 'I',\n",
       " 'would',\n",
       " 'smile',\n",
       " 'at',\n",
       " 'her',\n",
       " 'and',\n",
       " 'all',\n",
       " 'she',\n",
       " 'will',\n",
       " 'do',\n",
       " 'is',\n",
       " 'frown',\n",
       " 'and',\n",
       " 'walk',\n",
       " 'away',\n",
       " 'That',\n",
       " 'made',\n",
       " 'my',\n",
       " 'friend',\n",
       " 'laugh',\n",
       " 'we',\n",
       " 'were',\n",
       " 'cracking',\n",
       " 'up',\n",
       " 'so',\n",
       " 'hard',\n",
       " 'that',\n",
       " 'we',\n",
       " 'got',\n",
       " 'in',\n",
       " 'trouble',\n",
       " 'in',\n",
       " 'class',\n",
       " 'The',\n",
       " 'next',\n",
       " 'day',\n",
       " 'PERSON2',\n",
       " 'and',\n",
       " 'I',\n",
       " 'were',\n",
       " 'eating',\n",
       " 'lunch',\n",
       " 'at',\n",
       " 'school',\n",
       " 'when',\n",
       " 'he',\n",
       " 'says',\n",
       " 'to',\n",
       " 'me',\n",
       " 'lthey',\n",
       " 'your',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'at',\n",
       " 'making',\n",
       " 'people',\n",
       " 'laughgt',\n",
       " 'I',\n",
       " 'said',\n",
       " 'ltno',\n",
       " 'I',\n",
       " 'am',\n",
       " 'not',\n",
       " 'my',\n",
       " 'jokesare',\n",
       " 'horriblegt',\n",
       " 'He',\n",
       " 'said',\n",
       " 'ltCAPS1',\n",
       " 'lets',\n",
       " 'put',\n",
       " 'them',\n",
       " 'to',\n",
       " 'the',\n",
       " 'test',\n",
       " 'go',\n",
       " 'up',\n",
       " 'to',\n",
       " 'some',\n",
       " 'one',\n",
       " 'new',\n",
       " 'to',\n",
       " 'this',\n",
       " 'schoolgt',\n",
       " 'I',\n",
       " 'said',\n",
       " 'CAPS1',\n",
       " 'so',\n",
       " 'we',\n",
       " 'went',\n",
       " 'around',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'school',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'a',\n",
       " 'new',\n",
       " 'student',\n",
       " 'unfortunately',\n",
       " 'we',\n",
       " 'could',\n",
       " 'nt',\n",
       " 'find',\n",
       " 'one',\n",
       " 'we',\n",
       " 'heard',\n",
       " 'the',\n",
       " 'bell',\n",
       " 'ring',\n",
       " 'and',\n",
       " 'we',\n",
       " 'ran',\n",
       " 'to',\n",
       " 'our',\n",
       " 'class',\n",
       " 'We',\n",
       " 'sat',\n",
       " 'in',\n",
       " 'the',\n",
       " 'back',\n",
       " 'of',\n",
       " 'the',\n",
       " 'classroom',\n",
       " 'its',\n",
       " 'only',\n",
       " 'PERSON2',\n",
       " 'and',\n",
       " 'I',\n",
       " 'and',\n",
       " 'anempty',\n",
       " 'seat',\n",
       " 'between',\n",
       " 'us',\n",
       " 'We',\n",
       " 'were',\n",
       " 'excited',\n",
       " 'because',\n",
       " 'our',\n",
       " 'teacher',\n",
       " 'PERSON1',\n",
       " 'was',\n",
       " 'going',\n",
       " 'to',\n",
       " 'show',\n",
       " 'us',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'PERSON1',\n",
       " 'got',\n",
       " 'the',\n",
       " 'front',\n",
       " 'of',\n",
       " 'the',\n",
       " 'room',\n",
       " 'andclass',\n",
       " 'today',\n",
       " 'I',\n",
       " 'have',\n",
       " 'an',\n",
       " 'announcement',\n",
       " 'we',\n",
       " 'have',\n",
       " 'a',\n",
       " 'new',\n",
       " 'student',\n",
       " 'in',\n",
       " 'our',\n",
       " 'class',\n",
       " 'say',\n",
       " 'hello',\n",
       " 'to',\n",
       " 'LOCATION1',\n",
       " 'ORGANIZATION1',\n",
       " 'walked',\n",
       " 'through',\n",
       " 'the',\n",
       " 'door',\n",
       " 'PERSON1',\n",
       " 'told',\n",
       " 'ORGANIZATION1',\n",
       " 'shecould',\n",
       " 'sit',\n",
       " 'in',\n",
       " 'the',\n",
       " 'back',\n",
       " 'in',\n",
       " 'between',\n",
       " 'PERSON2',\n",
       " 'and',\n",
       " 'I',\n",
       " 'She',\n",
       " 'sat',\n",
       " 'down',\n",
       " 'turned',\n",
       " 'to',\n",
       " 'the',\n",
       " 'both',\n",
       " 'of',\n",
       " 'us',\n",
       " 'and',\n",
       " 'said',\n",
       " 'hello',\n",
       " 'PERSON2',\n",
       " 'gave',\n",
       " 'me',\n",
       " 'a',\n",
       " 'look',\n",
       " 'that',\n",
       " 'said',\n",
       " 'tell',\n",
       " 'her',\n",
       " 'the',\n",
       " 'joke',\n",
       " 'and',\n",
       " 'CAPS3',\n",
       " 'him',\n",
       " 'a',\n",
       " 'thumbs',\n",
       " 'up',\n",
       " 'I',\n",
       " 'turned',\n",
       " 'to',\n",
       " 'LOCATION1',\n",
       " 'and',\n",
       " 'said',\n",
       " 'hi',\n",
       " 'I',\n",
       " 'm',\n",
       " 'CAPS4',\n",
       " 'do',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'hear',\n",
       " 'a',\n",
       " 'joke',\n",
       " 'ORGANIZATION1',\n",
       " 'said',\n",
       " 'yeah',\n",
       " 'sure',\n",
       " 'I',\n",
       " 'started',\n",
       " 'ltCAPS5',\n",
       " 'knockshe',\n",
       " 'said',\n",
       " 'ltwho',\n",
       " 's',\n",
       " 'thereI',\n",
       " 'said',\n",
       " 'ltbooshe',\n",
       " 'said',\n",
       " 'ltboo',\n",
       " 'whoI',\n",
       " 'said',\n",
       " 'ltoh',\n",
       " 'do',\n",
       " 'nt',\n",
       " 'cry',\n",
       " 'I',\n",
       " 'am',\n",
       " 'right',\n",
       " 'heregt',\n",
       " 'At',\n",
       " 'first',\n",
       " 'she',\n",
       " 'did',\n",
       " 'nt',\n",
       " 'laugh',\n",
       " 'because',\n",
       " 'she',\n",
       " 'did',\n",
       " 'nt',\n",
       " 'get',\n",
       " 'it',\n",
       " 'but',\n",
       " 'duringthe',\n",
       " 'middle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'she',\n",
       " 'said',\n",
       " 'ltohhhh',\n",
       " 'I',\n",
       " 'get',\n",
       " 'itand',\n",
       " 'she',\n",
       " 'started',\n",
       " 'to',\n",
       " 'laugh',\n",
       " 'PERSON2',\n",
       " 'turned',\n",
       " 'to',\n",
       " 'me',\n",
       " 'and',\n",
       " 'said',\n",
       " 'ltI',\n",
       " 'told',\n",
       " 'you',\n",
       " 'sogt',\n",
       " 'PERSON2',\n",
       " 'got',\n",
       " 'this',\n",
       " 'crazy',\n",
       " 'idea',\n",
       " 'that',\n",
       " 'if',\n",
       " 'I',\n",
       " 'spent',\n",
       " 'the',\n",
       " 'night',\n",
       " 'at',\n",
       " 'his',\n",
       " 'house',\n",
       " 'that',\n",
       " 'I',\n",
       " 'could',\n",
       " 'make',\n",
       " 'his',\n",
       " 'mom',\n",
       " 'laugh',\n",
       " 'or',\n",
       " 'at',\n",
       " 'least',\n",
       " 'make',\n",
       " 'her',\n",
       " 'smile',\n",
       " 'I',\n",
       " 'said',\n",
       " 'ltCAPS1',\n",
       " 'sounds',\n",
       " 'like',\n",
       " 'a',\n",
       " 'plangt',\n",
       " 'I',\n",
       " 'asked',\n",
       " 'mt',\n",
       " 'mom',\n",
       " 'if',\n",
       " 'it',\n",
       " 'was',\n",
       " 'CAPS1',\n",
       " 'if',\n",
       " 'I',\n",
       " 'could',\n",
       " 'spend',\n",
       " 'the',\n",
       " 'night',\n",
       " 'at',\n",
       " 'PERSON2',\n",
       " 'house',\n",
       " 'she',\n",
       " 'said',\n",
       " 'ltyeah',\n",
       " 'just',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'its',\n",
       " 'CAPS1',\n",
       " 'with',\n",
       " 'his',\n",
       " 'momgt',\n",
       " 'PERSON2',\n",
       " 'asked',\n",
       " 'his',\n",
       " 'mom',\n",
       " 'she',\n",
       " 'said',\n",
       " 'it',\n",
       " 'was',\n",
       " 'CAPS1',\n",
       " 'When',\n",
       " 'I',\n",
       " 'got',\n",
       " 'to',\n",
       " 'PERSON2',\n",
       " 'house',\n",
       " 'the',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'we',\n",
       " 'did',\n",
       " 'was',\n",
       " 'play',\n",
       " 'video',\n",
       " 'games',\n",
       " 'When',\n",
       " 'it',\n",
       " 'was',\n",
       " 'dinner',\n",
       " 'time',\n",
       " 'we',\n",
       " 'all',\n",
       " 'sat',\n",
       " 'down',\n",
       " 'at',\n",
       " 'the',\n",
       " 'table',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'PERSON2',\n",
       " 'and',\n",
       " 'I',\n",
       " 'were',\n",
       " 'on',\n",
       " 'one',\n",
       " 'side',\n",
       " 'and',\n",
       " 'his',\n",
       " 'parents',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other',\n",
       " 'When',\n",
       " 'we',\n",
       " 'started',\n",
       " 'eating',\n",
       " 'PERSON2',\n",
       " 'told',\n",
       " 'me',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'the',\n",
       " 'joke',\n",
       " 'to',\n",
       " 'his',\n",
       " 'parents',\n",
       " 'i',\n",
       " 'said',\n",
       " 'CAPS1',\n",
       " 'So',\n",
       " 'I',\n",
       " 'said',\n",
       " 'to',\n",
       " 'them',\n",
       " 'ltCAPS5',\n",
       " 'CAPS5gtthey',\n",
       " 'replied',\n",
       " 'ltwho',\n",
       " 's',\n",
       " 'thereI',\n",
       " 'said',\n",
       " 'ltboothey',\n",
       " 'said',\n",
       " 'ltboo',\n",
       " 'whoI',\n",
       " 'said',\n",
       " 'ltoh',\n",
       " 'do',\n",
       " 'nt',\n",
       " 'cry',\n",
       " 'I',\n",
       " 'am',\n",
       " 'right',\n",
       " 'heregt',\n",
       " 'His',\n",
       " 'parents',\n",
       " 'started',\n",
       " 'to',\n",
       " 'laugh',\n",
       " 'and',\n",
       " 'laugh',\n",
       " 'and',\n",
       " 'they',\n",
       " 'keptlaughing',\n",
       " 'for',\n",
       " 'like',\n",
       " 'five',\n",
       " 'minutes',\n",
       " 'PERSON2',\n",
       " 'turned',\n",
       " 'to',\n",
       " 'me',\n",
       " 'and',\n",
       " 'yelled',\n",
       " 'it',\n",
       " 'worked',\n",
       " 'his',\n",
       " 'mom',\n",
       " 'asked',\n",
       " 'what',\n",
       " 'worked',\n",
       " 'PERSON2',\n",
       " 'explained',\n",
       " 'everything',\n",
       " 'to',\n",
       " 'them',\n",
       " 'His',\n",
       " 'mom',\n",
       " 'told',\n",
       " 'usthat',\n",
       " 'her',\n",
       " 'mom',\n",
       " 'had',\n",
       " 'recently',\n",
       " 'died',\n",
       " 'and',\n",
       " 'that',\n",
       " 's',\n",
       " 'why',\n",
       " 'she',\n",
       " 'was',\n",
       " 'in',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'mood',\n",
       " 'After',\n",
       " 'dinner',\n",
       " 'we',\n",
       " 'went',\n",
       " 'to',\n",
       " 'bed',\n",
       " 'and',\n",
       " 'fell',\n",
       " 'asleep',\n",
       " 'The',\n",
       " 'next',\n",
       " 'morning',\n",
       " 'when',\n",
       " 'my',\n",
       " 'mom',\n",
       " 'came',\n",
       " 'to',\n",
       " 'pick',\n",
       " 'me',\n",
       " 'up',\n",
       " 'she',\n",
       " 'asked',\n",
       " 'me',\n",
       " 'how',\n",
       " 'the',\n",
       " 'sleep',\n",
       " 'over',\n",
       " 'went',\n",
       " 'I',\n",
       " 'said',\n",
       " 'it',\n",
       " 'was',\n",
       " 'fun',\n",
       " 'I',\n",
       " 'made',\n",
       " 'people',\n",
       " 'laugh',\n",
       " 'She',\n",
       " 'said',\n",
       " 'CAPS1',\n",
       " 'but',\n",
       " 'laughter',\n",
       " 'is',\n",
       " 'nt',\n",
       " 'going',\n",
       " 'to',\n",
       " 'help',\n",
       " 'you',\n",
       " 'clean',\n",
       " 'your',\n",
       " 'roomall',\n",
       " 'i',\n",
       " 'could',\n",
       " 'say',\n",
       " 'was',\n",
       " 'gosh',\n",
       " 'darn',\n",
       " 'it',\n",
       " 'FIN']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"word_tok\"] = training_data_1[\"essay\"].apply(lambda text: list(filter(None, [remove_punctuation(word.text) for word in nlp_english(text) if len(word.text.strip()) > 0 and not word.text.isdigit()])))\n",
    "training_data_2[\"word_tok\"] = training_data_2[\"essay\"].apply(lambda text: list(filter(None, [remove_punctuation(word.text) for word in nlp_english(text) if len(word.text.strip()) > 0 and not word.text.isdigit()])))\n",
    "training_data_3[\"word_tok\"] = training_data_3[\"essay\"].apply(lambda text: list(filter(None, [remove_punctuation(word.text) for word in nlp_english(text) if len(word.text.strip()) > 0 and not word.text.isdigit()])))\n",
    "training_data_4[\"word_tok\"] = training_data_4[\"essay\"].apply(lambda text: list(filter(None, [remove_punctuation(word.text) for word in nlp_english(text) if len(word.text.strip()) > 0 and not word.text.isdigit()])))\n",
    "training_data_5[\"word_tok\"] = training_data_5[\"essay\"].apply(lambda text: list(filter(None, [remove_punctuation(word.text) for word in nlp_english(text) if len(word.text.strip()) > 0 and not word.text.isdigit()])))\n",
    "training_data_6[\"word_tok\"] = training_data_6[\"essay\"].apply(lambda text: list(filter(None, [remove_punctuation(word.text) for word in nlp_english(text) if len(word.text.strip()) > 0 and not word.text.isdigit()])))\n",
    "training_data_7[\"word_tok\"] = training_data_7[\"essay\"].apply(lambda text: list(filter(None, [remove_punctuation(word.text) for word in nlp_english(text) if len(word.text.strip()) > 0 and not word.text.isdigit()])))\n",
    "training_data_8[\"word_tok\"] = training_data_8[\"essay\"].apply(lambda text: list(filter(None, [remove_punctuation(word.text) for word in nlp_english(text) if len(word.text.strip()) > 0 and not word.text.isdigit()])))\n",
    "\n",
    "training_data_8[\"word_tok\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/473563036.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_1[\"sentence_tok\"] = training_data_1[\"essay\"].apply(lambda text: list(filter(None, [sent.text.strip() for sent in nlp_english(text).sents if len(sent.text.strip()) > 0 and not sent.text.isdigit()])))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/473563036.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_2[\"sentence_tok\"] = training_data_2[\"essay\"].apply(lambda text: list(filter(None, [sent.text.strip() for sent in nlp_english(text).sents if len(sent.text.strip()) > 0 and not sent.text.isdigit()])))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/473563036.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_3[\"sentence_tok\"] = training_data_3[\"essay\"].apply(lambda text: list(filter(None, [sent.text.strip() for sent in nlp_english(text).sents if len(sent.text.strip()) > 0 and not sent.text.isdigit()])))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/473563036.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_4[\"sentence_tok\"] = training_data_4[\"essay\"].apply(lambda text: list(filter(None, [sent.text.strip() for sent in nlp_english(text).sents if len(sent.text.strip()) > 0 and not sent.text.isdigit()])))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/473563036.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_5[\"sentence_tok\"] = training_data_5[\"essay\"].apply(lambda text: list(filter(None, [sent.text.strip() for sent in nlp_english(text).sents if len(sent.text.strip()) > 0 and not sent.text.isdigit()])))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/473563036.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_6[\"sentence_tok\"] = training_data_6[\"essay\"].apply(lambda text: list(filter(None, [sent.text.strip() for sent in nlp_english(text).sents if len(sent.text.strip()) > 0 and not sent.text.isdigit()])))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/473563036.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_7[\"sentence_tok\"] = training_data_7[\"essay\"].apply(lambda text: list(filter(None, [sent.text.strip() for sent in nlp_english(text).sents if len(sent.text.strip()) > 0 and not sent.text.isdigit()])))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/473563036.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_8[\"sentence_tok\"] = training_data_8[\"essay\"].apply(lambda text: list(filter(None, [sent.text.strip() for sent in nlp_english(text).sents if len(sent.text.strip()) > 0 and not sent.text.isdigit()])))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"A long time ago when I was in third grade I had a friend @PERSON2 who's mom was in a bad mood.\",\n",
       " 'She never laughed and she never smiled.',\n",
       " 'Every time I saw her I would smile at her and all she would do was frown and keep walking.',\n",
       " \"At first I didn't know she was a grouch i just thought she didn't like me or something.\",\n",
       " 'When @PERSON2 told me his mom was a grouch I started to laugh and laugh.',\n",
       " \"He asked me what was so funny i told him that i thought his mom didn't like me or something because every time I see his mom I would smile at her and all she will do is frown and walk away.\",\n",
       " 'That made my friend laugh we were cracking up so hard that we got in trouble in class.',\n",
       " 'The next day @PERSON2',\n",
       " 'and I were eating lunch at school when he says to me &lt;hey your pretty good at making people laugh&gt;.',\n",
       " 'I said &lt;no',\n",
       " 'I am not my jokesare horrible&gt;.',\n",
       " 'He said &lt;@CAPS1 lets put them to the test go up to some one new to this school&gt;.',\n",
       " 'I said @CAPS1',\n",
       " \"so we went around the whole school looking for a new student unfortunately we couldn't find one we heard the bell ring and we ran to our class.\",\n",
       " 'We sat in the back of the classroom its only @PERSON2 and I and anempty seat between us.',\n",
       " 'We were excited because our teacher @PERSON1 was going to show us a movie.',\n",
       " '@PERSON1 got the front of the room andclass today I have an announcement we have a new student in our class say hello to @LOCATION1.',\n",
       " '@ORGANIZATION1 walked through the door.',\n",
       " '@PERSON1 told @ORGANIZATION1 shecould sit in the back in between @PERSON2 and I.',\n",
       " 'She sat down turned to the both of us and said hello.',\n",
       " '@PERSON2 gave me a look that said tell her the joke and @CAPS3 him a thumbs up.',\n",
       " \"I turned to @LOCATION1 and said hi I'm @CAPS4 do you want to hear a joke.\",\n",
       " '@ORGANIZATION1 said yeah',\n",
       " 'sure.',\n",
       " \"I started &lt;@CAPS5 knockshe said &lt;who's thereI said &lt;booshe said &lt;boo who?I said &lt;oh don't cry I am right here&gt;.\",\n",
       " \"At first she didn't laugh because she didn't get it but duringthe middle of the movie she said &lt;ohhhh I get itand she started to laugh.\",\n",
       " '@PERSON2 turned to me and said &lt;I told you so&gt;.',\n",
       " '@PERSON2 got this crazy idea that if I spent the night at his house that I could make his mom laugh or at least make her smile.',\n",
       " 'I said &lt;@CAPS1 sounds like a plan&gt;.',\n",
       " 'I asked mt mom if it was @CAPS1 if I could spend the night at @PERSON2 house she said &lt;yeah just make sure its @CAPS1 with his mom&gt;.',\n",
       " '@PERSON2 asked his mom she said it was @CAPS1.',\n",
       " 'When I got to @PERSON2 house the first thing we did was play video games.',\n",
       " 'When it was dinner time we all sat down at the table to eat @PERSON2',\n",
       " 'and I were on one side and his parents on the other.',\n",
       " 'When we started eating @PERSON2 told me to tell the joke to his parents i said @CAPS1.',\n",
       " \"So I said to them &lt;@CAPS5 @CAPS5&gt;they replied &lt;who's thereI said &lt;boothey said &lt;boo who?I said &lt;oh don't cry I am right here&gt;.\",\n",
       " 'His parents started to laugh and laugh and they keptlaughing for like five minutes.',\n",
       " '@PERSON2 turned to me and yelled it worked his mom asked what worked.',\n",
       " '@PERSON2 explained everything to them.',\n",
       " \"His mom told usthat her mom had recently died and that's why she was in a bad mood.\",\n",
       " 'After dinner we went to bed and fell asleep.',\n",
       " 'The next morning when my mom came to pick me up she asked me how the sleep over went.',\n",
       " 'I said it was fun I made people laugh.',\n",
       " \"She said @CAPS1 but laughter isn't going to help you clean your roomall i could say was gosh darn it.\",\n",
       " 'FIN']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"sentence_tok\"] = training_data_1[\"essay\"].apply(lambda text: list(filter(None, [sent.text.strip() for sent in nlp_english(text).sents if len(sent.text.strip()) > 0 and not sent.text.isdigit()])))\n",
    "training_data_2[\"sentence_tok\"] = training_data_2[\"essay\"].apply(lambda text: list(filter(None, [sent.text.strip() for sent in nlp_english(text).sents if len(sent.text.strip()) > 0 and not sent.text.isdigit()])))\n",
    "training_data_3[\"sentence_tok\"] = training_data_3[\"essay\"].apply(lambda text: list(filter(None, [sent.text.strip() for sent in nlp_english(text).sents if len(sent.text.strip()) > 0 and not sent.text.isdigit()])))\n",
    "training_data_4[\"sentence_tok\"] = training_data_4[\"essay\"].apply(lambda text: list(filter(None, [sent.text.strip() for sent in nlp_english(text).sents if len(sent.text.strip()) > 0 and not sent.text.isdigit()])))\n",
    "training_data_5[\"sentence_tok\"] = training_data_5[\"essay\"].apply(lambda text: list(filter(None, [sent.text.strip() for sent in nlp_english(text).sents if len(sent.text.strip()) > 0 and not sent.text.isdigit()])))\n",
    "training_data_6[\"sentence_tok\"] = training_data_6[\"essay\"].apply(lambda text: list(filter(None, [sent.text.strip() for sent in nlp_english(text).sents if len(sent.text.strip()) > 0 and not sent.text.isdigit()])))\n",
    "training_data_7[\"sentence_tok\"] = training_data_7[\"essay\"].apply(lambda text: list(filter(None, [sent.text.strip() for sent in nlp_english(text).sents if len(sent.text.strip()) > 0 and not sent.text.isdigit()]))) \n",
    "training_data_8[\"sentence_tok\"] = training_data_8[\"essay\"].apply(lambda text: list(filter(None, [sent.text.strip() for sent in nlp_english(text).sents if len(sent.text.strip()) > 0 and not sent.text.isdigit()])))\n",
    "\n",
    "training_data_8[\"sentence_tok\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_stem(text, language):\n",
    "\n",
    "    snow_stem = SnowballStemmer(language)\n",
    "    stemmed_doc = []\n",
    "\n",
    "    for word in text:\n",
    "        stemmed_doc.append(snow_stem.stem(word))\n",
    "\n",
    "    return stemmed_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/2759981228.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_1[\"stemmed_word_token\"] = training_data_1[\"word_tok\"].apply(get_word_stem, args = (\"english\",))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/2759981228.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_2[\"stemmed_word_token\"] = training_data_2[\"word_tok\"].apply(get_word_stem, args = (\"english\",))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/2759981228.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_3[\"stemmed_word_token\"] = training_data_3[\"word_tok\"].apply(get_word_stem, args = (\"english\",))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/2759981228.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_4[\"stemmed_word_token\"] = training_data_4[\"word_tok\"].apply(get_word_stem, args = (\"english\",))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/2759981228.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_5[\"stemmed_word_token\"] = training_data_5[\"word_tok\"].apply(get_word_stem, args = (\"english\",))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/2759981228.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_6[\"stemmed_word_token\"] = training_data_6[\"word_tok\"].apply(get_word_stem, args = (\"english\",))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/2759981228.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_7[\"stemmed_word_token\"] = training_data_7[\"word_tok\"].apply(get_word_stem, args = (\"english\",))\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/2759981228.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_8[\"stemmed_word_token\"] = training_data_8[\"word_tok\"].apply(get_word_stem, args = (\"english\",))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'long',\n",
       " 'time',\n",
       " 'ago',\n",
       " 'when',\n",
       " 'i',\n",
       " 'was',\n",
       " 'in',\n",
       " 'third',\n",
       " 'grade',\n",
       " 'i',\n",
       " 'had',\n",
       " 'a',\n",
       " 'friend',\n",
       " 'person2',\n",
       " 'who',\n",
       " 's',\n",
       " 'mom',\n",
       " 'was',\n",
       " 'in',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'mood',\n",
       " 'she',\n",
       " 'never',\n",
       " 'laugh',\n",
       " 'and',\n",
       " 'she',\n",
       " 'never',\n",
       " 'smile',\n",
       " 'everi',\n",
       " 'time',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'her',\n",
       " 'i',\n",
       " 'would',\n",
       " 'smile',\n",
       " 'at',\n",
       " 'her',\n",
       " 'and',\n",
       " 'all',\n",
       " 'she',\n",
       " 'would',\n",
       " 'do',\n",
       " 'was',\n",
       " 'frown',\n",
       " 'and',\n",
       " 'keep',\n",
       " 'walk',\n",
       " 'at',\n",
       " 'first',\n",
       " 'i',\n",
       " 'did',\n",
       " 'nt',\n",
       " 'know',\n",
       " 'she',\n",
       " 'was',\n",
       " 'a',\n",
       " 'grouch',\n",
       " 'i',\n",
       " 'just',\n",
       " 'thought',\n",
       " 'she',\n",
       " 'did',\n",
       " 'nt',\n",
       " 'like',\n",
       " 'me',\n",
       " 'or',\n",
       " 'someth',\n",
       " 'when',\n",
       " 'person2',\n",
       " 'told',\n",
       " 'me',\n",
       " 'his',\n",
       " 'mom',\n",
       " 'was',\n",
       " 'a',\n",
       " 'grouch',\n",
       " 'i',\n",
       " 'start',\n",
       " 'to',\n",
       " 'laugh',\n",
       " 'and',\n",
       " 'laugh',\n",
       " 'he',\n",
       " 'ask',\n",
       " 'me',\n",
       " 'what',\n",
       " 'was',\n",
       " 'so',\n",
       " 'funni',\n",
       " 'i',\n",
       " 'told',\n",
       " 'him',\n",
       " 'that',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'his',\n",
       " 'mom',\n",
       " 'did',\n",
       " 'nt',\n",
       " 'like',\n",
       " 'me',\n",
       " 'or',\n",
       " 'someth',\n",
       " 'becaus',\n",
       " 'everi',\n",
       " 'time',\n",
       " 'i',\n",
       " 'see',\n",
       " 'his',\n",
       " 'mom',\n",
       " 'i',\n",
       " 'would',\n",
       " 'smile',\n",
       " 'at',\n",
       " 'her',\n",
       " 'and',\n",
       " 'all',\n",
       " 'she',\n",
       " 'will',\n",
       " 'do',\n",
       " 'is',\n",
       " 'frown',\n",
       " 'and',\n",
       " 'walk',\n",
       " 'away',\n",
       " 'that',\n",
       " 'made',\n",
       " 'my',\n",
       " 'friend',\n",
       " 'laugh',\n",
       " 'we',\n",
       " 'were',\n",
       " 'crack',\n",
       " 'up',\n",
       " 'so',\n",
       " 'hard',\n",
       " 'that',\n",
       " 'we',\n",
       " 'got',\n",
       " 'in',\n",
       " 'troubl',\n",
       " 'in',\n",
       " 'class',\n",
       " 'the',\n",
       " 'next',\n",
       " 'day',\n",
       " 'person2',\n",
       " 'and',\n",
       " 'i',\n",
       " 'were',\n",
       " 'eat',\n",
       " 'lunch',\n",
       " 'at',\n",
       " 'school',\n",
       " 'when',\n",
       " 'he',\n",
       " 'say',\n",
       " 'to',\n",
       " 'me',\n",
       " 'lthey',\n",
       " 'your',\n",
       " 'pretti',\n",
       " 'good',\n",
       " 'at',\n",
       " 'make',\n",
       " 'peopl',\n",
       " 'laughgt',\n",
       " 'i',\n",
       " 'said',\n",
       " 'ltno',\n",
       " 'i',\n",
       " 'am',\n",
       " 'not',\n",
       " 'my',\n",
       " 'jokesar',\n",
       " 'horriblegt',\n",
       " 'he',\n",
       " 'said',\n",
       " 'ltcaps1',\n",
       " 'let',\n",
       " 'put',\n",
       " 'them',\n",
       " 'to',\n",
       " 'the',\n",
       " 'test',\n",
       " 'go',\n",
       " 'up',\n",
       " 'to',\n",
       " 'some',\n",
       " 'one',\n",
       " 'new',\n",
       " 'to',\n",
       " 'this',\n",
       " 'schoolgt',\n",
       " 'i',\n",
       " 'said',\n",
       " 'caps1',\n",
       " 'so',\n",
       " 'we',\n",
       " 'went',\n",
       " 'around',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'school',\n",
       " 'look',\n",
       " 'for',\n",
       " 'a',\n",
       " 'new',\n",
       " 'student',\n",
       " 'unfortun',\n",
       " 'we',\n",
       " 'could',\n",
       " 'nt',\n",
       " 'find',\n",
       " 'one',\n",
       " 'we',\n",
       " 'heard',\n",
       " 'the',\n",
       " 'bell',\n",
       " 'ring',\n",
       " 'and',\n",
       " 'we',\n",
       " 'ran',\n",
       " 'to',\n",
       " 'our',\n",
       " 'class',\n",
       " 'we',\n",
       " 'sat',\n",
       " 'in',\n",
       " 'the',\n",
       " 'back',\n",
       " 'of',\n",
       " 'the',\n",
       " 'classroom',\n",
       " 'it',\n",
       " 'onli',\n",
       " 'person2',\n",
       " 'and',\n",
       " 'i',\n",
       " 'and',\n",
       " 'anempti',\n",
       " 'seat',\n",
       " 'between',\n",
       " 'us',\n",
       " 'we',\n",
       " 'were',\n",
       " 'excit',\n",
       " 'becaus',\n",
       " 'our',\n",
       " 'teacher',\n",
       " 'person1',\n",
       " 'was',\n",
       " 'go',\n",
       " 'to',\n",
       " 'show',\n",
       " 'us',\n",
       " 'a',\n",
       " 'movi',\n",
       " 'person1',\n",
       " 'got',\n",
       " 'the',\n",
       " 'front',\n",
       " 'of',\n",
       " 'the',\n",
       " 'room',\n",
       " 'andclass',\n",
       " 'today',\n",
       " 'i',\n",
       " 'have',\n",
       " 'an',\n",
       " 'announc',\n",
       " 'we',\n",
       " 'have',\n",
       " 'a',\n",
       " 'new',\n",
       " 'student',\n",
       " 'in',\n",
       " 'our',\n",
       " 'class',\n",
       " 'say',\n",
       " 'hello',\n",
       " 'to',\n",
       " 'location1',\n",
       " 'organization1',\n",
       " 'walk',\n",
       " 'through',\n",
       " 'the',\n",
       " 'door',\n",
       " 'person1',\n",
       " 'told',\n",
       " 'organization1',\n",
       " 'shecould',\n",
       " 'sit',\n",
       " 'in',\n",
       " 'the',\n",
       " 'back',\n",
       " 'in',\n",
       " 'between',\n",
       " 'person2',\n",
       " 'and',\n",
       " 'i',\n",
       " 'she',\n",
       " 'sat',\n",
       " 'down',\n",
       " 'turn',\n",
       " 'to',\n",
       " 'the',\n",
       " 'both',\n",
       " 'of',\n",
       " 'us',\n",
       " 'and',\n",
       " 'said',\n",
       " 'hello',\n",
       " 'person2',\n",
       " 'gave',\n",
       " 'me',\n",
       " 'a',\n",
       " 'look',\n",
       " 'that',\n",
       " 'said',\n",
       " 'tell',\n",
       " 'her',\n",
       " 'the',\n",
       " 'joke',\n",
       " 'and',\n",
       " 'caps3',\n",
       " 'him',\n",
       " 'a',\n",
       " 'thumb',\n",
       " 'up',\n",
       " 'i',\n",
       " 'turn',\n",
       " 'to',\n",
       " 'location1',\n",
       " 'and',\n",
       " 'said',\n",
       " 'hi',\n",
       " 'i',\n",
       " 'm',\n",
       " 'caps4',\n",
       " 'do',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'hear',\n",
       " 'a',\n",
       " 'joke',\n",
       " 'organization1',\n",
       " 'said',\n",
       " 'yeah',\n",
       " 'sure',\n",
       " 'i',\n",
       " 'start',\n",
       " 'ltcaps5',\n",
       " 'knocksh',\n",
       " 'said',\n",
       " 'ltwho',\n",
       " 's',\n",
       " 'therei',\n",
       " 'said',\n",
       " 'ltboosh',\n",
       " 'said',\n",
       " 'ltboo',\n",
       " 'whoi',\n",
       " 'said',\n",
       " 'ltoh',\n",
       " 'do',\n",
       " 'nt',\n",
       " 'cri',\n",
       " 'i',\n",
       " 'am',\n",
       " 'right',\n",
       " 'heregt',\n",
       " 'at',\n",
       " 'first',\n",
       " 'she',\n",
       " 'did',\n",
       " 'nt',\n",
       " 'laugh',\n",
       " 'becaus',\n",
       " 'she',\n",
       " 'did',\n",
       " 'nt',\n",
       " 'get',\n",
       " 'it',\n",
       " 'but',\n",
       " 'duringth',\n",
       " 'middl',\n",
       " 'of',\n",
       " 'the',\n",
       " 'movi',\n",
       " 'she',\n",
       " 'said',\n",
       " 'ltohhhh',\n",
       " 'i',\n",
       " 'get',\n",
       " 'itand',\n",
       " 'she',\n",
       " 'start',\n",
       " 'to',\n",
       " 'laugh',\n",
       " 'person2',\n",
       " 'turn',\n",
       " 'to',\n",
       " 'me',\n",
       " 'and',\n",
       " 'said',\n",
       " 'lti',\n",
       " 'told',\n",
       " 'you',\n",
       " 'sogt',\n",
       " 'person2',\n",
       " 'got',\n",
       " 'this',\n",
       " 'crazi',\n",
       " 'idea',\n",
       " 'that',\n",
       " 'if',\n",
       " 'i',\n",
       " 'spent',\n",
       " 'the',\n",
       " 'night',\n",
       " 'at',\n",
       " 'his',\n",
       " 'hous',\n",
       " 'that',\n",
       " 'i',\n",
       " 'could',\n",
       " 'make',\n",
       " 'his',\n",
       " 'mom',\n",
       " 'laugh',\n",
       " 'or',\n",
       " 'at',\n",
       " 'least',\n",
       " 'make',\n",
       " 'her',\n",
       " 'smile',\n",
       " 'i',\n",
       " 'said',\n",
       " 'ltcaps1',\n",
       " 'sound',\n",
       " 'like',\n",
       " 'a',\n",
       " 'plangt',\n",
       " 'i',\n",
       " 'ask',\n",
       " 'mt',\n",
       " 'mom',\n",
       " 'if',\n",
       " 'it',\n",
       " 'was',\n",
       " 'caps1',\n",
       " 'if',\n",
       " 'i',\n",
       " 'could',\n",
       " 'spend',\n",
       " 'the',\n",
       " 'night',\n",
       " 'at',\n",
       " 'person2',\n",
       " 'hous',\n",
       " 'she',\n",
       " 'said',\n",
       " 'ltyeah',\n",
       " 'just',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'it',\n",
       " 'caps1',\n",
       " 'with',\n",
       " 'his',\n",
       " 'momgt',\n",
       " 'person2',\n",
       " 'ask',\n",
       " 'his',\n",
       " 'mom',\n",
       " 'she',\n",
       " 'said',\n",
       " 'it',\n",
       " 'was',\n",
       " 'caps1',\n",
       " 'when',\n",
       " 'i',\n",
       " 'got',\n",
       " 'to',\n",
       " 'person2',\n",
       " 'hous',\n",
       " 'the',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'we',\n",
       " 'did',\n",
       " 'was',\n",
       " 'play',\n",
       " 'video',\n",
       " 'game',\n",
       " 'when',\n",
       " 'it',\n",
       " 'was',\n",
       " 'dinner',\n",
       " 'time',\n",
       " 'we',\n",
       " 'all',\n",
       " 'sat',\n",
       " 'down',\n",
       " 'at',\n",
       " 'the',\n",
       " 'tabl',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'person2',\n",
       " 'and',\n",
       " 'i',\n",
       " 'were',\n",
       " 'on',\n",
       " 'one',\n",
       " 'side',\n",
       " 'and',\n",
       " 'his',\n",
       " 'parent',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other',\n",
       " 'when',\n",
       " 'we',\n",
       " 'start',\n",
       " 'eat',\n",
       " 'person2',\n",
       " 'told',\n",
       " 'me',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'the',\n",
       " 'joke',\n",
       " 'to',\n",
       " 'his',\n",
       " 'parent',\n",
       " 'i',\n",
       " 'said',\n",
       " 'caps1',\n",
       " 'so',\n",
       " 'i',\n",
       " 'said',\n",
       " 'to',\n",
       " 'them',\n",
       " 'ltcaps5',\n",
       " 'caps5gtthey',\n",
       " 'repli',\n",
       " 'ltwho',\n",
       " 's',\n",
       " 'therei',\n",
       " 'said',\n",
       " 'ltboothey',\n",
       " 'said',\n",
       " 'ltboo',\n",
       " 'whoi',\n",
       " 'said',\n",
       " 'ltoh',\n",
       " 'do',\n",
       " 'nt',\n",
       " 'cri',\n",
       " 'i',\n",
       " 'am',\n",
       " 'right',\n",
       " 'heregt',\n",
       " 'his',\n",
       " 'parent',\n",
       " 'start',\n",
       " 'to',\n",
       " 'laugh',\n",
       " 'and',\n",
       " 'laugh',\n",
       " 'and',\n",
       " 'they',\n",
       " 'keptlaugh',\n",
       " 'for',\n",
       " 'like',\n",
       " 'five',\n",
       " 'minut',\n",
       " 'person2',\n",
       " 'turn',\n",
       " 'to',\n",
       " 'me',\n",
       " 'and',\n",
       " 'yell',\n",
       " 'it',\n",
       " 'work',\n",
       " 'his',\n",
       " 'mom',\n",
       " 'ask',\n",
       " 'what',\n",
       " 'work',\n",
       " 'person2',\n",
       " 'explain',\n",
       " 'everyth',\n",
       " 'to',\n",
       " 'them',\n",
       " 'his',\n",
       " 'mom',\n",
       " 'told',\n",
       " 'usthat',\n",
       " 'her',\n",
       " 'mom',\n",
       " 'had',\n",
       " 'recent',\n",
       " 'die',\n",
       " 'and',\n",
       " 'that',\n",
       " 's',\n",
       " 'whi',\n",
       " 'she',\n",
       " 'was',\n",
       " 'in',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'mood',\n",
       " 'after',\n",
       " 'dinner',\n",
       " 'we',\n",
       " 'went',\n",
       " 'to',\n",
       " 'bed',\n",
       " 'and',\n",
       " 'fell',\n",
       " 'asleep',\n",
       " 'the',\n",
       " 'next',\n",
       " 'morn',\n",
       " 'when',\n",
       " 'my',\n",
       " 'mom',\n",
       " 'came',\n",
       " 'to',\n",
       " 'pick',\n",
       " 'me',\n",
       " 'up',\n",
       " 'she',\n",
       " 'ask',\n",
       " 'me',\n",
       " 'how',\n",
       " 'the',\n",
       " 'sleep',\n",
       " 'over',\n",
       " 'went',\n",
       " 'i',\n",
       " 'said',\n",
       " 'it',\n",
       " 'was',\n",
       " 'fun',\n",
       " 'i',\n",
       " 'made',\n",
       " 'peopl',\n",
       " 'laugh',\n",
       " 'she',\n",
       " 'said',\n",
       " 'caps1',\n",
       " 'but',\n",
       " 'laughter',\n",
       " 'is',\n",
       " 'nt',\n",
       " 'go',\n",
       " 'to',\n",
       " 'help',\n",
       " 'you',\n",
       " 'clean',\n",
       " 'your',\n",
       " 'roomal',\n",
       " 'i',\n",
       " 'could',\n",
       " 'say',\n",
       " 'was',\n",
       " 'gosh',\n",
       " 'darn',\n",
       " 'it',\n",
       " 'fin']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"stemmed_word_token\"] = training_data_1[\"word_tok\"].apply(get_word_stem, args = (\"english\",))\n",
    "training_data_2[\"stemmed_word_token\"] = training_data_2[\"word_tok\"].apply(get_word_stem, args = (\"english\",))\n",
    "training_data_3[\"stemmed_word_token\"] = training_data_3[\"word_tok\"].apply(get_word_stem, args = (\"english\",))\n",
    "training_data_4[\"stemmed_word_token\"] = training_data_4[\"word_tok\"].apply(get_word_stem, args = (\"english\",))\n",
    "training_data_5[\"stemmed_word_token\"] = training_data_5[\"word_tok\"].apply(get_word_stem, args = (\"english\",))\n",
    "training_data_6[\"stemmed_word_token\"] = training_data_6[\"word_tok\"].apply(get_word_stem, args = (\"english\",))\n",
    "training_data_7[\"stemmed_word_token\"] = training_data_7[\"word_tok\"].apply(get_word_stem, args = (\"english\",))\n",
    "training_data_8[\"stemmed_word_token\"] = training_data_8[\"word_tok\"].apply(get_word_stem, args = (\"english\",))\n",
    "\n",
    "training_data_8[\"stemmed_word_token\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_no_punctuation_series, language):\n",
    "\n",
    "    tokenized_no_stopwords = []\n",
    "\n",
    "    for doc in tokenized_no_punctuation_series:\n",
    "        new_term_vector = []\n",
    "        for word in doc:\n",
    "            if not word in stopwords.words(language):\n",
    "                new_term_vector.append(word)\n",
    "    \n",
    "        tokenized_no_stopwords.append(new_term_vector)\n",
    "    \n",
    "    return tokenized_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/736471159.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_1[\"stemmed_no_stopwords\"] = remove_stopwords(training_data_1[\"stemmed_word_token\"], \"english\")\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/736471159.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_2[\"stemmed_no_stopwords\"] = remove_stopwords(training_data_2[\"stemmed_word_token\"], \"english\")\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/736471159.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_3[\"stemmed_no_stopwords\"] = remove_stopwords(training_data_3[\"stemmed_word_token\"], \"english\")\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/736471159.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_4[\"stemmed_no_stopwords\"] = remove_stopwords(training_data_4[\"stemmed_word_token\"], \"english\")\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/736471159.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_5[\"stemmed_no_stopwords\"] = remove_stopwords(training_data_5[\"stemmed_word_token\"], \"english\")\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/736471159.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_6[\"stemmed_no_stopwords\"] = remove_stopwords(training_data_6[\"stemmed_word_token\"], \"english\")\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/736471159.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_7[\"stemmed_no_stopwords\"] = remove_stopwords(training_data_7[\"stemmed_word_token\"], \"english\")\n",
      "C:\\Users\\jessi\\AppData\\Local\\Temp/ipykernel_12412/736471159.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data_8[\"stemmed_no_stopwords\"] = remove_stopwords(training_data_8[\"stemmed_word_token\"], \"english\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['long',\n",
       " 'time',\n",
       " 'ago',\n",
       " 'third',\n",
       " 'grade',\n",
       " 'friend',\n",
       " 'person2',\n",
       " 'mom',\n",
       " 'bad',\n",
       " 'mood',\n",
       " 'never',\n",
       " 'laugh',\n",
       " 'never',\n",
       " 'smile',\n",
       " 'everi',\n",
       " 'time',\n",
       " 'saw',\n",
       " 'would',\n",
       " 'smile',\n",
       " 'would',\n",
       " 'frown',\n",
       " 'keep',\n",
       " 'walk',\n",
       " 'first',\n",
       " 'nt',\n",
       " 'know',\n",
       " 'grouch',\n",
       " 'thought',\n",
       " 'nt',\n",
       " 'like',\n",
       " 'someth',\n",
       " 'person2',\n",
       " 'told',\n",
       " 'mom',\n",
       " 'grouch',\n",
       " 'start',\n",
       " 'laugh',\n",
       " 'laugh',\n",
       " 'ask',\n",
       " 'funni',\n",
       " 'told',\n",
       " 'thought',\n",
       " 'mom',\n",
       " 'nt',\n",
       " 'like',\n",
       " 'someth',\n",
       " 'becaus',\n",
       " 'everi',\n",
       " 'time',\n",
       " 'see',\n",
       " 'mom',\n",
       " 'would',\n",
       " 'smile',\n",
       " 'frown',\n",
       " 'walk',\n",
       " 'away',\n",
       " 'made',\n",
       " 'friend',\n",
       " 'laugh',\n",
       " 'crack',\n",
       " 'hard',\n",
       " 'got',\n",
       " 'troubl',\n",
       " 'class',\n",
       " 'next',\n",
       " 'day',\n",
       " 'person2',\n",
       " 'eat',\n",
       " 'lunch',\n",
       " 'school',\n",
       " 'say',\n",
       " 'lthey',\n",
       " 'pretti',\n",
       " 'good',\n",
       " 'make',\n",
       " 'peopl',\n",
       " 'laughgt',\n",
       " 'said',\n",
       " 'ltno',\n",
       " 'jokesar',\n",
       " 'horriblegt',\n",
       " 'said',\n",
       " 'ltcaps1',\n",
       " 'let',\n",
       " 'put',\n",
       " 'test',\n",
       " 'go',\n",
       " 'one',\n",
       " 'new',\n",
       " 'schoolgt',\n",
       " 'said',\n",
       " 'caps1',\n",
       " 'went',\n",
       " 'around',\n",
       " 'whole',\n",
       " 'school',\n",
       " 'look',\n",
       " 'new',\n",
       " 'student',\n",
       " 'unfortun',\n",
       " 'could',\n",
       " 'nt',\n",
       " 'find',\n",
       " 'one',\n",
       " 'heard',\n",
       " 'bell',\n",
       " 'ring',\n",
       " 'ran',\n",
       " 'class',\n",
       " 'sat',\n",
       " 'back',\n",
       " 'classroom',\n",
       " 'onli',\n",
       " 'person2',\n",
       " 'anempti',\n",
       " 'seat',\n",
       " 'us',\n",
       " 'excit',\n",
       " 'becaus',\n",
       " 'teacher',\n",
       " 'person1',\n",
       " 'go',\n",
       " 'show',\n",
       " 'us',\n",
       " 'movi',\n",
       " 'person1',\n",
       " 'got',\n",
       " 'front',\n",
       " 'room',\n",
       " 'andclass',\n",
       " 'today',\n",
       " 'announc',\n",
       " 'new',\n",
       " 'student',\n",
       " 'class',\n",
       " 'say',\n",
       " 'hello',\n",
       " 'location1',\n",
       " 'organization1',\n",
       " 'walk',\n",
       " 'door',\n",
       " 'person1',\n",
       " 'told',\n",
       " 'organization1',\n",
       " 'shecould',\n",
       " 'sit',\n",
       " 'back',\n",
       " 'person2',\n",
       " 'sat',\n",
       " 'turn',\n",
       " 'us',\n",
       " 'said',\n",
       " 'hello',\n",
       " 'person2',\n",
       " 'gave',\n",
       " 'look',\n",
       " 'said',\n",
       " 'tell',\n",
       " 'joke',\n",
       " 'caps3',\n",
       " 'thumb',\n",
       " 'turn',\n",
       " 'location1',\n",
       " 'said',\n",
       " 'hi',\n",
       " 'caps4',\n",
       " 'want',\n",
       " 'hear',\n",
       " 'joke',\n",
       " 'organization1',\n",
       " 'said',\n",
       " 'yeah',\n",
       " 'sure',\n",
       " 'start',\n",
       " 'ltcaps5',\n",
       " 'knocksh',\n",
       " 'said',\n",
       " 'ltwho',\n",
       " 'therei',\n",
       " 'said',\n",
       " 'ltboosh',\n",
       " 'said',\n",
       " 'ltboo',\n",
       " 'whoi',\n",
       " 'said',\n",
       " 'ltoh',\n",
       " 'nt',\n",
       " 'cri',\n",
       " 'right',\n",
       " 'heregt',\n",
       " 'first',\n",
       " 'nt',\n",
       " 'laugh',\n",
       " 'becaus',\n",
       " 'nt',\n",
       " 'get',\n",
       " 'duringth',\n",
       " 'middl',\n",
       " 'movi',\n",
       " 'said',\n",
       " 'ltohhhh',\n",
       " 'get',\n",
       " 'itand',\n",
       " 'start',\n",
       " 'laugh',\n",
       " 'person2',\n",
       " 'turn',\n",
       " 'said',\n",
       " 'lti',\n",
       " 'told',\n",
       " 'sogt',\n",
       " 'person2',\n",
       " 'got',\n",
       " 'crazi',\n",
       " 'idea',\n",
       " 'spent',\n",
       " 'night',\n",
       " 'hous',\n",
       " 'could',\n",
       " 'make',\n",
       " 'mom',\n",
       " 'laugh',\n",
       " 'least',\n",
       " 'make',\n",
       " 'smile',\n",
       " 'said',\n",
       " 'ltcaps1',\n",
       " 'sound',\n",
       " 'like',\n",
       " 'plangt',\n",
       " 'ask',\n",
       " 'mt',\n",
       " 'mom',\n",
       " 'caps1',\n",
       " 'could',\n",
       " 'spend',\n",
       " 'night',\n",
       " 'person2',\n",
       " 'hous',\n",
       " 'said',\n",
       " 'ltyeah',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'caps1',\n",
       " 'momgt',\n",
       " 'person2',\n",
       " 'ask',\n",
       " 'mom',\n",
       " 'said',\n",
       " 'caps1',\n",
       " 'got',\n",
       " 'person2',\n",
       " 'hous',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'play',\n",
       " 'video',\n",
       " 'game',\n",
       " 'dinner',\n",
       " 'time',\n",
       " 'sat',\n",
       " 'tabl',\n",
       " 'eat',\n",
       " 'person2',\n",
       " 'one',\n",
       " 'side',\n",
       " 'parent',\n",
       " 'start',\n",
       " 'eat',\n",
       " 'person2',\n",
       " 'told',\n",
       " 'tell',\n",
       " 'joke',\n",
       " 'parent',\n",
       " 'said',\n",
       " 'caps1',\n",
       " 'said',\n",
       " 'ltcaps5',\n",
       " 'caps5gtthey',\n",
       " 'repli',\n",
       " 'ltwho',\n",
       " 'therei',\n",
       " 'said',\n",
       " 'ltboothey',\n",
       " 'said',\n",
       " 'ltboo',\n",
       " 'whoi',\n",
       " 'said',\n",
       " 'ltoh',\n",
       " 'nt',\n",
       " 'cri',\n",
       " 'right',\n",
       " 'heregt',\n",
       " 'parent',\n",
       " 'start',\n",
       " 'laugh',\n",
       " 'laugh',\n",
       " 'keptlaugh',\n",
       " 'like',\n",
       " 'five',\n",
       " 'minut',\n",
       " 'person2',\n",
       " 'turn',\n",
       " 'yell',\n",
       " 'work',\n",
       " 'mom',\n",
       " 'ask',\n",
       " 'work',\n",
       " 'person2',\n",
       " 'explain',\n",
       " 'everyth',\n",
       " 'mom',\n",
       " 'told',\n",
       " 'usthat',\n",
       " 'mom',\n",
       " 'recent',\n",
       " 'die',\n",
       " 'whi',\n",
       " 'bad',\n",
       " 'mood',\n",
       " 'dinner',\n",
       " 'went',\n",
       " 'bed',\n",
       " 'fell',\n",
       " 'asleep',\n",
       " 'next',\n",
       " 'morn',\n",
       " 'mom',\n",
       " 'came',\n",
       " 'pick',\n",
       " 'ask',\n",
       " 'sleep',\n",
       " 'went',\n",
       " 'said',\n",
       " 'fun',\n",
       " 'made',\n",
       " 'peopl',\n",
       " 'laugh',\n",
       " 'said',\n",
       " 'caps1',\n",
       " 'laughter',\n",
       " 'nt',\n",
       " 'go',\n",
       " 'help',\n",
       " 'clean',\n",
       " 'roomal',\n",
       " 'could',\n",
       " 'say',\n",
       " 'gosh',\n",
       " 'darn',\n",
       " 'fin']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1[\"stemmed_no_stopwords\"] = remove_stopwords(training_data_1[\"stemmed_word_token\"], \"english\")\n",
    "training_data_2[\"stemmed_no_stopwords\"] = remove_stopwords(training_data_2[\"stemmed_word_token\"], \"english\")\n",
    "training_data_3[\"stemmed_no_stopwords\"] = remove_stopwords(training_data_3[\"stemmed_word_token\"], \"english\")\n",
    "training_data_4[\"stemmed_no_stopwords\"] = remove_stopwords(training_data_4[\"stemmed_word_token\"], \"english\")\n",
    "training_data_5[\"stemmed_no_stopwords\"] = remove_stopwords(training_data_5[\"stemmed_word_token\"], \"english\")\n",
    "training_data_6[\"stemmed_no_stopwords\"] = remove_stopwords(training_data_6[\"stemmed_word_token\"], \"english\")\n",
    "training_data_7[\"stemmed_no_stopwords\"] = remove_stopwords(training_data_7[\"stemmed_word_token\"], \"english\")\n",
    "training_data_8[\"stemmed_no_stopwords\"] = remove_stopwords(training_data_8[\"stemmed_word_token\"], \"english\")\n",
    "\n",
    "training_data_8[\"stemmed_no_stopwords\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>normalised_docs</th>\n",
       "      <th>word_tok</th>\n",
       "      <th>sentence_tok</th>\n",
       "      <th>stemmed_word_token</th>\n",
       "      <th>stemmed_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12255</th>\n",
       "      <td>20716</td>\n",
       "      <td>8</td>\n",
       "      <td>A long time ago when I was in third grade I h...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a long time ago when i was in third grade i h...</td>\n",
       "      <td>[A, long, time, ago, when, I, was, in, third, ...</td>\n",
       "      <td>[A long time ago when I was in third grade I h...</td>\n",
       "      <td>[a, long, time, ago, when, i, was, in, third, ...</td>\n",
       "      <td>[long, time, ago, third, grade, friend, person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12256</th>\n",
       "      <td>20717</td>\n",
       "      <td>8</td>\n",
       "      <td>Softball has to be one of the single most gre...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>softball has to be one of the single most gre...</td>\n",
       "      <td>[Softball, has, to, be, one, of, the, single, ...</td>\n",
       "      <td>[Softball has to be one of the single most gre...</td>\n",
       "      <td>[softbal, has, to, be, one, of, the, singl, mo...</td>\n",
       "      <td>[softbal, one, singl, greatest, sport, aliv, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12257</th>\n",
       "      <td>20718</td>\n",
       "      <td>8</td>\n",
       "      <td>Some people like making people laugh, I love ...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>some people like making people laugh, i love ...</td>\n",
       "      <td>[Some, people, like, making, people, laugh, I,...</td>\n",
       "      <td>[Some people like making people laugh, I love ...</td>\n",
       "      <td>[some, peopl, like, make, peopl, laugh, i, lov...</td>\n",
       "      <td>[peopl, like, make, peopl, laugh, love, anyth,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12258</th>\n",
       "      <td>20719</td>\n",
       "      <td>8</td>\n",
       "      <td>\"LAUGHTER\"  @CAPS1 I hang out with my friends...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>\"laughter\"  @caps1 i hang out with my friends...</td>\n",
       "      <td>[LAUGHTER, CAPS1, I, hang, out, with, my, frie...</td>\n",
       "      <td>[\"LAUGHTER\"  @CAPS1 I hang out with my friends...</td>\n",
       "      <td>[laughter, caps1, i, hang, out, with, my, frie...</td>\n",
       "      <td>[laughter, caps1, hang, friend, one, thing, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12259</th>\n",
       "      <td>20721</td>\n",
       "      <td>8</td>\n",
       "      <td>Well ima tell a story about the time i got @CA...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>well ima tell a story about the time i got @ca...</td>\n",
       "      <td>[Well, i, m, a, tell, a, story, about, the, ti...</td>\n",
       "      <td>[Well ima tell a story about the time i got @C...</td>\n",
       "      <td>[well, i, m, a, tell, a, stori, about, the, ti...</td>\n",
       "      <td>[well, tell, stori, time, got, caps3, town, he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                              essay  \\\n",
       "12255     20716          8   A long time ago when I was in third grade I h...   \n",
       "12256     20717          8   Softball has to be one of the single most gre...   \n",
       "12257     20718          8   Some people like making people laugh, I love ...   \n",
       "12258     20719          8   \"LAUGHTER\"  @CAPS1 I hang out with my friends...   \n",
       "12259     20721          8  Well ima tell a story about the time i got @CA...   \n",
       "\n",
       "       rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "12255            18.0            16.0             NaN           34.0   \n",
       "12256            21.0            26.0            46.0           46.0   \n",
       "12257            15.0            20.0            40.0           40.0   \n",
       "12258            12.0            20.0            30.0           30.0   \n",
       "12259            11.0            15.0             NaN           26.0   \n",
       "\n",
       "       rater1_domain2  rater2_domain2  domain2_score  ...  rater3_trait2  \\\n",
       "12255             NaN             NaN            NaN  ...            NaN   \n",
       "12256             NaN             NaN            NaN  ...            5.0   \n",
       "12257             NaN             NaN            NaN  ...            4.0   \n",
       "12258             NaN             NaN            NaN  ...            3.0   \n",
       "12259             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "       rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \\\n",
       "12255            NaN            NaN            NaN            NaN   \n",
       "12256            5.0            5.0            5.0            4.0   \n",
       "12257            4.0            4.0            4.0            4.0   \n",
       "12258            3.0            3.0            3.0            3.0   \n",
       "12259            NaN            NaN            NaN            NaN   \n",
       "\n",
       "                                         normalised_docs  \\\n",
       "12255   a long time ago when i was in third grade i h...   \n",
       "12256   softball has to be one of the single most gre...   \n",
       "12257   some people like making people laugh, i love ...   \n",
       "12258   \"laughter\"  @caps1 i hang out with my friends...   \n",
       "12259  well ima tell a story about the time i got @ca...   \n",
       "\n",
       "                                                word_tok  \\\n",
       "12255  [A, long, time, ago, when, I, was, in, third, ...   \n",
       "12256  [Softball, has, to, be, one, of, the, single, ...   \n",
       "12257  [Some, people, like, making, people, laugh, I,...   \n",
       "12258  [LAUGHTER, CAPS1, I, hang, out, with, my, frie...   \n",
       "12259  [Well, i, m, a, tell, a, story, about, the, ti...   \n",
       "\n",
       "                                            sentence_tok  \\\n",
       "12255  [A long time ago when I was in third grade I h...   \n",
       "12256  [Softball has to be one of the single most gre...   \n",
       "12257  [Some people like making people laugh, I love ...   \n",
       "12258  [\"LAUGHTER\"  @CAPS1 I hang out with my friends...   \n",
       "12259  [Well ima tell a story about the time i got @C...   \n",
       "\n",
       "                                      stemmed_word_token  \\\n",
       "12255  [a, long, time, ago, when, i, was, in, third, ...   \n",
       "12256  [softbal, has, to, be, one, of, the, singl, mo...   \n",
       "12257  [some, peopl, like, make, peopl, laugh, i, lov...   \n",
       "12258  [laughter, caps1, i, hang, out, with, my, frie...   \n",
       "12259  [well, i, m, a, tell, a, stori, about, the, ti...   \n",
       "\n",
       "                                    stemmed_no_stopwords  \n",
       "12255  [long, time, ago, third, grade, friend, person...  \n",
       "12256  [softbal, one, singl, greatest, sport, aliv, p...  \n",
       "12257  [peopl, like, make, peopl, laugh, love, anyth,...  \n",
       "12258  [laughter, caps1, hang, friend, one, thing, be...  \n",
       "12259  [well, tell, stori, time, got, caps3, town, he...  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_8.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1.to_csv(\"csv_files/preprosessed_essays_training_set_1.csv\", index = False)\n",
    "training_data_2.to_csv(\"csv_files/preprosessed_essays_training_set_2.csv\", index = False)\n",
    "training_data_3.to_csv(\"csv_files/preprosessed_essays_training_set_3.csv\", index = False)\n",
    "training_data_4.to_csv(\"csv_files/preprosessed_essays_training_set_4.csv\", index = False)\n",
    "training_data_5.to_csv(\"csv_files/preprosessed_essays_training_set_5.csv\", index = False)\n",
    "training_data_6.to_csv(\"csv_files/preprosessed_essays_training_set_6.csv\", index = False)\n",
    "training_data_7.to_csv(\"csv_files/preprosessed_essays_training_set_7.csv\", index = False)\n",
    "training_data_8.to_csv(\"csv_files/preprosessed_essays_training_set_8.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "046d87f254256109b116e79e0701516fb755096a759c052df2135e4cba4ba2a7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
