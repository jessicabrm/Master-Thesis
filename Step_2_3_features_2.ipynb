{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.spatial.distance\n",
    "from sklearn.neighbors import kneighbors_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data = pd.read_csv(\"csv_files/features_1_training_set_1.csv\", converters={\"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "#training_data = pd.read_csv(\"csv_files/features_1_training_set_2.csv\", converters={\"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "#training_data = pd.read_csv(\"csv_files/features_1_training_set_2_2.csv\", converters={\"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "#training_data = pd.read_csv(\"csv_files/features_1_training_set_3.csv\", converters={\"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "#training_data = pd.read_csv(\"csv_files/features_1_training_set_4.csv\", converters={\"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "#training_data = pd.read_csv(\"csv_files/features_1_training_set_5.csv\", converters={\"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "#training_data = pd.read_csv(\"csv_files/features_1_training_set_6.csv\", converters={\"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "#training_data = pd.read_csv(\"csv_files/features_1_training_set_7.csv\", converters={\"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "training_data = pd.read_csv(\"csv_files/features_1_training_set_8.csv\", converters={\"word_tok\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade</th>\n",
       "      <th>normalised_docs</th>\n",
       "      <th>word_tok</th>\n",
       "      <th>sentence_tok</th>\n",
       "      <th>stemmed_word_token</th>\n",
       "      <th>stemmed_no_stopwords</th>\n",
       "      <th>nr_stopwords</th>\n",
       "      <th>avg_sentence_len_words</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>nr_spelling_errors</th>\n",
       "      <th>nr_capitalization_errors</th>\n",
       "      <th>nr_punctuation_errors</th>\n",
       "      <th>grade_as_feature</th>\n",
       "      <th>avg_cosine_similariy_high_grade</th>\n",
       "      <th>pattern_cosine</th>\n",
       "      <th>weighted_cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.0</td>\n",
       "      <td>a long time ago when i was in third grade i h...</td>\n",
       "      <td>[A, long, time, ago, when, I, was, in, third, ...</td>\n",
       "      <td>['\"A long time ago when I was in third grade I...</td>\n",
       "      <td>['a', 'long', 'time', 'ago', 'when', 'i', 'was...</td>\n",
       "      <td>['long', 'time', 'ago', 'third', 'grade', 'fri...</td>\n",
       "      <td>281</td>\n",
       "      <td>15.0</td>\n",
       "      <td>259</td>\n",
       "      <td>679</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>7</td>\n",
       "      <td>80</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.285749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.0</td>\n",
       "      <td>softball has to be one of the single most gre...</td>\n",
       "      <td>[Softball, has, to, be, one, of, the, single, ...</td>\n",
       "      <td>['Softball has to be one of the single most gr...</td>\n",
       "      <td>['softbal', 'has', 'to', 'be', 'one', 'of', 't...</td>\n",
       "      <td>['softbal', 'one', 'singl', 'greatest', 'sport...</td>\n",
       "      <td>348</td>\n",
       "      <td>12.0</td>\n",
       "      <td>308</td>\n",
       "      <td>785</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>99</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.285749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.0</td>\n",
       "      <td>some people like making people laugh, i love ...</td>\n",
       "      <td>[Some, people, like, making, people, laugh, I,...</td>\n",
       "      <td>['Some people like making people laugh', 'I lo...</td>\n",
       "      <td>['some', 'peopl', 'like', 'make', 'peopl', 'la...</td>\n",
       "      <td>['peopl', 'like', 'make', 'peopl', 'laugh', 'l...</td>\n",
       "      <td>381</td>\n",
       "      <td>10.0</td>\n",
       "      <td>323</td>\n",
       "      <td>861</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>72</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.285749</td>\n",
       "      <td>6.908973</td>\n",
       "      <td>-6.908973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0</td>\n",
       "      <td>\"laughter\"  @caps1 i hang out with my friends...</td>\n",
       "      <td>[LAUGHTER, CAPS1, I, hang, out, with, my, frie...</td>\n",
       "      <td>['\"LAUGHTER\"  @CAPS1 I hang out with my friend...</td>\n",
       "      <td>['laughter', 'caps1', 'i', 'hang', 'out', 'wit...</td>\n",
       "      <td>['laughter', 'caps1', 'hang', 'friend', 'one',...</td>\n",
       "      <td>378</td>\n",
       "      <td>15.0</td>\n",
       "      <td>255</td>\n",
       "      <td>712</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>132</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.285749</td>\n",
       "      <td>13.552038</td>\n",
       "      <td>-13.552038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.0</td>\n",
       "      <td>well ima tell a story about the time i got @ca...</td>\n",
       "      <td>[Well, i, m, a, tell, a, story, about, the, ti...</td>\n",
       "      <td>['Well ima tell a story about the time i got @...</td>\n",
       "      <td>['well', 'i', 'm', 'a', 'tell', 'a', 'stori', ...</td>\n",
       "      <td>['well', 'tell', 'stori', 'time', 'got', 'caps...</td>\n",
       "      <td>370</td>\n",
       "      <td>11.0</td>\n",
       "      <td>203</td>\n",
       "      <td>671</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>108</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.285749</td>\n",
       "      <td>19.177102</td>\n",
       "      <td>-19.177102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>35.0</td>\n",
       "      <td>in most stories mothers and daughters are eit...</td>\n",
       "      <td>[In, most, stories, mothers, and, daughters, a...</td>\n",
       "      <td>['In most stories mothers and daughters are ei...</td>\n",
       "      <td>['in', 'most', 'stori', 'mother', 'and', 'daug...</td>\n",
       "      <td>['stori', 'mother', 'daughter', 'either', 'ene...</td>\n",
       "      <td>399</td>\n",
       "      <td>9.0</td>\n",
       "      <td>342</td>\n",
       "      <td>862</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>95</td>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.285749</td>\n",
       "      <td>1984.037703</td>\n",
       "      <td>1984.037703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>32.0</td>\n",
       "      <td>i never understood the meaning laughter is th...</td>\n",
       "      <td>[I, never, understood, the, meaning, laughter,...</td>\n",
       "      <td>['I never understood the meaning laughter is t...</td>\n",
       "      <td>['i', 'never', 'understood', 'the', 'mean', 'l...</td>\n",
       "      <td>['never', 'understood', 'mean', 'laughter', 's...</td>\n",
       "      <td>259</td>\n",
       "      <td>9.0</td>\n",
       "      <td>218</td>\n",
       "      <td>556</td>\n",
       "      <td>...</td>\n",
       "      <td>79</td>\n",
       "      <td>7</td>\n",
       "      <td>121</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.285749</td>\n",
       "      <td>2128.729145</td>\n",
       "      <td>2128.729145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>40.0</td>\n",
       "      <td>when you laugh, is @caps5 out of habit, or is ...</td>\n",
       "      <td>[When, you, laugh, is, CAPS5, out, of, habit, ...</td>\n",
       "      <td>['When you laugh', 'is @CAPS5 out of habit', '...</td>\n",
       "      <td>['when', 'you', 'laugh', 'is', 'caps5', 'out',...</td>\n",
       "      <td>['laugh', 'caps5', 'habit', 'caps1', 'caus', '...</td>\n",
       "      <td>390</td>\n",
       "      <td>9.0</td>\n",
       "      <td>369</td>\n",
       "      <td>835</td>\n",
       "      <td>...</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>147</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.285749</td>\n",
       "      <td>11.891063</td>\n",
       "      <td>11.891063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>40.0</td>\n",
       "      <td>trippin' on fen...</td>\n",
       "      <td>[Trippin, on, fences, I, am, NUM1, years, youn...</td>\n",
       "      <td>['\"Trippin on fences I am @NUM1 years young', ...</td>\n",
       "      <td>['trippin', 'on', 'fenc', 'i', 'am', 'num1', '...</td>\n",
       "      <td>['trippin', 'fenc', 'num1', 'year', 'young', '...</td>\n",
       "      <td>272</td>\n",
       "      <td>7.0</td>\n",
       "      <td>250</td>\n",
       "      <td>576</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>70</td>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.285749</td>\n",
       "      <td>23.782125</td>\n",
       "      <td>23.782125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>40.0</td>\n",
       "      <td>many people believe that laughter can improve...</td>\n",
       "      <td>[Many, people, believe, that, laughter, can, i...</td>\n",
       "      <td>['Many people believe that laughter can improv...</td>\n",
       "      <td>['mani', 'peopl', 'believ', 'that', 'laughter'...</td>\n",
       "      <td>['mani', 'peopl', 'believ', 'laughter', 'impro...</td>\n",
       "      <td>208</td>\n",
       "      <td>8.0</td>\n",
       "      <td>232</td>\n",
       "      <td>475</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.285749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>723 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     grade                                    normalised_docs  \\\n",
       "0     34.0   a long time ago when i was in third grade i h...   \n",
       "1     46.0   softball has to be one of the single most gre...   \n",
       "2     40.0   some people like making people laugh, i love ...   \n",
       "3     30.0   \"laughter\"  @caps1 i hang out with my friends...   \n",
       "4     26.0  well ima tell a story about the time i got @ca...   \n",
       "..     ...                                                ...   \n",
       "718   35.0   in most stories mothers and daughters are eit...   \n",
       "719   32.0   i never understood the meaning laughter is th...   \n",
       "720   40.0  when you laugh, is @caps5 out of habit, or is ...   \n",
       "721   40.0                                 trippin' on fen...   \n",
       "722   40.0   many people believe that laughter can improve...   \n",
       "\n",
       "                                              word_tok  \\\n",
       "0    [A, long, time, ago, when, I, was, in, third, ...   \n",
       "1    [Softball, has, to, be, one, of, the, single, ...   \n",
       "2    [Some, people, like, making, people, laugh, I,...   \n",
       "3    [LAUGHTER, CAPS1, I, hang, out, with, my, frie...   \n",
       "4    [Well, i, m, a, tell, a, story, about, the, ti...   \n",
       "..                                                 ...   \n",
       "718  [In, most, stories, mothers, and, daughters, a...   \n",
       "719  [I, never, understood, the, meaning, laughter,...   \n",
       "720  [When, you, laugh, is, CAPS5, out, of, habit, ...   \n",
       "721  [Trippin, on, fences, I, am, NUM1, years, youn...   \n",
       "722  [Many, people, believe, that, laughter, can, i...   \n",
       "\n",
       "                                          sentence_tok  \\\n",
       "0    ['\"A long time ago when I was in third grade I...   \n",
       "1    ['Softball has to be one of the single most gr...   \n",
       "2    ['Some people like making people laugh', 'I lo...   \n",
       "3    ['\"LAUGHTER\"  @CAPS1 I hang out with my friend...   \n",
       "4    ['Well ima tell a story about the time i got @...   \n",
       "..                                                 ...   \n",
       "718  ['In most stories mothers and daughters are ei...   \n",
       "719  ['I never understood the meaning laughter is t...   \n",
       "720  ['When you laugh', 'is @CAPS5 out of habit', '...   \n",
       "721  ['\"Trippin on fences I am @NUM1 years young', ...   \n",
       "722  ['Many people believe that laughter can improv...   \n",
       "\n",
       "                                    stemmed_word_token  \\\n",
       "0    ['a', 'long', 'time', 'ago', 'when', 'i', 'was...   \n",
       "1    ['softbal', 'has', 'to', 'be', 'one', 'of', 't...   \n",
       "2    ['some', 'peopl', 'like', 'make', 'peopl', 'la...   \n",
       "3    ['laughter', 'caps1', 'i', 'hang', 'out', 'wit...   \n",
       "4    ['well', 'i', 'm', 'a', 'tell', 'a', 'stori', ...   \n",
       "..                                                 ...   \n",
       "718  ['in', 'most', 'stori', 'mother', 'and', 'daug...   \n",
       "719  ['i', 'never', 'understood', 'the', 'mean', 'l...   \n",
       "720  ['when', 'you', 'laugh', 'is', 'caps5', 'out',...   \n",
       "721  ['trippin', 'on', 'fenc', 'i', 'am', 'num1', '...   \n",
       "722  ['mani', 'peopl', 'believ', 'that', 'laughter'...   \n",
       "\n",
       "                                  stemmed_no_stopwords  nr_stopwords  \\\n",
       "0    ['long', 'time', 'ago', 'third', 'grade', 'fri...           281   \n",
       "1    ['softbal', 'one', 'singl', 'greatest', 'sport...           348   \n",
       "2    ['peopl', 'like', 'make', 'peopl', 'laugh', 'l...           381   \n",
       "3    ['laughter', 'caps1', 'hang', 'friend', 'one',...           378   \n",
       "4    ['well', 'tell', 'stori', 'time', 'got', 'caps...           370   \n",
       "..                                                 ...           ...   \n",
       "718  ['stori', 'mother', 'daughter', 'either', 'ene...           399   \n",
       "719  ['never', 'understood', 'mean', 'laughter', 's...           259   \n",
       "720  ['laugh', 'caps5', 'habit', 'caps1', 'caus', '...           390   \n",
       "721  ['trippin', 'fenc', 'num1', 'year', 'young', '...           272   \n",
       "722  ['mani', 'peopl', 'believ', 'laughter', 'impro...           208   \n",
       "\n",
       "     avg_sentence_len_words  unique_words  word_count  ...  PRON  PROPN  NOUN  \\\n",
       "0                      15.0           259         679  ...    47      7    80   \n",
       "1                      12.0           308         785  ...    49      5    99   \n",
       "2                      10.0           323         861  ...    25      4    72   \n",
       "3                      15.0           255         712  ...    32     38   132   \n",
       "4                      11.0           203         671  ...    41      3   108   \n",
       "..                      ...           ...         ...  ...   ...    ...   ...   \n",
       "718                     9.0           342         862  ...    38      4    95   \n",
       "719                     9.0           218         556  ...    79      7   121   \n",
       "720                     9.0           369         835  ...    54      4   147   \n",
       "721                     7.0           250         576  ...    27     14    70   \n",
       "722                     8.0           232         475  ...    23     13    38   \n",
       "\n",
       "     nr_spelling_errors  nr_capitalization_errors  nr_punctuation_errors  \\\n",
       "0                    14                         4                     17   \n",
       "1                     7                        27                     13   \n",
       "2                     9                        34                      9   \n",
       "3                     5                        24                      4   \n",
       "4                    28                        19                      3   \n",
       "..                  ...                       ...                    ...   \n",
       "718                   7                        48                      2   \n",
       "719                  11                        25                      4   \n",
       "720                   5                        34                      3   \n",
       "721                  10                        33                      1   \n",
       "722                   2                        18                      1   \n",
       "\n",
       "     grade_as_feature  avg_cosine_similariy_high_grade  pattern_cosine  \\\n",
       "0                26.0                         0.285749        0.000000   \n",
       "1                40.0                         0.285749        0.000000   \n",
       "2                36.0                         0.285749        6.908973   \n",
       "3                40.0                         0.285749       13.552038   \n",
       "4                34.0                         0.285749       19.177102   \n",
       "..                ...                              ...             ...   \n",
       "718              40.0                         0.285749     1984.037703   \n",
       "719              40.0                         0.285749     2128.729145   \n",
       "720              40.0                         0.285749       11.891063   \n",
       "721              41.0                         0.285749       23.782125   \n",
       "722              41.0                         0.285749        0.000000   \n",
       "\n",
       "     weighted_cosine  \n",
       "0           0.000000  \n",
       "1           0.000000  \n",
       "2          -6.908973  \n",
       "3         -13.552038  \n",
       "4         -19.177102  \n",
       "..               ...  \n",
       "718      1984.037703  \n",
       "719      2128.729145  \n",
       "720        11.891063  \n",
       "721        23.782125  \n",
       "722         0.000000  \n",
       "\n",
       "[723 rows x 62 columns]"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.isna().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split theses into multiple overlapping parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      679\n",
       "1      785\n",
       "2      861\n",
       "3      712\n",
       "4      671\n",
       "      ... \n",
       "718    862\n",
       "719    556\n",
       "720    835\n",
       "721    576\n",
       "722    475\n",
       "Name: word_tok, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "each_theses_length = training_data[\"word_tok\"].apply(len)\n",
    "each_theses_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "execution_count": 782,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_thesis_length = round(each_theses_length.mean())\n",
    "avg_thesis_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = int(avg_thesis_length * 0.25)\n",
    "window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [[A, long, time, ago, when, I, was, in, third,...\n",
       "1      [[Softball, has, to, be, one, of, the, single,...\n",
       "2      [[Some, people, like, making, people, laugh, I...\n",
       "3      [[LAUGHTER, CAPS1, I, hang, out, with, my, fri...\n",
       "4      [[Well, i, m, a, tell, a, story, about, the, t...\n",
       "                             ...                        \n",
       "718    [[In, most, stories, mothers, and, daughters, ...\n",
       "719    [[I, never, understood, the, meaning, laughter...\n",
       "720    [[When, you, laugh, is, CAPS5, out, of, habit,...\n",
       "721    [[Trippin, on, fences, I, am, NUM1, years, you...\n",
       "722    [[Many, people, believe, that, laughter, can, ...\n",
       "Name: theses_parts, Length: 723, dtype: object"
      ]
     },
     "execution_count": 784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"theses_parts\"] = training_data[\"word_tok\"].apply(lambda word_list: [word_list[i:i + window_size] for i in range(0, len(word_list), 10)])\n",
    "training_data[\"theses_parts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [A long time ago when I was in third grade I h...\n",
       "1      [Softball has to be one of the single most gre...\n",
       "2      [Some people like making people laugh I love i...\n",
       "3      [LAUGHTER CAPS1 I hang out with my friends the...\n",
       "4      [Well i m a tell a story about the time i got ...\n",
       "                             ...                        \n",
       "718    [In most stories mothers and daughters are eit...\n",
       "719    [I never understood the meaning laughter is th...\n",
       "720    [When you laugh is CAPS5 out of habit or is CA...\n",
       "721    [Trippin on fences I am NUM1 years young and i...\n",
       "722    [Many people believe that laughter can improve...\n",
       "Name: corpus, Length: 723, dtype: object"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"corpus\"] = training_data[\"theses_parts\"].apply(lambda theses_part: [\" \".join(word_list) for word_list in theses_part])\n",
    "training_data[\"corpus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      68\n",
       "1      79\n",
       "2      87\n",
       "3      72\n",
       "4      68\n",
       "       ..\n",
       "718    87\n",
       "719    56\n",
       "720    84\n",
       "721    58\n",
       "722    48\n",
       "Name: nr_thesis_parts, Length: 723, dtype: int64"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of theses parts = nr of Tfidf vectors\n",
    "# starts from 0 so -1\n",
    "training_data[\"nr_thesis_parts\"] = training_data[\"corpus\"].apply(lambda text_part: len(text_part))\n",
    "training_data[\"nr_thesis_parts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade</th>\n",
       "      <th>normalised_docs</th>\n",
       "      <th>word_tok</th>\n",
       "      <th>sentence_tok</th>\n",
       "      <th>stemmed_word_token</th>\n",
       "      <th>stemmed_no_stopwords</th>\n",
       "      <th>nr_stopwords</th>\n",
       "      <th>avg_sentence_len_words</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>nr_spelling_errors</th>\n",
       "      <th>nr_capitalization_errors</th>\n",
       "      <th>nr_punctuation_errors</th>\n",
       "      <th>grade_as_feature</th>\n",
       "      <th>avg_cosine_similariy_high_grade</th>\n",
       "      <th>pattern_cosine</th>\n",
       "      <th>weighted_cosine</th>\n",
       "      <th>theses_parts</th>\n",
       "      <th>corpus</th>\n",
       "      <th>nr_thesis_parts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.0</td>\n",
       "      <td>a long time ago when i was in third grade i h...</td>\n",
       "      <td>[A, long, time, ago, when, I, was, in, third, ...</td>\n",
       "      <td>['\"A long time ago when I was in third grade I...</td>\n",
       "      <td>['a', 'long', 'time', 'ago', 'when', 'i', 'was...</td>\n",
       "      <td>['long', 'time', 'ago', 'third', 'grade', 'fri...</td>\n",
       "      <td>281</td>\n",
       "      <td>15.0</td>\n",
       "      <td>259</td>\n",
       "      <td>679</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.285749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[A, long, time, ago, when, I, was, in, third,...</td>\n",
       "      <td>[A long time ago when I was in third grade I h...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.0</td>\n",
       "      <td>softball has to be one of the single most gre...</td>\n",
       "      <td>[Softball, has, to, be, one, of, the, single, ...</td>\n",
       "      <td>['Softball has to be one of the single most gr...</td>\n",
       "      <td>['softbal', 'has', 'to', 'be', 'one', 'of', 't...</td>\n",
       "      <td>['softbal', 'one', 'singl', 'greatest', 'sport...</td>\n",
       "      <td>348</td>\n",
       "      <td>12.0</td>\n",
       "      <td>308</td>\n",
       "      <td>785</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.285749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[Softball, has, to, be, one, of, the, single,...</td>\n",
       "      <td>[Softball has to be one of the single most gre...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.0</td>\n",
       "      <td>some people like making people laugh, i love ...</td>\n",
       "      <td>[Some, people, like, making, people, laugh, I,...</td>\n",
       "      <td>['Some people like making people laugh', 'I lo...</td>\n",
       "      <td>['some', 'peopl', 'like', 'make', 'peopl', 'la...</td>\n",
       "      <td>['peopl', 'like', 'make', 'peopl', 'laugh', 'l...</td>\n",
       "      <td>381</td>\n",
       "      <td>10.0</td>\n",
       "      <td>323</td>\n",
       "      <td>861</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.285749</td>\n",
       "      <td>6.908973</td>\n",
       "      <td>-6.908973</td>\n",
       "      <td>[[Some, people, like, making, people, laugh, I...</td>\n",
       "      <td>[Some people like making people laugh I love i...</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0</td>\n",
       "      <td>\"laughter\"  @caps1 i hang out with my friends...</td>\n",
       "      <td>[LAUGHTER, CAPS1, I, hang, out, with, my, frie...</td>\n",
       "      <td>['\"LAUGHTER\"  @CAPS1 I hang out with my friend...</td>\n",
       "      <td>['laughter', 'caps1', 'i', 'hang', 'out', 'wit...</td>\n",
       "      <td>['laughter', 'caps1', 'hang', 'friend', 'one',...</td>\n",
       "      <td>378</td>\n",
       "      <td>15.0</td>\n",
       "      <td>255</td>\n",
       "      <td>712</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.285749</td>\n",
       "      <td>13.552038</td>\n",
       "      <td>-13.552038</td>\n",
       "      <td>[[LAUGHTER, CAPS1, I, hang, out, with, my, fri...</td>\n",
       "      <td>[LAUGHTER CAPS1 I hang out with my friends the...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.0</td>\n",
       "      <td>well ima tell a story about the time i got @ca...</td>\n",
       "      <td>[Well, i, m, a, tell, a, story, about, the, ti...</td>\n",
       "      <td>['Well ima tell a story about the time i got @...</td>\n",
       "      <td>['well', 'i', 'm', 'a', 'tell', 'a', 'stori', ...</td>\n",
       "      <td>['well', 'tell', 'stori', 'time', 'got', 'caps...</td>\n",
       "      <td>370</td>\n",
       "      <td>11.0</td>\n",
       "      <td>203</td>\n",
       "      <td>671</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.285749</td>\n",
       "      <td>19.177102</td>\n",
       "      <td>-19.177102</td>\n",
       "      <td>[[Well, i, m, a, tell, a, story, about, the, t...</td>\n",
       "      <td>[Well i m a tell a story about the time i got ...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   grade                                    normalised_docs  \\\n",
       "0   34.0   a long time ago when i was in third grade i h...   \n",
       "1   46.0   softball has to be one of the single most gre...   \n",
       "2   40.0   some people like making people laugh, i love ...   \n",
       "3   30.0   \"laughter\"  @caps1 i hang out with my friends...   \n",
       "4   26.0  well ima tell a story about the time i got @ca...   \n",
       "\n",
       "                                            word_tok  \\\n",
       "0  [A, long, time, ago, when, I, was, in, third, ...   \n",
       "1  [Softball, has, to, be, one, of, the, single, ...   \n",
       "2  [Some, people, like, making, people, laugh, I,...   \n",
       "3  [LAUGHTER, CAPS1, I, hang, out, with, my, frie...   \n",
       "4  [Well, i, m, a, tell, a, story, about, the, ti...   \n",
       "\n",
       "                                        sentence_tok  \\\n",
       "0  ['\"A long time ago when I was in third grade I...   \n",
       "1  ['Softball has to be one of the single most gr...   \n",
       "2  ['Some people like making people laugh', 'I lo...   \n",
       "3  ['\"LAUGHTER\"  @CAPS1 I hang out with my friend...   \n",
       "4  ['Well ima tell a story about the time i got @...   \n",
       "\n",
       "                                  stemmed_word_token  \\\n",
       "0  ['a', 'long', 'time', 'ago', 'when', 'i', 'was...   \n",
       "1  ['softbal', 'has', 'to', 'be', 'one', 'of', 't...   \n",
       "2  ['some', 'peopl', 'like', 'make', 'peopl', 'la...   \n",
       "3  ['laughter', 'caps1', 'i', 'hang', 'out', 'wit...   \n",
       "4  ['well', 'i', 'm', 'a', 'tell', 'a', 'stori', ...   \n",
       "\n",
       "                                stemmed_no_stopwords  nr_stopwords  \\\n",
       "0  ['long', 'time', 'ago', 'third', 'grade', 'fri...           281   \n",
       "1  ['softbal', 'one', 'singl', 'greatest', 'sport...           348   \n",
       "2  ['peopl', 'like', 'make', 'peopl', 'laugh', 'l...           381   \n",
       "3  ['laughter', 'caps1', 'hang', 'friend', 'one',...           378   \n",
       "4  ['well', 'tell', 'stori', 'time', 'got', 'caps...           370   \n",
       "\n",
       "   avg_sentence_len_words  unique_words  word_count  ...  nr_spelling_errors  \\\n",
       "0                    15.0           259         679  ...                  14   \n",
       "1                    12.0           308         785  ...                   7   \n",
       "2                    10.0           323         861  ...                   9   \n",
       "3                    15.0           255         712  ...                   5   \n",
       "4                    11.0           203         671  ...                  28   \n",
       "\n",
       "   nr_capitalization_errors  nr_punctuation_errors  grade_as_feature  \\\n",
       "0                         4                     17              26.0   \n",
       "1                        27                     13              40.0   \n",
       "2                        34                      9              36.0   \n",
       "3                        24                      4              40.0   \n",
       "4                        19                      3              34.0   \n",
       "\n",
       "   avg_cosine_similariy_high_grade  pattern_cosine  weighted_cosine  \\\n",
       "0                         0.285749        0.000000         0.000000   \n",
       "1                         0.285749        0.000000         0.000000   \n",
       "2                         0.285749        6.908973        -6.908973   \n",
       "3                         0.285749       13.552038       -13.552038   \n",
       "4                         0.285749       19.177102       -19.177102   \n",
       "\n",
       "                                        theses_parts  \\\n",
       "0  [[A, long, time, ago, when, I, was, in, third,...   \n",
       "1  [[Softball, has, to, be, one, of, the, single,...   \n",
       "2  [[Some, people, like, making, people, laugh, I...   \n",
       "3  [[LAUGHTER, CAPS1, I, hang, out, with, my, fri...   \n",
       "4  [[Well, i, m, a, tell, a, story, about, the, t...   \n",
       "\n",
       "                                              corpus  nr_thesis_parts  \n",
       "0  [A long time ago when I was in third grade I h...               68  \n",
       "1  [Softball has to be one of the single most gre...               79  \n",
       "2  [Some people like making people laugh I love i...               87  \n",
       "3  [LAUGHTER CAPS1 I hang out with my friends the...               72  \n",
       "4  [Well i m a tell a story about the time i got ...               68  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_short_essays = training_data[training_data[\"nr_thesis_parts\"] <= 1].copy()\n",
    "\n",
    "# remove short essays for now\n",
    "training_data = training_data[training_data[\"nr_thesis_parts\"] > 1].copy()\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        (0, 35)\\t0.07492133308391932\\n  (0, 139)\\t0....\n",
       "1        (0, 249)\\t0.047173585111079296\\n  (0, 151)\\t...\n",
       "2        (0, 18)\\t0.05742009437708632\\n  (0, 65)\\t0.0...\n",
       "3        (0, 1)\\t0.036307368663450655\\n  (0, 4)\\t0.03...\n",
       "4        (0, 26)\\t0.03815729697580636\\n  (0, 149)\\t0....\n",
       "                             ...                        \n",
       "718      (0, 217)\\t0.04753506906737664\\n  (0, 110)\\t0...\n",
       "719      (0, 66)\\t0.064224465238445\\n  (0, 113)\\t0.06...\n",
       "720      (0, 332)\\t0.02472833755589354\\n  (0, 2)\\t0.0...\n",
       "721      (0, 225)\\t0.06672803337634182\\n  (0, 30)\\t0....\n",
       "722      (0, 100)\\t0.04261248587729716\\n  (0, 130)\\t0...\n",
       "Name: TFIDF, Length: 722, dtype: object"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "training_data[\"TFIDF\"] = training_data[\"corpus\"].apply(vectorizer.fit_transform)\n",
    "training_data[\"TFIDF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<68x243 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5478 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 789,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"TFIDF\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic coherence attributes (Euclidian distance & cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [[0.0, 0.1382133935717067, 0.1157765617931835,...\n",
       "1      [[0.0, 0.08130017871482748, 0.0, 0.10544538355...\n",
       "2      [[0.0, 0.0, 0.10192836995247587, 0.0, 0.0, 0.0...\n",
       "3      [[0.08604198729883417, 0.036307368663450655, 0...\n",
       "4      [[0.07357637214147816, 0.0, 0.0, 0.0, 0.116569...\n",
       "                             ...                        \n",
       "718    [[0.0, 0.0, 0.0, 0.08396974808670399, 0.086277...\n",
       "719    [[0.04584147766752948, 0.0, 0.0372544085137157...\n",
       "720    [[0.0, 0.06869252355430468, 0.0622888963321292...\n",
       "721    [[0.08563719166391544, 0.09773176275129315, 0....\n",
       "722    [[0.10474777998671708, 0.12302548158438405, 0....\n",
       "Name: TFIDF_vector, Length: 722, dtype: object"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"TFIDF_vector\"] = training_data[\"TFIDF\"].apply(lambda x: x.toarray())\n",
    "training_data[\"TFIDF_vector\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Matrix\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html#scipy.spatial.distance.pdist\n",
    "\n",
    "* cosine_distance = 1 - cosine_similarity\n",
    "* cosine_similarity = 1 - cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(sparse_m, dist_metric, full_matrix = True):\n",
    "\n",
    "    X = sparse_m.todense()\n",
    "    dist_m = scipy.spatial.distance.pdist(X, metric = dist_metric)\n",
    "\n",
    "    if full_matrix:\n",
    "        # form condensed matrix to full matrix\n",
    "        full_m = scipy.spatial.distance.squareform(dist_m)\n",
    "        return full_m\n",
    "    else:\n",
    "        return dist_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.34344266, 0.44826083, ..., 1.33008588, 1.37409994,\n",
       "        1.38599382],\n",
       "       [0.34344266, 0.        , 0.2725315 , ..., 1.33098428, 1.37345615,\n",
       "        1.39004629],\n",
       "       [0.44826083, 0.2725315 , 0.        , ..., 1.31718043, 1.36722937,\n",
       "        1.3946548 ],\n",
       "       ...,\n",
       "       [1.33008588, 1.33098428, 1.31718043, ..., 0.        , 0.48325022,\n",
       "        0.84931995],\n",
       "       [1.37409994, 1.37345615, 1.36722937, ..., 0.48325022, 0.        ,\n",
       "        0.78902671],\n",
       "       [1.38599382, 1.39004629, 1.3946548 , ..., 0.84931995, 0.78902671,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"euclidian_distance_matrix\"] = training_data[\"TFIDF\"].apply(lambda m: get_distance_matrix(m, dist_metric = \"euclidean\"))\n",
    "training_data[\"euclidian_distance_matrix\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [[0.0, 0.05897643091158067, 0.1004688838429044...\n",
       "1      [[0.0, 0.04414812978987592, 0.0944659382057905...\n",
       "2      [[0.0, 0.03281673965419385, 0.0778123965351562...\n",
       "3      [[0.0, 0.047924349030667446, 0.085782170356536...\n",
       "4      [[0.0, 0.04099141916654414, 0.0691034861564843...\n",
       "                             ...                        \n",
       "718    [[0.0, 0.0789749013140616, 0.14460113138063324...\n",
       "719    [[0.0, 0.0501778783417256, 0.07890967154182638...\n",
       "720    [[0.0, 0.032916360663586275, 0.074313822430500...\n",
       "721    [[0.0, 0.06592245216095416, 0.1590587733799970...\n",
       "722    [[0.0, 0.05118834582332499, 0.1271396151684704...\n",
       "Name: cosine_distance_matrix, Length: 722, dtype: object"
      ]
     },
     "execution_count": 793,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosine distance not cosine similarity\n",
    "training_data[\"cosine_distance_matrix\"] = training_data[\"TFIDF\"].apply(lambda m: get_distance_matrix(m, dist_metric = \"cosine\"))\n",
    "training_data[\"cosine_distance_matrix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [[1.0, 0.9410235690884193, 0.8995311161570956,...\n",
       "1      [[1.0, 0.9558518702101241, 0.9055340617942095,...\n",
       "2      [[1.0, 0.9671832603458062, 0.9221876034648437,...\n",
       "3      [[1.0, 0.9520756509693326, 0.9142178296434633,...\n",
       "4      [[1.0, 0.9590085808334559, 0.9308965138435157,...\n",
       "                             ...                        \n",
       "718    [[1.0, 0.9210250986859384, 0.8553988686193668,...\n",
       "719    [[1.0, 0.9498221216582744, 0.9210903284581736,...\n",
       "720    [[1.0, 0.9670836393364137, 0.9256861775694994,...\n",
       "721    [[1.0, 0.9340775478390458, 0.8409412266200029,...\n",
       "722    [[1.0, 0.948811654176675, 0.8728603848315296, ...\n",
       "Name: cosine_similarity_matrix, Length: 722, dtype: object"
      ]
     },
     "execution_count": 794,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"cosine_similarity_matrix\"] = training_data[\"cosine_distance_matrix\"].apply(lambda matrix: [1 - dist for dist in matrix])\n",
    "training_data[\"cosine_similarity_matrix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.3434426616236857, 0.27253150065656184, 0.38...\n",
       "1      [0.29714686533724843, 0.27666235586270216, 0.2...\n",
       "2      [0.25619031852977464, 0.2882261379041853, 0.35...\n",
       "3      [0.3095944089632991, 0.28441145170284127, 0.28...\n",
       "4      [0.28632645412725743, 0.2669284709157069, 0.30...\n",
       "                             ...                        \n",
       "718    [0.39742899067396, 0.2928953125921563, 0.33423...\n",
       "719    [0.3167897673275623, 0.23868265076479858, 0.31...\n",
       "720    [0.25657887934740986, 0.2618133472486362, 0.24...\n",
       "721    [0.36310453635545337, 0.3509454420598844, 0.26...\n",
       "722    [0.3199635786252082, 0.37657651575169954, 0.33...\n",
       "Name: eucl_dist_neighbour, Length: 722, dtype: object"
      ]
     },
     "execution_count": 795,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get distance to neighbouring vector\n",
    "\n",
    "training_data[\"eucl_dist_neighbour\"] = training_data.apply(lambda x: [val[i + 1:][0] for i, val in enumerate(x.euclidian_distance_matrix) if i < len(x.euclidian_distance_matrix) - 1], axis = 1)\n",
    "training_data[\"eucl_dist_neighbour\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.3434426616236857, 0.27253150065656184, 0.38...\n",
       "1      [0.29714686533724843, 0.27666235586270216, 0.2...\n",
       "2      [0.25619031852977464, 0.2882261379041853, 0.35...\n",
       "3      [0.3095944089632991, 0.28441145170284127, 0.28...\n",
       "4      [0.28632645412725743, 0.2669284709157069, 0.30...\n",
       "                             ...                        \n",
       "718    [0.39742899067396, 0.2928953125921563, 0.33423...\n",
       "719    [0.3167897673275623, 0.23868265076479858, 0.31...\n",
       "720    [0.25657887934740986, 0.2618133472486362, 0.24...\n",
       "721    [0.36310453635545337, 0.3509454420598844, 0.26...\n",
       "722    [0.3199635786252082, 0.37657651575169954, 0.33...\n",
       "Name: cos_dist_neighbour, Length: 722, dtype: object"
      ]
     },
     "execution_count": 796,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get distance to neighbouring vector\n",
    "\n",
    "training_data[\"cos_dist_neighbour\"] = training_data.apply(lambda x: [val[i + 1:][0] for i, val in enumerate(x.euclidian_distance_matrix) if i < len(x.euclidian_distance_matrix) - 1], axis = 1)\n",
    "training_data[\"cos_dist_neighbour\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.6565573383763144, 0.7274684993434382, 0.614...\n",
       "1      [0.7028531346627516, 0.7233376441372978, 0.723...\n",
       "2      [0.7438096814702253, 0.7117738620958147, 0.642...\n",
       "3      [0.6904055910367008, 0.7155885482971587, 0.711...\n",
       "4      [0.7136735458727426, 0.7330715290842931, 0.696...\n",
       "                             ...                        \n",
       "718    [0.60257100932604, 0.7071046874078437, 0.66576...\n",
       "719    [0.6832102326724376, 0.7613173492352014, 0.683...\n",
       "720    [0.7434211206525901, 0.7381866527513639, 0.757...\n",
       "721    [0.6368954636445466, 0.6490545579401156, 0.734...\n",
       "722    [0.6800364213747918, 0.6234234842483004, 0.665...\n",
       "Name: cos_similarity_neighbour, Length: 722, dtype: object"
      ]
     },
     "execution_count": 797,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"cos_similarity_neighbour\"] = training_data[\"cos_dist_neighbour\"].apply(lambda cos_distances: [1 - cos_dist for cos_dist in cos_distances])\n",
    "training_data[\"cos_similarity_neighbour\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.3434426616236857, 0.27253150065656184, 0.38...\n",
       "1      [0.29714686533724843, 0.27666235586270216, 0.2...\n",
       "2      [0.25619031852977464, 0.2882261379041853, 0.35...\n",
       "3      [0.3095944089632991, 0.28441145170284127, 0.28...\n",
       "4      [0.28632645412725743, 0.2669284709157069, 0.30...\n",
       "                             ...                        \n",
       "718    [0.39742899067396, 0.2928953125921563, 0.33423...\n",
       "719    [0.3167897673275623, 0.23868265076479858, 0.31...\n",
       "720    [0.25657887934740986, 0.2618133472486362, 0.24...\n",
       "721    [0.36310453635545337, 0.3509454420598844, 0.26...\n",
       "722    [0.3199635786252082, 0.37657651575169954, 0.33...\n",
       "Name: eucl_dist_nn, Length: 722, dtype: object"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get distance to closest neighbouring vectors (nearest neighbour)\n",
    "\n",
    "training_data[\"eucl_dist_nn\"] = training_data.apply(lambda x: [(np.sort(val[i+1:]))[0] for i, val in enumerate(x.euclidian_distance_matrix) if i < len(x.euclidian_distance_matrix) - 1], axis = 1)\n",
    "training_data[\"eucl_dist_nn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.05897643091158067, 0.03713670942505898, 0.0...\n",
       "1      [0.04414812978987592, 0.03827102957575024, 0.0...\n",
       "2      [0.03281673965419385, 0.04153715328558116, 0.0...\n",
       "3      [0.047924349030667446, 0.0404449369298594, 0.0...\n",
       "4      [0.04099141916654414, 0.035625404292699336, 0....\n",
       "                             ...                        \n",
       "718    [0.0789749013140616, 0.042893832069228255, 0.0...\n",
       "719    [0.0501778783417256, 0.028484703888055685, 0.0...\n",
       "720    [0.032916360663586275, 0.03427311439876746, 0....\n",
       "721    [0.06592245216095416, 0.06158135165130407, 0.0...\n",
       "722    [0.05118834582332499, 0.07090493610784465, 0.0...\n",
       "Name: cos_dist_nn, Length: 722, dtype: object"
      ]
     },
     "execution_count": 799,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the distance to the neighbour\n",
    "training_data[\"cos_dist_nn\"] = training_data.apply(lambda x: [(np.sort(val[i+1:]))[0] for i, val in enumerate(x.cosine_distance_matrix) if i < len(x.cosine_distance_matrix) - 1], axis = 1)\n",
    "training_data[\"cos_dist_nn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.9410235690884193, 0.962863290574941, 0.9257...\n",
       "1      [0.9558518702101241, 0.9617289704242498, 0.961...\n",
       "2      [0.9671832603458062, 0.9584628467144188, 0.936...\n",
       "3      [0.9520756509693326, 0.9595550630701406, 0.958...\n",
       "4      [0.9590085808334559, 0.9643745957073007, 0.954...\n",
       "                             ...                        \n",
       "718    [0.9210250986859384, 0.9571061679307717, 0.944...\n",
       "719    [0.9498221216582744, 0.9715152961119443, 0.949...\n",
       "720    [0.9670836393364137, 0.9657268856012325, 0.970...\n",
       "721    [0.9340775478390458, 0.9384186483486959, 0.964...\n",
       "722    [0.948811654176675, 0.9290950638921553, 0.9441...\n",
       "Name: cos_similarity_nn, Length: 722, dtype: object"
      ]
     },
     "execution_count": 800,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the distance to the neighbour\n",
    "training_data[\"cos_similarity_nn\"] = training_data[\"cos_dist_nn\"].apply(lambda cos_distances: [1 - cos_dist for cos_dist in cos_distances])\n",
    "training_data[\"cos_similarity_nn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.3434426616236857, 0.4482608255087754, 0.600...\n",
       "1      [0.29714686533724843, 0.4346629457540416, 0.51...\n",
       "2      [0.25619031852977464, 0.39449308367867847, 0.5...\n",
       "3      [0.3095944089632991, 0.41420326014297976, 0.51...\n",
       "4      [0.28632645412725743, 0.3717619834154222, 0.50...\n",
       "                             ...                        \n",
       "718    [0.39742899067396, 0.5377752902107591, 0.63915...\n",
       "719    [0.3167897673275623, 0.39726482739307795, 0.50...\n",
       "720    [0.25657887934740986, 0.385522560767849, 0.471...\n",
       "721    [0.36310453635545337, 0.5640191014141223, 0.63...\n",
       "722    [0.3199635786252082, 0.5042610735888122, 0.605...\n",
       "Name: euclidian_distances, Length: 722, dtype: object"
      ]
     },
     "execution_count": 801,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"euclidian_distances\"] = training_data[\"TFIDF\"].apply(lambda m: get_distance_matrix(m, dist_metric = \"euclidean\", full_matrix = False))\n",
    "training_data[\"euclidian_distances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.05897643091158067, 0.10046888384290442, 0.1...\n",
       "1      [0.04414812978987592, 0.09446593820579052, 0.1...\n",
       "2      [0.03281673965419385, 0.07781239653515626, 0.1...\n",
       "3      [0.047924349030667446, 0.08578217035653668, 0....\n",
       "4      [0.04099141916654414, 0.06910348615648432, 0.1...\n",
       "                             ...                        \n",
       "718    [0.0789749013140616, 0.14460113138063324, 0.20...\n",
       "719    [0.0501778783417256, 0.07890967154182638, 0.12...\n",
       "720    [0.032916360663586275, 0.07431382243050055, 0....\n",
       "721    [0.06592245216095416, 0.15905877337999708, 0.2...\n",
       "722    [0.05118834582332499, 0.1271396151684704, 0.18...\n",
       "Name: cosine_distances, Length: 722, dtype: object"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"cosine_distances\"] = training_data[\"TFIDF\"].apply(lambda m: get_distance_matrix(m, dist_metric = \"cosine\", full_matrix = False))\n",
    "training_data[\"cosine_distances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.9410235690884193, 0.8995311161570956, 0.819...\n",
       "1      [0.9558518702101241, 0.9055340617942095, 0.866...\n",
       "2      [0.9671832603458062, 0.9221876034648437, 0.846...\n",
       "3      [0.9520756509693326, 0.9142178296434633, 0.869...\n",
       "4      [0.9590085808334559, 0.9308965138435157, 0.874...\n",
       "                             ...                        \n",
       "718    [0.9210250986859384, 0.8553988686193668, 0.795...\n",
       "719    [0.9498221216582744, 0.9210903284581736, 0.872...\n",
       "720    [0.9670836393364137, 0.9256861775694994, 0.888...\n",
       "721    [0.9340775478390458, 0.8409412266200029, 0.796...\n",
       "722    [0.948811654176675, 0.8728603848315296, 0.8168...\n",
       "Name: cosine_similarities, Length: 722, dtype: object"
      ]
     },
     "execution_count": 803,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"cosine_similarities\"] = training_data[\"cosine_distances\"].apply(lambda cos_dist: 1 - cos_dist)\n",
    "training_data[\"cosine_similarities\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Coherence Measures\n",
    "\n",
    "\n",
    " Change coseine distance to cosine similarity\n",
    "\n",
    "* Cosine Distance = 1 - Cosine Similarity\n",
    "* Cosine Similarity = 1 - Cosine Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average distance between any two points (2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.023180\n",
       "1      1.067530\n",
       "2      1.106320\n",
       "3      1.026832\n",
       "4      0.972721\n",
       "         ...   \n",
       "718    1.062556\n",
       "719    1.013773\n",
       "720    1.092305\n",
       "721    1.031751\n",
       "722    1.000312\n",
       "Name: avg_eucl_dist, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 804,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"avg_eucl_dist\"] = training_data[\"euclidian_distances\"].apply(np.mean)\n",
    "training_data[\"avg_eucl_dist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.560944\n",
       "1      0.604392\n",
       "2      0.648943\n",
       "3      0.562906\n",
       "4      0.507182\n",
       "         ...   \n",
       "718    0.595961\n",
       "719    0.561965\n",
       "720    0.628564\n",
       "721    0.578545\n",
       "722    0.546817\n",
       "Name: avg_cos_dist, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 805,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"avg_cos_dist\"] = training_data[\"cosine_distances\"].apply(np.mean)\n",
    "training_data[\"avg_cos_dist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.439056\n",
       "1      0.395608\n",
       "2      0.351057\n",
       "3      0.437094\n",
       "4      0.492818\n",
       "         ...   \n",
       "718    0.404039\n",
       "719    0.438035\n",
       "720    0.371436\n",
       "721    0.421455\n",
       "722    0.453183\n",
       "Name: avg_cos_similarity, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 806,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"avg_cos_similarity\"] = training_data[\"cosine_similarities\"].apply(np.mean)\n",
    "training_data[\"avg_cos_similarity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum difference(distance) between any two points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.406485\n",
       "1      1.388623\n",
       "2      1.414214\n",
       "3      1.396542\n",
       "4      1.414214\n",
       "         ...   \n",
       "718    1.414214\n",
       "719    1.414214\n",
       "720    1.414214\n",
       "721    1.375512\n",
       "722    1.414214\n",
       "Name: max_eucl_dist, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 807,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"max_eucl_dist\"] = training_data[\"euclidian_distances\"].apply(np.max)\n",
    "training_data[\"max_eucl_dist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.986793\n",
       "1      0.977745\n",
       "2      0.987549\n",
       "3      0.980955\n",
       "4      0.983596\n",
       "         ...   \n",
       "718    0.984662\n",
       "719    0.984826\n",
       "720    0.973800\n",
       "721    0.984202\n",
       "722    0.981622\n",
       "Name: max_cos_similarity, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"max_cos_similarity\"] = training_data[\"cosine_similarities\"].apply(np.max)\n",
    "training_data[\"max_cos_similarity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Distance between neighbouring points (2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.162524\n",
       "1      0.210972\n",
       "2      0.157805\n",
       "3      0.195168\n",
       "4      0.181132\n",
       "         ...   \n",
       "718    0.175146\n",
       "719    0.174205\n",
       "720    0.228913\n",
       "721    0.177750\n",
       "722    0.191719\n",
       "Name: min_neighbouring_eucl_dist, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 809,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"min_neighbouring_eucl_dist\"] = training_data[\"eucl_dist_neighbour\"].apply(lambda dist: np.array(dist).min())\n",
    "training_data[\"min_neighbouring_eucl_dist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.162524\n",
       "1      0.210972\n",
       "2      0.157805\n",
       "3      0.195168\n",
       "4      0.181132\n",
       "         ...   \n",
       "718    0.175146\n",
       "719    0.174205\n",
       "720    0.228913\n",
       "721    0.177750\n",
       "722    0.191719\n",
       "Name: min_neighbouring_cos_dist, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"min_neighbouring_cos_dist\"] = training_data[\"cos_dist_neighbour\"].apply(lambda dist: np.array(dist).min())\n",
    "training_data[\"min_neighbouring_cos_dist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.210973\n",
       "1      0.034945\n",
       "2     -0.326379\n",
       "3     -0.096903\n",
       "4     -0.131577\n",
       "         ...   \n",
       "718    0.014270\n",
       "719    0.080145\n",
       "720    0.103982\n",
       "721    0.108835\n",
       "722    0.247931\n",
       "Name: min_neighbouring_cos_similarity, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"min_neighbouring_cos_similarity\"] = training_data[\"cos_similarity_neighbour\"].apply(lambda dist: np.array(dist).min())\n",
    "training_data[\"min_neighbouring_cos_similarity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Distance between neighbouring points (2x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.789027\n",
       "1      0.965055\n",
       "2      1.326379\n",
       "3      1.096903\n",
       "4      1.131577\n",
       "         ...   \n",
       "718    0.985730\n",
       "719    0.919855\n",
       "720    0.896018\n",
       "721    0.891165\n",
       "722    0.752069\n",
       "Name: max_neighbouring_eucl_dist, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 812,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"max_neighbouring_eucl_dist\"] = training_data[\"eucl_dist_neighbour\"].apply(lambda dist: np.array(dist).max())\n",
    "training_data[\"max_neighbouring_eucl_dist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.789027\n",
       "1      0.965055\n",
       "2      1.326379\n",
       "3      1.096903\n",
       "4      1.131577\n",
       "         ...   \n",
       "718    0.985730\n",
       "719    0.919855\n",
       "720    0.896018\n",
       "721    0.891165\n",
       "722    0.752069\n",
       "Name: max_neighbouring_cos_dist, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 813,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"max_neighbouring_cos_dist\"] = training_data[\"cos_dist_neighbour\"].apply(lambda dist: np.array(dist).max())\n",
    "training_data[\"max_neighbouring_cos_dist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.837476\n",
       "1      0.789028\n",
       "2      0.842195\n",
       "3      0.804832\n",
       "4      0.818868\n",
       "         ...   \n",
       "718    0.824854\n",
       "719    0.825795\n",
       "720    0.771087\n",
       "721    0.822250\n",
       "722    0.808281\n",
       "Name: max_neighbouring_cos_similarity, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 814,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"max_neighbouring_cos_similarity\"] = training_data[\"cos_similarity_neighbour\"].apply(lambda dist: np.array(dist).max())\n",
    "training_data[\"max_neighbouring_cos_similarity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quotient of Min/Max Distance between neighbouring points (2x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.205981\n",
       "1      0.218611\n",
       "2      0.118974\n",
       "3      0.177926\n",
       "4      0.160070\n",
       "         ...   \n",
       "718    0.177681\n",
       "719    0.189383\n",
       "720    0.255478\n",
       "721    0.199458\n",
       "722    0.254922\n",
       "Name: eucl_min_max_quotient, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 815,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"eucl_min_max_quotient\"] = training_data.apply(lambda x: x.min_neighbouring_eucl_dist / x.max_neighbouring_eucl_dist if abs(x.max_neighbouring_eucl_dist) > 0 else 0, axis = 1)\n",
    "training_data[\"eucl_min_max_quotient\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.251916\n",
       "1      0.044288\n",
       "2     -0.387534\n",
       "3     -0.120401\n",
       "4     -0.160681\n",
       "         ...   \n",
       "718    0.017300\n",
       "719    0.097052\n",
       "720    0.134851\n",
       "721    0.132363\n",
       "722    0.306738\n",
       "Name: cos_min_max_quotient, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"cos_min_max_quotient\"] = training_data.apply(lambda x: x.min_neighbouring_cos_similarity / x.max_neighbouring_cos_similarity if abs(x.max_neighbouring_cos_similarity) > 0 else 0, axis = 1)\n",
    "training_data[\"cos_min_max_quotient\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average distance between neighboring points (2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.286399\n",
       "1      0.296551\n",
       "2      0.284852\n",
       "3      0.293724\n",
       "4      0.284645\n",
       "         ...   \n",
       "718    0.298230\n",
       "719    0.280517\n",
       "720    0.308581\n",
       "721    0.292770\n",
       "722    0.316295\n",
       "Name: avg_neighbouring_eucl_dist, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"avg_neighbouring_eucl_dist\"] = training_data[\"eucl_dist_neighbour\"].apply(lambda dist: np.array(dist).mean())\n",
    "training_data[\"avg_neighbouring_eucl_dist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.286399\n",
       "1      0.296551\n",
       "2      0.284852\n",
       "3      0.293724\n",
       "4      0.284645\n",
       "         ...   \n",
       "718    0.298230\n",
       "719    0.280517\n",
       "720    0.308581\n",
       "721    0.292770\n",
       "722    0.316295\n",
       "Name: avg_neighbouring_cos_dist, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"avg_neighbouring_cos_dist\"] = training_data[\"cos_dist_neighbour\"].apply(lambda dist: np.array(dist).mean())\n",
    "training_data[\"avg_neighbouring_cos_dist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.713601\n",
       "1      0.703449\n",
       "2      0.715148\n",
       "3      0.706276\n",
       "4      0.715355\n",
       "         ...   \n",
       "718    0.701770\n",
       "719    0.719483\n",
       "720    0.691419\n",
       "721    0.707230\n",
       "722    0.683705\n",
       "Name: avg_neighbouring_cos_similarity, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 819,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"avg_neighbouring_cos_similarity\"] = training_data[\"cos_similarity_neighbour\"].apply(lambda dist: np.array(dist).mean())\n",
    "training_data[\"avg_neighbouring_cos_similarity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clark and Evans’ distance to the nearest neighbor ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      4.688562\n",
       "1      5.238134\n",
       "2      5.283220\n",
       "3      4.949919\n",
       "4      4.659844\n",
       "         ...   \n",
       "718    5.531345\n",
       "719    4.160739\n",
       "720    5.622612\n",
       "721    4.420725\n",
       "722    4.336825\n",
       "Name: eucl_clark_evan_dist_nn, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 820,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"eucl_clark_evan_dist_nn\"] = training_data.apply(lambda x: (2 * np.sqrt(len(x.eucl_dist_nn)) * sum(x.eucl_dist_nn)) / len(x.eucl_dist_nn), axis = 1)\n",
    "training_data[\"eucl_clark_evan_dist_nn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      15.638643\n",
       "1      16.809977\n",
       "2      17.659164\n",
       "3      16.000071\n",
       "4      15.550602\n",
       "         ...    \n",
       "718    17.630718\n",
       "719    14.162076\n",
       "720    17.289607\n",
       "721    14.349812\n",
       "722    12.937942\n",
       "Name: cos_clark_evan_dist_nn, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"cos_clark_evan_dist_nn\"] = training_data.apply(lambda x: (2 * np.sqrt(len(x.cos_similarity_nn)) * sum(x.cos_similarity_nn)) / len(x.cos_similarity_nn), axis = 1)\n",
    "training_data[\"cos_clark_evan_dist_nn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average distance to the neirest neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.286399\n",
       "1      0.296551\n",
       "2      0.284852\n",
       "3      0.293724\n",
       "4      0.284645\n",
       "         ...   \n",
       "718    0.298230\n",
       "719    0.280517\n",
       "720    0.308581\n",
       "721    0.292770\n",
       "722    0.316295\n",
       "Name: avg_eucl_dist_nn, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 822,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"avg_eucl_dist_nn\"] = training_data[\"eucl_dist_nn\"].apply(np.mean)\n",
    "training_data[\"avg_eucl_dist_nn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.044718\n",
       "1      0.048322\n",
       "2      0.047882\n",
       "3      0.050570\n",
       "4      0.050096\n",
       "         ...   \n",
       "718    0.049415\n",
       "719    0.045193\n",
       "720    0.051110\n",
       "721    0.049660\n",
       "722    0.056404\n",
       "Name: avg_cos_dist_nn, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 823,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"avg_cos_dist_nn\"] = training_data[\"cos_dist_nn\"].apply(np.mean)\n",
    "training_data[\"avg_cos_dist_nn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.955282\n",
       "1      0.951678\n",
       "2      0.952118\n",
       "3      0.949430\n",
       "4      0.949904\n",
       "         ...   \n",
       "718    0.950585\n",
       "719    0.954807\n",
       "720    0.948890\n",
       "721    0.950340\n",
       "722    0.943596\n",
       "Name: avg_cos_similarity_nn, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"avg_cos_similarity_nn\"] = training_data[\"cos_similarity_nn\"].apply(np.mean)\n",
    "training_data[\"avg_cos_similarity_nn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative frequency distribution G of the nearest neighbors’ distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.00406763433815764, 0.0037056903240001936, 0...\n",
       "1      [0.003546953280291053, 0.0035462155929452408, ...\n",
       "2      [0.002978957192206682, 0.0030769921319905988, ...\n",
       "3      [0.004005795094406215, 0.004060004438232743, 0...\n",
       "4      [0.00398400702859264, 0.0035633380783663357, 0...\n",
       "                             ...                        \n",
       "718    [0.0034057594487460034, 0.0031361005162917413,...\n",
       "719    [0.004339684559359974, 0.00443649511462985, 0....\n",
       "720    [0.0030913117993663836, 0.003154377677694412, ...\n",
       "721    [0.004653917840253136, 0.004886915250331405, 0...\n",
       "722    [0.005811828671732947, 0.006081026708776762, 0...\n",
       "Name: eucl_cumulative_freq_distribution, Length: 722, dtype: object"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"eucl_cumulative_freq_distribution\"] = training_data.apply(lambda x: abs(np.array(x.eucl_dist_nn)[np.array(x.eucl_dist_nn) <= x.avg_eucl_dist_nn])/len(x.eucl_dist_nn), axis = 1)\n",
    "training_data[\"eucl_cumulative_freq_distribution\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.003713\n",
       "1      0.003321\n",
       "2      0.002853\n",
       "3      0.003625\n",
       "4      0.003573\n",
       "         ...   \n",
       "718    0.003096\n",
       "719    0.004297\n",
       "720    0.003377\n",
       "721    0.004319\n",
       "722    0.005731\n",
       "Name: avg_eucl_cumulative_freq_distribution, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"avg_eucl_cumulative_freq_distribution\"] = training_data[\"eucl_cumulative_freq_distribution\"].apply(lambda vec: np.array(vec).mean())\n",
    "training_data[\"avg_eucl_cumulative_freq_distribution\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.014045127896842079, 0.013816788481681917, 0...\n",
       "1      [0.012127624274223725, 0.012010067740714423, 0...\n",
       "2      [0.010886749511260691, 0.010906907135324112, 0...\n",
       "3      [0.013089800388392964, 0.013312109577398498, 0...\n",
       "4      [0.014176579590618294, 0.013964201722340412, 0...\n",
       "                             ...                        \n",
       "718    [0.010709594170766725, 0.010978427843319934, 0...\n",
       "719    [0.017269493121059534, 0.017269808581060905, 0...\n",
       "720    [0.011408057431436571, 0.011393187294752949, 0...\n",
       "721    [0.016387325400685014, 0.01646348505874905, 0....\n",
       "722    [0.019767980082811817, 0.0200380024186599, 0.0...\n",
       "Name: cos_cumulative_freq_distribution, Length: 722, dtype: object"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"cos_cumulative_freq_distribution\"] = training_data.apply(lambda x: abs(np.array(x.cos_similarity_nn)[np.array(x.cos_similarity_nn) <= x.avg_cos_similarity_nn])/len(x.cos_similarity_nn), axis = 1)\n",
    "training_data[\"cos_cumulative_freq_distribution\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.013720\n",
       "1      0.011773\n",
       "2      0.010453\n",
       "3      0.011638\n",
       "4      0.012344\n",
       "         ...   \n",
       "718    0.010378\n",
       "719    0.016220\n",
       "720    0.010889\n",
       "721    0.015313\n",
       "722    0.018016\n",
       "Name: avg_cos_cumulative_freq_distribution, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"avg_cos_cumulative_freq_distribution\"] = training_data[\"cos_cumulative_freq_distribution\"].apply(lambda vec: np.array(vec).mean())\n",
    "training_data[\"avg_cos_cumulative_freq_distribution\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.025078351529480847, 0.002032549905466275, 0...\n",
       "1      [0.0401228825064311, 0.008198136821014364, 0.0...\n",
       "2      [0.05213391609745086, 0.018474419469081276, 0....\n",
       "3      [0.08549178695172073, 0.052649652433352095, 0....\n",
       "4      [0.03545963019016269, 0.043749123898988634, 0....\n",
       "                             ...                        \n",
       "718    [0.015963207075289237, 0.02635589173433269, 0....\n",
       "719    [0.07624400762000917, 0.04740629263556807, 0.0...\n",
       "720    [0.01647511655510294, 0.012253711857395341, 0....\n",
       "721    [0.013124600249852321, 0.04877728984995034, 0....\n",
       "722    [0.006903666584652725, 0.0482229519999813, 0.0...\n",
       "Name: TFIDF_centroid, Length: 722, dtype: object"
      ]
     },
     "execution_count": 829,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the centroid of all points\n",
    "training_data[\"TFIDF_centroid\"] = training_data[\"TFIDF\"].apply(lambda x: np.average(x.toarray(), axis = 0))\n",
    "training_data[\"TFIDF_centroid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [[[[[0.02507835 0.00203255 0.03033583 0.032682...\n",
       "1      [[[[[0.04012288 0.00819814 0.01700845 0.002727...\n",
       "2      [[[[[0.05213392 0.01847442 0.01095669 0.046866...\n",
       "3      [[[[[0.08549179 0.05264965 0.01921853 0.007860...\n",
       "4      [[[[[0.03545963 0.04374912 0.02099514 0.043086...\n",
       "                             ...                        \n",
       "718    [[[[[0.01596321 0.02635589 0.01572556 0.012890...\n",
       "719    [[[[[0.07624401 0.04740629 0.05991404 0.017914...\n",
       "720    [[[[[0.01647512 0.01225371 0.01546376 0.015947...\n",
       "721    [[[[[0.0131246  0.04877729 0.01889175 0.036904...\n",
       "722    [[[[[0.00690367 0.04822295 0.01792584 0.037986...\n",
       "Name: added_centroid, Length: 722, dtype: object"
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adjusted shape before adding centroid array to the other arrays\n",
    "training_data[\"added_centroid\"] = training_data.apply(lambda x: np.concatenate([x.TFIDF_centroid.reshape(1, -1), x.TFIDF.todense()]), axis = 1)\n",
    "training_data[\"added_centroid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid_distance_matrix(matrix, dist_metric, full_matrix = True):\n",
    "\n",
    "    dist_m = scipy.spatial.distance.pdist(matrix, metric = dist_metric)\n",
    "\n",
    "    if full_matrix:\n",
    "        # form condensed matrix to full matrix\n",
    "        full_m = scipy.spatial.distance.squareform(dist_m)\n",
    "        return full_m\n",
    "    else:\n",
    "        return dist_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.87443284, 0.84616064, ..., 0.95520049, 1.02385596,\n",
       "        1.08388449],\n",
       "       [0.87443284, 0.        , 0.34344266, ..., 1.33008588, 1.37409994,\n",
       "        1.38599382],\n",
       "       [0.84616064, 0.34344266, 0.        , ..., 1.33098428, 1.37345615,\n",
       "        1.39004629],\n",
       "       ...,\n",
       "       [0.95520049, 1.33008588, 1.33098428, ..., 0.        , 0.48325022,\n",
       "        0.84931995],\n",
       "       [1.02385596, 1.37409994, 1.37345615, ..., 0.48325022, 0.        ,\n",
       "        0.78902671],\n",
       "       [1.08388449, 1.38599382, 1.39004629, ..., 0.84931995, 0.78902671,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 832,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"eucl_centroid_distance_matrix\"] = training_data.apply(lambda m: get_centroid_distance_matrix(m.added_centroid , dist_metric = \"euclidean\"), axis = 1)\n",
    "training_data[\"eucl_centroid_distance_matrix\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.48963557, 0.4532687 , ..., 0.60011195, 0.70169034,\n",
       "        0.79627981],\n",
       "       [0.48963557, 0.        , 0.05897643, ..., 0.88456422, 0.94407532,\n",
       "        0.96048944],\n",
       "       [0.4532687 , 0.05897643, 0.        , ..., 0.88575957, 0.94319089,\n",
       "        0.96611434],\n",
       "       ...,\n",
       "       [0.60011195, 0.88456422, 0.88575957, ..., 0.        , 0.11676539,\n",
       "        0.36067219],\n",
       "       [0.70169034, 0.94407532, 0.94319089, ..., 0.11676539, 0.        ,\n",
       "        0.31128157],\n",
       "       [0.79627981, 0.96048944, 0.96611434, ..., 0.36067219, 0.31128157,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 833,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"cos_centroid_distance_matrix\"] = training_data.apply(lambda m: get_centroid_distance_matrix(m.added_centroid , dist_metric = \"cosine\"), axis = 1)\n",
    "training_data[\"cos_centroid_distance_matrix\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.51036443, 0.5467313 , ..., 0.39988805, 0.29830966,\n",
       "        0.20372019],\n",
       "       [0.51036443, 1.        , 0.94102357, ..., 0.11543578, 0.05592468,\n",
       "        0.03951056],\n",
       "       [0.5467313 , 0.94102357, 1.        , ..., 0.11424043, 0.05680911,\n",
       "        0.03388566],\n",
       "       ...,\n",
       "       [0.39988805, 0.11543578, 0.11424043, ..., 1.        , 0.88323461,\n",
       "        0.63932781],\n",
       "       [0.29830966, 0.05592468, 0.05680911, ..., 0.88323461, 1.        ,\n",
       "        0.68871843],\n",
       "       [0.20372019, 0.03951056, 0.03388566, ..., 0.63932781, 0.68871843,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"cos_centroid_similarity_matrix\"] = training_data.apply(lambda m: 1 - get_centroid_distance_matrix(m.added_centroid , dist_metric = \"cosine\"), axis = 1)\n",
    "training_data[\"cos_centroid_similarity_matrix\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance of centroid with each point \n",
    "* (first row of distance matrix without 0 element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.8744328423128997, 0.8461606410286182, 0.837...\n",
       "1      [0.9284150120318378, 0.9038625869943485, 0.882...\n",
       "2      [0.8693996964907765, 0.8701808936849607, 0.845...\n",
       "3      [0.8594747727056868, 0.8380796043602232, 0.829...\n",
       "4      [0.8020141125211879, 0.7983585269821908, 0.766...\n",
       "                             ...                        \n",
       "718    [0.8446311833648924, 0.8026627198939663, 0.800...\n",
       "719    [0.8382347948753048, 0.830712008775461, 0.8162...\n",
       "720    [0.9584461418822402, 0.9404669288621743, 0.917...\n",
       "721    [0.8833904800500046, 0.8511294380385445, 0.823...\n",
       "722    [0.9058450520588263, 0.8806061272442876, 0.847...\n",
       "Name: eucl_centroid_distances, Length: 722, dtype: object"
      ]
     },
     "execution_count": 835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"eucl_centroid_distances\"] = training_data[\"eucl_centroid_distance_matrix\"].apply(lambda centroid_dist_m: centroid_dist_m[0][1:])\n",
    "training_data[\"eucl_centroid_distances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.48963556648921513, 0.4532687022859717, 0.44...\n",
       "1      [0.5737940247871554, 0.5383727740122273, 0.507...\n",
       "2      [0.4967448050812774, 0.49787960872692705, 0.46...\n",
       "3      [0.47061708799048274, 0.4433918001479239, 0.43...\n",
       "4      [0.3941417701908353, 0.39000612560655645, 0.35...\n",
       "                             ...                        \n",
       "718    [0.4559434798360752, 0.4020170572179659, 0.399...\n",
       "719    [0.4431921680934142, 0.4338139924047283, 0.416...\n",
       "720    [0.6261151877697575, 0.5983837421571712, 0.563...\n",
       "721    [0.5044014854939042, 0.46180511636560584, 0.42...\n",
       "722    [0.527565764270915, 0.4944904580335623, 0.4519...\n",
       "Name: cos_centroid_distances, Length: 722, dtype: object"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"cos_centroid_distances\"] = training_data[\"cos_centroid_distance_matrix\"].apply(lambda centroid_dist_m: centroid_dist_m[0][1:])\n",
    "training_data[\"cos_centroid_distances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.5103644335107849, 0.5467312977140283, 0.557...\n",
       "1      [0.42620597521284465, 0.4616272259877727, 0.49...\n",
       "2      [0.5032551949187226, 0.502120391273073, 0.5371...\n",
       "3      [0.5293829120095173, 0.5566081998520761, 0.566...\n",
       "4      [0.6058582298091647, 0.6099938743934435, 0.645...\n",
       "                             ...                        \n",
       "718    [0.5440565201639248, 0.5979829427820341, 0.600...\n",
       "719    [0.5568078319065858, 0.5661860075952717, 0.583...\n",
       "720    [0.3738848122302425, 0.40161625784282884, 0.43...\n",
       "721    [0.4955985145060958, 0.5381948836343942, 0.573...\n",
       "722    [0.47243423572908505, 0.5055095419664377, 0.54...\n",
       "Name: cos_centroid_similarities, Length: 722, dtype: object"
      ]
     },
     "execution_count": 837,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"cos_centroid_similarities\"] = training_data[\"cos_centroid_similarity_matrix\"].apply(lambda centroid_dist_m: centroid_dist_m[0][1:])\n",
    "training_data[\"cos_centroid_similarities\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average distance between the centroid and each point (2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.738316\n",
       "1      0.769604\n",
       "2      0.798330\n",
       "3      0.739620\n",
       "4      0.701244\n",
       "         ...   \n",
       "718    0.764402\n",
       "719    0.736932\n",
       "720    0.785634\n",
       "721    0.751063\n",
       "722    0.724932\n",
       "Name: avg_eucl_centroid_distances, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 838,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"avg_eucl_centroid_distances\"] = training_data[\"eucl_centroid_distances\"].apply(np.mean)\n",
    "training_data[\"avg_eucl_centroid_distances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.331191\n",
       "1      0.364974\n",
       "2      0.401238\n",
       "3      0.332983\n",
       "4      0.292698\n",
       "         ...   \n",
       "718    0.358993\n",
       "719    0.330620\n",
       "720    0.384436\n",
       "721    0.343167\n",
       "722    0.318402\n",
       "Name: avg_cos_centroid_distances, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"avg_cos_centroid_distances\"] = training_data[\"cos_centroid_distances\"].apply(np.mean)\n",
    "training_data[\"avg_cos_centroid_distances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.668809\n",
       "1      0.635026\n",
       "2      0.598762\n",
       "3      0.667017\n",
       "4      0.707302\n",
       "         ...   \n",
       "718    0.641007\n",
       "719    0.669380\n",
       "720    0.615564\n",
       "721    0.656833\n",
       "722    0.681598\n",
       "Name: avg_cos_centroid_similarities, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 840,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"avg_cos_centroid_similarities\"] = training_data[\"cos_centroid_similarities\"].apply(lambda x: np.mean(x))\n",
    "training_data[\"avg_cos_centroid_similarities\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal distance between the centroid and each point (2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.627981\n",
       "1      0.690605\n",
       "2      0.719405\n",
       "3      0.633794\n",
       "4      0.612015\n",
       "         ...   \n",
       "718    0.693431\n",
       "719    0.633374\n",
       "720    0.710322\n",
       "721    0.689771\n",
       "722    0.609169\n",
       "Name: min_eucl_centroid_distances, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 841,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"min_eucl_centroid_distances\"] = training_data[\"eucl_centroid_distances\"].apply(np.min)\n",
    "training_data[\"min_eucl_centroid_distances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.212820\n",
       "1      0.270642\n",
       "2      0.297741\n",
       "3      0.217998\n",
       "4      0.204220\n",
       "         ...   \n",
       "718    0.274545\n",
       "719    0.218003\n",
       "720    0.289789\n",
       "721    0.272535\n",
       "722    0.197849\n",
       "Name: min_cos_centroid_distances, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"min_cos_centroid_distances\"] = training_data[\"cos_centroid_distances\"].apply(np.min)\n",
    "training_data[\"min_cos_centroid_distances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.203720\n",
       "1      0.208918\n",
       "2      0.129920\n",
       "3      0.138855\n",
       "4      0.092020\n",
       "         ...   \n",
       "718    0.086128\n",
       "719    0.180740\n",
       "720    0.165792\n",
       "721    0.264821\n",
       "722    0.223006\n",
       "Name: min_cos_centroid_similarities, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"min_cos_centroid_similarities\"] = training_data[\"cos_centroid_similarities\"].apply(np.min)\n",
    "training_data[\"min_cos_centroid_similarities\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximal distance between the centroid and each point (2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.083884\n",
       "1      1.066734\n",
       "2      1.096784\n",
       "3      1.122352\n",
       "4      1.170515\n",
       "         ...   \n",
       "718    1.140383\n",
       "719    1.098227\n",
       "720    1.083885\n",
       "721    1.040934\n",
       "722    1.077300\n",
       "Name: max_eucl_centroid_distances, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 844,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"max_eucl_centroid_distances\"] = training_data[\"eucl_centroid_distances\"].apply(np.max)\n",
    "training_data[\"max_eucl_centroid_distances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.796280\n",
       "1      0.791082\n",
       "2      0.870080\n",
       "3      0.861145\n",
       "4      0.907980\n",
       "         ...   \n",
       "718    0.913872\n",
       "719    0.819260\n",
       "720    0.834208\n",
       "721    0.735179\n",
       "722    0.776994\n",
       "Name: max_cos_centroid_distances, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"max_cos_centroid_distances\"] = training_data[\"cos_centroid_distances\"].apply(np.max)\n",
    "training_data[\"max_cos_centroid_distances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.787180\n",
       "1      0.729358\n",
       "2      0.702259\n",
       "3      0.782002\n",
       "4      0.795780\n",
       "         ...   \n",
       "718    0.725455\n",
       "719    0.781997\n",
       "720    0.710211\n",
       "721    0.727465\n",
       "722    0.802151\n",
       "Name: max_cos_centroid_similarities, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 846,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"max_cos_centroid_similarities\"] = training_data[\"cos_centroid_similarities\"].apply(np.max)\n",
    "training_data[\"max_cos_centroid_similarities\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index (minimumdistance/maximumdistance) (2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.579380\n",
       "1      0.647402\n",
       "2      0.655923\n",
       "3      0.564701\n",
       "4      0.522859\n",
       "         ...   \n",
       "718    0.608069\n",
       "719    0.576724\n",
       "720    0.655348\n",
       "721    0.662646\n",
       "722    0.565459\n",
       "Name: eucl_centroid_min_max, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 847,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"eucl_centroid_min_max\"] = training_data.apply(lambda x: x.min_eucl_centroid_distances / x.max_eucl_centroid_distances if abs(x.max_eucl_centroid_distances) > 0 else 0, axis = 1)\n",
    "training_data[\"eucl_centroid_min_max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses distances\n",
    "#training_data[\"cos_centroid_min_max\"] = training_data.apply(lambda x: x.min_cos_centroid_distances / x.max_cos_centroid_distances, axis = 1)\n",
    "#training_data[\"cos_centroid_min_max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.258798\n",
       "1      0.286441\n",
       "2      0.185003\n",
       "3      0.177563\n",
       "4      0.115634\n",
       "         ...   \n",
       "718    0.118723\n",
       "719    0.231126\n",
       "720    0.233441\n",
       "721    0.364033\n",
       "722    0.278010\n",
       "Name: cos_centroid_min_max, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 849,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"cos_centroid_min_max\"] = training_data.apply(lambda x: x.min_cos_centroid_similarities / x.max_cos_centroid_similarities if abs(x.max_cos_centroid_similarities) > 0 else 0, axis = 1)\n",
    "training_data[\"cos_centroid_min_max\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.090155\n",
       "1      0.086912\n",
       "2      0.085868\n",
       "3      0.087804\n",
       "4      0.085726\n",
       "         ...   \n",
       "718    0.082288\n",
       "719    0.099277\n",
       "720    0.085987\n",
       "721    0.099010\n",
       "722    0.105616\n",
       "Name: standard_distance, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 850,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"standard_distance\"] = training_data.apply(lambda x: np.sqrt(sum([sum(np.subtract(vector, x.TFIDF_centroid)**2) for vector in x.TFIDF_vector])) / len(x.TFIDF_vector), axis = 1)\n",
    "training_data[\"standard_distance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.083177\n",
       "1      0.081475\n",
       "2      0.078291\n",
       "3      0.078232\n",
       "4      0.073238\n",
       "         ...   \n",
       "718    0.072159\n",
       "719    0.090397\n",
       "720    0.079332\n",
       "721    0.095116\n",
       "722    0.098037\n",
       "Name: eucl_relative_distance, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"eucl_relative_distance\"] = training_data.apply(lambda x: x.standard_distance / x.max_eucl_centroid_distances if abs(x.max_eucl_centroid_distances) > 0 else 0, axis = 1)\n",
    "training_data[\"eucl_relative_distance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.113220\n",
       "1      0.109865\n",
       "2      0.098690\n",
       "3      0.101962\n",
       "4      0.094414\n",
       "         ...   \n",
       "718    0.090044\n",
       "719    0.121179\n",
       "720    0.103077\n",
       "721    0.134674\n",
       "722    0.135929\n",
       "Name: cos_relative_distance, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"cos_relative_distance\"] = training_data.apply(lambda x: x.standard_distance / x.max_cos_centroid_distances if abs(x.max_cos_centroid_distances) > 0 else 0, axis = 1)\n",
    "training_data[\"cos_relative_distance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinant of the distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -4.476404e-22\n",
       "1      1.701262e-24\n",
       "2      1.425617e-28\n",
       "3     -4.205273e-23\n",
       "4     -2.971179e-23\n",
       "           ...     \n",
       "718    5.194272e-27\n",
       "719   -8.136834e-19\n",
       "720   -1.450856e-24\n",
       "721   -9.458844e-19\n",
       "722   -6.220687e-14\n",
       "Name: det_eucl_dist_matrix, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"det_eucl_dist_matrix\"] = training_data[\"euclidian_distance_matrix\"].apply(np.linalg.det)\n",
    "training_data[\"det_eucl_dist_matrix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -8.782959e-86\n",
       "1       5.848917e-96\n",
       "2      3.988340e-110\n",
       "3      -9.181403e-90\n",
       "4      -3.234747e-88\n",
       "           ...      \n",
       "718    8.421853e-106\n",
       "719    -6.137569e-71\n",
       "720    -3.702463e-98\n",
       "721    -9.150974e-72\n",
       "722    -1.901672e-54\n",
       "Name: det_cos_dist_matrix, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"det_cos_dist_matrix\"] = training_data[\"cosine_distance_matrix\"].apply(np.linalg.det)\n",
    "training_data[\"det_cos_dist_matrix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.857575e-86\n",
       "1       1.386964e-96\n",
       "2      4.384316e-111\n",
       "3       1.826199e-90\n",
       "4       8.858751e-89\n",
       "           ...      \n",
       "718    2.157336e-106\n",
       "719     1.773205e-71\n",
       "720     8.837047e-99\n",
       "721     2.657527e-72\n",
       "722     7.421679e-55\n",
       "Name: det_cos_similarity_matrix, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"det_cos_similarity_matrix\"] = training_data[\"cosine_similarity_matrix\"].apply(np.linalg.det)\n",
    "training_data[\"det_cos_similarity_matrix\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morans I\n",
    "\n",
    "https://stackoverflow.com/questions/45839957/python-how-do-i-compute-interactive-spatial-autocorrelation-moran-i-using-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 1.]])"
      ]
     },
     "execution_count": 857,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"weights_matrix\"] = training_data[\"TFIDF_vector\"].apply(lambda vectors: kneighbors_graph(vectors, 2, mode='connectivity', include_self = True).toarray())\n",
    "training_data[\"weights_matrix\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 858,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"sum_weights_matrix\"] = training_data[\"weights_matrix\"].apply(lambda vec: int(sum(sum(vec))))\n",
    "training_data[\"sum_weights_matrix\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"nr_dimensions\"] = training_data[\"TFIDF_vector\"].apply(lambda vectors: vectors.shape[1])\n",
    "training_data[\"nr_dimensions\"].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      37.583236\n",
       "1      47.142577\n",
       "2      55.809074\n",
       "3      39.966356\n",
       "4      33.981227\n",
       "         ...    \n",
       "718    51.252608\n",
       "719    30.908090\n",
       "720    52.170794\n",
       "721    32.977068\n",
       "722    25.700378\n",
       "Name: morans_i_denom, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 860,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"morans_i_denom\"] = training_data.apply(lambda x: sum([np.dot(np.subtract(vector, x.TFIDF_centroid), np.subtract(vector, x.TFIDF_centroid)) for vector in x.TFIDF_vector]), axis = 1)\n",
    "training_data[\"morans_i_denom\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -0.000170\n",
       "1     -0.000132\n",
       "2     -0.000117\n",
       "3     -0.000192\n",
       "4     -0.000266\n",
       "         ...   \n",
       "718   -0.000128\n",
       "719   -0.000206\n",
       "720   -0.000116\n",
       "721   -0.000193\n",
       "722   -0.000262\n",
       "Name: morans_i, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_morans_i(vec):\n",
    "    \"\"\"\n",
    "    Result: positive -> positive spacial autocorrelation (clustering of neighbouring points), Negative --> opposite of positive & Zero -> spacial randomness\n",
    "    \"\"\"\n",
    "\n",
    "    part1 = ((vec.nr_thesis_parts + 1) / vec.sum_weights_matrix) * (1 / vec.nr_dimensions)\n",
    "    numerator_sum = []\n",
    "\n",
    "    # loop over i points\n",
    "    for i, vector in enumerate(vec.TFIDF_vector):\n",
    "\n",
    "        numerator = []\n",
    "\n",
    "        # loop over j points\n",
    "        for j, vector2 in enumerate(vec.TFIDF_vector):\n",
    "            \n",
    "            if i != j:\n",
    "                # wij * Di * Dj\n",
    "                numerator.append(np.array([vec.weights_matrix[i][i], vec.weights_matrix[i][j]]) * np.dot(np.subtract(vector, vec.TFIDF_centroid), np.subtract(vector2, vec.TFIDF_centroid)))\n",
    "        \n",
    "        # sum of j\n",
    "        numerator_sum.append(sum(numerator))\n",
    "\n",
    "    # sum over all dimensions and sum of i\n",
    "    part2 = sum(sum(numerator_sum) / vec.morans_i_denom)\n",
    "    return part1 * part2\n",
    "\n",
    "\n",
    "training_data[\"morans_i\"] = training_data.apply(get_morans_i, axis = 1)\n",
    "training_data[\"morans_i\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geary’s C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.304821\n",
       "1      0.290923\n",
       "2      0.307679\n",
       "3      0.325623\n",
       "4      0.389794\n",
       "         ...   \n",
       "718    0.293233\n",
       "719    0.296006\n",
       "720    0.260629\n",
       "721    0.272364\n",
       "722    0.246744\n",
       "Name: gearys_c, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 862,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_gearys_c(vec):\n",
    "    \"\"\"\n",
    "    Result: \n",
    "    \"\"\"\n",
    "    # nr_thesis parts is already -1\n",
    "    part1 = (np.array(vec.nr_thesis_parts) / 2) * (1 / np.array(vec.nr_dimensions))\n",
    "    numerator_sum = []\n",
    "    denominator_sum = []\n",
    "\n",
    "    # loop over i points\n",
    "    for i, vector in enumerate(vec.TFIDF_vector):\n",
    "\n",
    "        numerator = []\n",
    "        denominator = []\n",
    "\n",
    "        # loop over j points\n",
    "        for j, vector2 in enumerate(vec.TFIDF_vector):\n",
    "            \n",
    "            if i != j:\n",
    "                numerator.append(np.array([vec.weights_matrix[i][i], vec.weights_matrix[i][j]]) * np.dot(np.subtract(vector, vector2), np.subtract(vector, vector2)))\n",
    "                denominator.append(np.array([vec.weights_matrix[i][i], vec.weights_matrix[i][j]]) * np.dot(np.subtract(vector, vec.TFIDF_centroid), np.subtract(vector, vec.TFIDF_centroid)))\n",
    "        \n",
    "        # sum of j points\n",
    "        numerator_sum.append(sum(numerator))\n",
    "        denominator_sum.append(sum(denominator))\n",
    "    \n",
    "    # sum of i points\n",
    "    num = np.array(sum(numerator_sum))\n",
    "    denom = np.array(sum(denominator_sum))\n",
    "\n",
    "    # outer sum over all dimensions\n",
    "    part2 = sum(num / denom)\n",
    "    return part1 * part2\n",
    "\n",
    "\n",
    "training_data[\"gearys_c\"] = training_data.apply(get_gearys_c, axis = 1)\n",
    "training_data[\"gearys_c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.13821339, 0.11577656, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.1191529 , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.1207373 , ..., 0.        , 0.        ,\n",
       "        0.05713795],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.16357415,\n",
       "        0.18443422],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.19413428,\n",
       "        0.21889158],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 863,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"TFIDF_vector\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 2])"
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,1]) * np.array([2,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gettis’s G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighting(x, i, j):\n",
    "    \"\"\"\n",
    "    A weighting function wij(d) is used to assign binary weights to every pair of points, where wij(d) = 1, if i and j are within distance d and wij(d) = 0.\n",
    "    Check in the distance matrix is i and j are within distance or not-\n",
    "    \"\"\"\n",
    "\n",
    "    d = x.avg_eucl_dist\n",
    "    distance = x.euclidian_distance_matrix[i][j]\n",
    "    \n",
    "    if distance <= d:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.007702\n",
       "1      0.006433\n",
       "2      0.006090\n",
       "3      0.007084\n",
       "4      0.007050\n",
       "         ...   \n",
       "718    0.005433\n",
       "719    0.010910\n",
       "720    0.005894\n",
       "721    0.010456\n",
       "722    0.013335\n",
       "Name: gettis_g, Length: 722, dtype: float64"
      ]
     },
     "execution_count": 866,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_gettis_g(vec):\n",
    "    \"\"\"\n",
    "    d is the average distance between any two points in semantic space\n",
    "    \"\"\"\n",
    "    # nr_thesis parts is already -1\n",
    "    part1 = (1 / np.array(vec.nr_thesis_parts + 1))\n",
    "\n",
    "    numerator_sum = []\n",
    "    denominator_sum = []\n",
    "\n",
    "    # loop over i points\n",
    "    for i, vector in enumerate(vec.TFIDF_vector):\n",
    "\n",
    "        numerator = []\n",
    "        denominator = []\n",
    "\n",
    "        # loop over j points\n",
    "        for j, vector2 in enumerate(vec.TFIDF_vector):\n",
    "            \n",
    "            if i != j:\n",
    "\n",
    "                d = get_weighting(vec, i, j)\n",
    "                \n",
    "                numerator.append(d * vector * vector2)\n",
    "                denominator.append(vector * vector2) \n",
    "                \n",
    "        \n",
    "        # sum of j points\n",
    "        numerator_sum.append(sum(numerator))\n",
    "        denominator_sum.append(sum(denominator))\n",
    "        \n",
    "\n",
    "    # sum over i points and divide\n",
    "    part2 = sum(sum(numerator_sum)) / sum(sum(denominator_sum))\n",
    "\n",
    "    # part2 outer sum over all dimensions already done\n",
    "    return part1 * part2\n",
    "\n",
    "\n",
    "training_data[\"gettis_g\"] = training_data.apply(get_gettis_g, axis = 1)\n",
    "training_data[\"gettis_g\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set features of short essays to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to fill with 0\n",
    "col = ['min_neighbouring_eucl_dist','min_neighbouring_cos_similarity','max_neighbouring_eucl_dist', 'max_neighbouring_cos_similarity', 'eucl_min_max_quotient','cos_min_max_quotient', \n",
    "'avg_neighbouring_eucl_dist','avg_neighbouring_cos_similarity', 'avg_eucl_centroid_distances','avg_cos_centroid_similarities', 'min_eucl_centroid_distances','min_cos_centroid_similarities', \n",
    "'max_eucl_centroid_distances','max_cos_centroid_similarities', 'eucl_centroid_min_max', 'cos_centroid_min_max', 'avg_max_diff', 'standard_distance', 'morans_i', 'gearys_c', 'gettis_g',\n",
    "'cos_clark_evan_dist_nn', 'eucl_clark_evan_dist_nn','avg_eucl_cumulative_freq_distribution','avg_cos_cumulative_freq_distribution', 'eucl_relative_distance','cos_relative_distance', \n",
    "'det_eucl_dist_matrix', \"det_cos_similarity_matrix\", \"avg_cos_similarity_nn\", \"avg_eucl_dist_nn\",\n",
    "\n",
    "'avg_eucl_dist', 'avg_cos_similarity', 'max_eucl_dist', 'max_cos_similarity'\n",
    "]\n",
    "\n",
    "save_short_essays[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "Name: avg_cos_centroid_similarities, dtype: int64"
      ]
     },
     "execution_count": 868,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_short_essays['avg_cos_centroid_similarities'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade</th>\n",
       "      <th>nr_stopwords</th>\n",
       "      <th>avg_sentence_len_words</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>nr_long_words</th>\n",
       "      <th>nr_short_words</th>\n",
       "      <th>most_freq_word_length</th>\n",
       "      <th>...</th>\n",
       "      <th>max_eucl_centroid_distances</th>\n",
       "      <th>max_cos_centroid_similarities</th>\n",
       "      <th>eucl_centroid_min_max</th>\n",
       "      <th>cos_centroid_min_max</th>\n",
       "      <th>standard_distance</th>\n",
       "      <th>eucl_relative_distance</th>\n",
       "      <th>det_eucl_dist_matrix</th>\n",
       "      <th>morans_i</th>\n",
       "      <th>gearys_c</th>\n",
       "      <th>gettis_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.0</td>\n",
       "      <td>281</td>\n",
       "      <td>15.0</td>\n",
       "      <td>259</td>\n",
       "      <td>679</td>\n",
       "      <td>2741</td>\n",
       "      <td>3.826215</td>\n",
       "      <td>335</td>\n",
       "      <td>344</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.083884</td>\n",
       "      <td>0.787180</td>\n",
       "      <td>0.579380</td>\n",
       "      <td>0.258798</td>\n",
       "      <td>0.090155</td>\n",
       "      <td>0.083177</td>\n",
       "      <td>-4.476404e-22</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>0.304821</td>\n",
       "      <td>0.007702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.0</td>\n",
       "      <td>348</td>\n",
       "      <td>12.0</td>\n",
       "      <td>308</td>\n",
       "      <td>785</td>\n",
       "      <td>3180</td>\n",
       "      <td>3.945223</td>\n",
       "      <td>444</td>\n",
       "      <td>341</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.066734</td>\n",
       "      <td>0.729358</td>\n",
       "      <td>0.647402</td>\n",
       "      <td>0.286441</td>\n",
       "      <td>0.086912</td>\n",
       "      <td>0.081475</td>\n",
       "      <td>1.701262e-24</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.006433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.0</td>\n",
       "      <td>381</td>\n",
       "      <td>10.0</td>\n",
       "      <td>323</td>\n",
       "      <td>861</td>\n",
       "      <td>3547</td>\n",
       "      <td>3.979094</td>\n",
       "      <td>447</td>\n",
       "      <td>414</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.096784</td>\n",
       "      <td>0.702259</td>\n",
       "      <td>0.655923</td>\n",
       "      <td>0.185003</td>\n",
       "      <td>0.085868</td>\n",
       "      <td>0.078291</td>\n",
       "      <td>1.425617e-28</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>0.307679</td>\n",
       "      <td>0.006090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0</td>\n",
       "      <td>378</td>\n",
       "      <td>15.0</td>\n",
       "      <td>255</td>\n",
       "      <td>712</td>\n",
       "      <td>2692</td>\n",
       "      <td>3.693820</td>\n",
       "      <td>349</td>\n",
       "      <td>363</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.122352</td>\n",
       "      <td>0.782002</td>\n",
       "      <td>0.564701</td>\n",
       "      <td>0.177563</td>\n",
       "      <td>0.087804</td>\n",
       "      <td>0.078232</td>\n",
       "      <td>-4.205273e-23</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>0.325623</td>\n",
       "      <td>0.007084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.0</td>\n",
       "      <td>370</td>\n",
       "      <td>11.0</td>\n",
       "      <td>203</td>\n",
       "      <td>671</td>\n",
       "      <td>2299</td>\n",
       "      <td>3.329359</td>\n",
       "      <td>313</td>\n",
       "      <td>358</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.170515</td>\n",
       "      <td>0.795780</td>\n",
       "      <td>0.522859</td>\n",
       "      <td>0.115634</td>\n",
       "      <td>0.085726</td>\n",
       "      <td>0.073238</td>\n",
       "      <td>-2.971179e-23</td>\n",
       "      <td>-0.000266</td>\n",
       "      <td>0.389794</td>\n",
       "      <td>0.007050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>35.0</td>\n",
       "      <td>399</td>\n",
       "      <td>9.0</td>\n",
       "      <td>342</td>\n",
       "      <td>862</td>\n",
       "      <td>3612</td>\n",
       "      <td>4.002320</td>\n",
       "      <td>296</td>\n",
       "      <td>566</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.140383</td>\n",
       "      <td>0.725455</td>\n",
       "      <td>0.608069</td>\n",
       "      <td>0.118723</td>\n",
       "      <td>0.082288</td>\n",
       "      <td>0.072159</td>\n",
       "      <td>5.194272e-27</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>0.293233</td>\n",
       "      <td>0.005433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>32.0</td>\n",
       "      <td>259</td>\n",
       "      <td>9.0</td>\n",
       "      <td>218</td>\n",
       "      <td>556</td>\n",
       "      <td>2269</td>\n",
       "      <td>3.865108</td>\n",
       "      <td>304</td>\n",
       "      <td>252</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098227</td>\n",
       "      <td>0.781997</td>\n",
       "      <td>0.576724</td>\n",
       "      <td>0.231126</td>\n",
       "      <td>0.099277</td>\n",
       "      <td>0.090397</td>\n",
       "      <td>-8.136834e-19</td>\n",
       "      <td>-0.000206</td>\n",
       "      <td>0.296006</td>\n",
       "      <td>0.010910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>40.0</td>\n",
       "      <td>390</td>\n",
       "      <td>9.0</td>\n",
       "      <td>369</td>\n",
       "      <td>835</td>\n",
       "      <td>3780</td>\n",
       "      <td>4.294611</td>\n",
       "      <td>308</td>\n",
       "      <td>527</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.083885</td>\n",
       "      <td>0.710211</td>\n",
       "      <td>0.655348</td>\n",
       "      <td>0.233441</td>\n",
       "      <td>0.085987</td>\n",
       "      <td>0.079332</td>\n",
       "      <td>-1.450856e-24</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>0.260629</td>\n",
       "      <td>0.005894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>40.0</td>\n",
       "      <td>272</td>\n",
       "      <td>7.0</td>\n",
       "      <td>250</td>\n",
       "      <td>576</td>\n",
       "      <td>2492</td>\n",
       "      <td>4.109375</td>\n",
       "      <td>188</td>\n",
       "      <td>388</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.040934</td>\n",
       "      <td>0.727465</td>\n",
       "      <td>0.662646</td>\n",
       "      <td>0.364033</td>\n",
       "      <td>0.099010</td>\n",
       "      <td>0.095116</td>\n",
       "      <td>-9.458844e-19</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>0.272364</td>\n",
       "      <td>0.010456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>40.0</td>\n",
       "      <td>208</td>\n",
       "      <td>8.0</td>\n",
       "      <td>232</td>\n",
       "      <td>475</td>\n",
       "      <td>2059</td>\n",
       "      <td>4.181053</td>\n",
       "      <td>163</td>\n",
       "      <td>312</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.077300</td>\n",
       "      <td>0.802151</td>\n",
       "      <td>0.565459</td>\n",
       "      <td>0.278010</td>\n",
       "      <td>0.105616</td>\n",
       "      <td>0.098037</td>\n",
       "      <td>-6.220687e-14</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>0.246744</td>\n",
       "      <td>0.013335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>722 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     grade  nr_stopwords  avg_sentence_len_words  unique_words  word_count  \\\n",
       "0     34.0           281                    15.0           259         679   \n",
       "1     46.0           348                    12.0           308         785   \n",
       "2     40.0           381                    10.0           323         861   \n",
       "3     30.0           378                    15.0           255         712   \n",
       "4     26.0           370                    11.0           203         671   \n",
       "..     ...           ...                     ...           ...         ...   \n",
       "718   35.0           399                     9.0           342         862   \n",
       "719   32.0           259                     9.0           218         556   \n",
       "720   40.0           390                     9.0           369         835   \n",
       "721   40.0           272                     7.0           250         576   \n",
       "722   40.0           208                     8.0           232         475   \n",
       "\n",
       "     char_count  avg_word_len  nr_long_words  nr_short_words  \\\n",
       "0          2741      3.826215            335             344   \n",
       "1          3180      3.945223            444             341   \n",
       "2          3547      3.979094            447             414   \n",
       "3          2692      3.693820            349             363   \n",
       "4          2299      3.329359            313             358   \n",
       "..          ...           ...            ...             ...   \n",
       "718        3612      4.002320            296             566   \n",
       "719        2269      3.865108            304             252   \n",
       "720        3780      4.294611            308             527   \n",
       "721        2492      4.109375            188             388   \n",
       "722        2059      4.181053            163             312   \n",
       "\n",
       "     most_freq_word_length  ...  max_eucl_centroid_distances  \\\n",
       "0                        3  ...                     1.083884   \n",
       "1                        4  ...                     1.066734   \n",
       "2                        3  ...                     1.096784   \n",
       "3                        2  ...                     1.122352   \n",
       "4                        4  ...                     1.170515   \n",
       "..                     ...  ...                          ...   \n",
       "718                      3  ...                     1.140383   \n",
       "719                      4  ...                     1.098227   \n",
       "720                      3  ...                     1.083885   \n",
       "721                      3  ...                     1.040934   \n",
       "722                      4  ...                     1.077300   \n",
       "\n",
       "     max_cos_centroid_similarities  eucl_centroid_min_max  \\\n",
       "0                         0.787180               0.579380   \n",
       "1                         0.729358               0.647402   \n",
       "2                         0.702259               0.655923   \n",
       "3                         0.782002               0.564701   \n",
       "4                         0.795780               0.522859   \n",
       "..                             ...                    ...   \n",
       "718                       0.725455               0.608069   \n",
       "719                       0.781997               0.576724   \n",
       "720                       0.710211               0.655348   \n",
       "721                       0.727465               0.662646   \n",
       "722                       0.802151               0.565459   \n",
       "\n",
       "     cos_centroid_min_max  standard_distance  eucl_relative_distance  \\\n",
       "0                0.258798           0.090155                0.083177   \n",
       "1                0.286441           0.086912                0.081475   \n",
       "2                0.185003           0.085868                0.078291   \n",
       "3                0.177563           0.087804                0.078232   \n",
       "4                0.115634           0.085726                0.073238   \n",
       "..                    ...                ...                     ...   \n",
       "718              0.118723           0.082288                0.072159   \n",
       "719              0.231126           0.099277                0.090397   \n",
       "720              0.233441           0.085987                0.079332   \n",
       "721              0.364033           0.099010                0.095116   \n",
       "722              0.278010           0.105616                0.098037   \n",
       "\n",
       "     det_eucl_dist_matrix  morans_i  gearys_c  gettis_g  \n",
       "0           -4.476404e-22 -0.000170  0.304821  0.007702  \n",
       "1            1.701262e-24 -0.000132  0.290923  0.006433  \n",
       "2            1.425617e-28 -0.000117  0.307679  0.006090  \n",
       "3           -4.205273e-23 -0.000192  0.325623  0.007084  \n",
       "4           -2.971179e-23 -0.000266  0.389794  0.007050  \n",
       "..                    ...       ...       ...       ...  \n",
       "718          5.194272e-27 -0.000128  0.293233  0.005433  \n",
       "719         -8.136834e-19 -0.000206  0.296006  0.010910  \n",
       "720         -1.450856e-24 -0.000116  0.260629  0.005894  \n",
       "721         -9.458844e-19 -0.000193  0.272364  0.010456  \n",
       "722         -6.220687e-14 -0.000262  0.246744  0.013335  \n",
       "\n",
       "[722 rows x 86 columns]"
      ]
     },
     "execution_count": 869,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = [\n",
    "    \n",
    "'grade',\n",
    "\n",
    "## preprocessed text\n",
    "#'normalised_docs', 'word_tok', 'sentence_tok', 'stemmed_word_token', 'stemmed_no_stopwords', \n",
    "       \n",
    "## Lexical sophistication\n",
    "'nr_stopwords', 'avg_sentence_len_words', 'unique_words', 'word_count', 'char_count', 'avg_word_len', 'nr_long_words', 'nr_short_words',\n",
    "'most_freq_word_length', 'nr_sentences', 'nr_long_sentences','nr_short_sentences', 'most_freq_sentence_length', \n",
    "\n",
    "## Readability measures\n",
    "'dale_chall_readability', 'gunning_fox_index', 'flesh_reading_ease', 'flesh_kincaid_grade_level','automated_readability_index', 'SMOG', 'LIX', 'OVIX', 'nominal_ratio',\n",
    "\n",
    "## Lexical diversity\n",
    "'TTR', 'guirauds_index', 'yules_k', 'd_estimate', 'hapax_legomena', 'advanced_guiraud',\n",
    "\n",
    "## Grammar\n",
    "'nr_unique_pos', 'avg_tree_height', 'nr_grammar_errors', \"correct_verb_form\",\n",
    "\n",
    "## POS tag\n",
    "'comparative_adj', 'superlative_adj', 'modal_aux', 'participle', 'infinitive_marker', 'verb_baseform', 'verb_past_tense', \n",
    "# coordinating conjunction\n",
    "'CCONJ', \n",
    "# subordinating conjunction\n",
    "\"SCONJ\", \n",
    "# numeral\n",
    "'NUM',\n",
    "# determiner\n",
    "'DET', \n",
    "# preposition\n",
    "'ADP',\n",
    "# adjektiv\n",
    "'ADJ', \n",
    "# adverb \n",
    "'ADV', \n",
    "# pronoun\n",
    "\"PRON\",\n",
    "# proper noun\n",
    "'PROPN', \n",
    "# common noun\n",
    "'NOUN',\n",
    "\n",
    "## Mechanics\n",
    "'nr_spelling_errors', 'nr_capitalization_errors', 'nr_punctuation_errors',\n",
    "\n",
    "## Content\n",
    "'grade_as_feature','avg_cosine_similariy_high_grade', 'pattern_cosine', 'weighted_cosine',\n",
    "\n",
    "#### Coherence ####\n",
    "\n",
    "## Basic coherence measures\n",
    "# Average distance between neighbouring points \n",
    "'avg_neighbouring_eucl_dist', 'avg_neighbouring_cos_similarity', \n",
    "# Minimum distance between neighbouring points \n",
    "'min_neighbouring_eucl_dist','min_neighbouring_cos_similarity',\n",
    "# Maximum distance between neighbouring points \n",
    "'max_neighbouring_eucl_dist', 'max_neighbouring_cos_similarity', \n",
    "# Index (minimum distance/maximum distance) \n",
    "'eucl_min_max_quotient','cos_min_max_quotient',\n",
    "# Average distance between any two points\n",
    "\"avg_eucl_dist\", \"avg_cos_similarity\",\n",
    "# Maximum difference between any two points\n",
    "\"max_eucl_dist\", \"max_cos_similarity\",\n",
    "# Clark’s and Evans’ distance to nearest neighbour\n",
    "'eucl_clark_evan_dist_nn', # 'cos_clark_evan_dist_nn'\n",
    "# Average distance to nearest neighbour\n",
    "\"avg_eucl_dist_nn\", #\"avg_cos_similarity_nn\",\n",
    "# Cumulative frequency distribution\n",
    "'avg_eucl_cumulative_freq_distribution',#'avg_cos_cumulative_freq_distribution',  \n",
    "\n",
    "## spatial data analysis\n",
    "# Average distance between points and centroid \n",
    "'avg_eucl_centroid_distances','avg_cos_centroid_similarities', \n",
    "# Minimum distance between points and centroid\n",
    "'min_eucl_centroid_distances','min_cos_centroid_similarities',\n",
    "# Maximum distance between points and centroid \n",
    "'max_eucl_centroid_distances','max_cos_centroid_similarities', \n",
    "# Index (minimum distance/maximum distance) \n",
    "'eucl_centroid_min_max', 'cos_centroid_min_max',\n",
    "# Standard distance\n",
    "'standard_distance',\n",
    "# Relative distance\n",
    "'eucl_relative_distance',#'cos_relative_distance', \n",
    "# Determinant of distance matrix\n",
    "'det_eucl_dist_matrix', #'det_cos_similarity_matrix'\n",
    "\n",
    "## spatial autocorrelation\n",
    "'morans_i', 'gearys_c', 'gettis_g',\n",
    "\n",
    "]\n",
    "\n",
    "training_data[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade</th>\n",
       "      <th>nr_stopwords</th>\n",
       "      <th>avg_sentence_len_words</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>nr_long_words</th>\n",
       "      <th>nr_short_words</th>\n",
       "      <th>most_freq_word_length</th>\n",
       "      <th>...</th>\n",
       "      <th>max_eucl_centroid_distances</th>\n",
       "      <th>max_cos_centroid_similarities</th>\n",
       "      <th>eucl_centroid_min_max</th>\n",
       "      <th>cos_centroid_min_max</th>\n",
       "      <th>standard_distance</th>\n",
       "      <th>eucl_relative_distance</th>\n",
       "      <th>det_eucl_dist_matrix</th>\n",
       "      <th>morans_i</th>\n",
       "      <th>gearys_c</th>\n",
       "      <th>gettis_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     grade  nr_stopwords  avg_sentence_len_words  unique_words  word_count  \\\n",
       "713   10.0             1                     4.0             5           5   \n",
       "\n",
       "     char_count  avg_word_len  nr_long_words  nr_short_words  \\\n",
       "713          18           3.6              2               3   \n",
       "\n",
       "     most_freq_word_length  ...  max_eucl_centroid_distances  \\\n",
       "713                      2  ...                            0   \n",
       "\n",
       "     max_cos_centroid_similarities  eucl_centroid_min_max  \\\n",
       "713                              0                      0   \n",
       "\n",
       "     cos_centroid_min_max  standard_distance  eucl_relative_distance  \\\n",
       "713                     0                  0                       0   \n",
       "\n",
       "     det_eucl_dist_matrix  morans_i  gearys_c  gettis_g  \n",
       "713                     0         0         0         0  \n",
       "\n",
       "[1 rows x 86 columns]"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_short_essays[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade</th>\n",
       "      <th>nr_stopwords</th>\n",
       "      <th>avg_sentence_len_words</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>nr_long_words</th>\n",
       "      <th>nr_short_words</th>\n",
       "      <th>most_freq_word_length</th>\n",
       "      <th>...</th>\n",
       "      <th>max_eucl_centroid_distances</th>\n",
       "      <th>max_cos_centroid_similarities</th>\n",
       "      <th>eucl_centroid_min_max</th>\n",
       "      <th>cos_centroid_min_max</th>\n",
       "      <th>standard_distance</th>\n",
       "      <th>eucl_relative_distance</th>\n",
       "      <th>det_eucl_dist_matrix</th>\n",
       "      <th>morans_i</th>\n",
       "      <th>gearys_c</th>\n",
       "      <th>gettis_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.0</td>\n",
       "      <td>281</td>\n",
       "      <td>15.0</td>\n",
       "      <td>259</td>\n",
       "      <td>679</td>\n",
       "      <td>2741</td>\n",
       "      <td>3.826215</td>\n",
       "      <td>335</td>\n",
       "      <td>344</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.083884</td>\n",
       "      <td>0.787180</td>\n",
       "      <td>0.579380</td>\n",
       "      <td>0.258798</td>\n",
       "      <td>0.090155</td>\n",
       "      <td>0.083177</td>\n",
       "      <td>-4.476404e-22</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>0.304821</td>\n",
       "      <td>0.007702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.0</td>\n",
       "      <td>348</td>\n",
       "      <td>12.0</td>\n",
       "      <td>308</td>\n",
       "      <td>785</td>\n",
       "      <td>3180</td>\n",
       "      <td>3.945223</td>\n",
       "      <td>444</td>\n",
       "      <td>341</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.066734</td>\n",
       "      <td>0.729358</td>\n",
       "      <td>0.647402</td>\n",
       "      <td>0.286441</td>\n",
       "      <td>0.086912</td>\n",
       "      <td>0.081475</td>\n",
       "      <td>1.701262e-24</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.006433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.0</td>\n",
       "      <td>381</td>\n",
       "      <td>10.0</td>\n",
       "      <td>323</td>\n",
       "      <td>861</td>\n",
       "      <td>3547</td>\n",
       "      <td>3.979094</td>\n",
       "      <td>447</td>\n",
       "      <td>414</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.096784</td>\n",
       "      <td>0.702259</td>\n",
       "      <td>0.655923</td>\n",
       "      <td>0.185003</td>\n",
       "      <td>0.085868</td>\n",
       "      <td>0.078291</td>\n",
       "      <td>1.425617e-28</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>0.307679</td>\n",
       "      <td>0.006090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0</td>\n",
       "      <td>378</td>\n",
       "      <td>15.0</td>\n",
       "      <td>255</td>\n",
       "      <td>712</td>\n",
       "      <td>2692</td>\n",
       "      <td>3.693820</td>\n",
       "      <td>349</td>\n",
       "      <td>363</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.122352</td>\n",
       "      <td>0.782002</td>\n",
       "      <td>0.564701</td>\n",
       "      <td>0.177563</td>\n",
       "      <td>0.087804</td>\n",
       "      <td>0.078232</td>\n",
       "      <td>-4.205273e-23</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>0.325623</td>\n",
       "      <td>0.007084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.0</td>\n",
       "      <td>370</td>\n",
       "      <td>11.0</td>\n",
       "      <td>203</td>\n",
       "      <td>671</td>\n",
       "      <td>2299</td>\n",
       "      <td>3.329359</td>\n",
       "      <td>313</td>\n",
       "      <td>358</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.170515</td>\n",
       "      <td>0.795780</td>\n",
       "      <td>0.522859</td>\n",
       "      <td>0.115634</td>\n",
       "      <td>0.085726</td>\n",
       "      <td>0.073238</td>\n",
       "      <td>-2.971179e-23</td>\n",
       "      <td>-0.000266</td>\n",
       "      <td>0.389794</td>\n",
       "      <td>0.007050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>32.0</td>\n",
       "      <td>259</td>\n",
       "      <td>9.0</td>\n",
       "      <td>218</td>\n",
       "      <td>556</td>\n",
       "      <td>2269</td>\n",
       "      <td>3.865108</td>\n",
       "      <td>304</td>\n",
       "      <td>252</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098227</td>\n",
       "      <td>0.781997</td>\n",
       "      <td>0.576724</td>\n",
       "      <td>0.231126</td>\n",
       "      <td>0.099277</td>\n",
       "      <td>0.090397</td>\n",
       "      <td>-8.136834e-19</td>\n",
       "      <td>-0.000206</td>\n",
       "      <td>0.296006</td>\n",
       "      <td>0.010910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>40.0</td>\n",
       "      <td>390</td>\n",
       "      <td>9.0</td>\n",
       "      <td>369</td>\n",
       "      <td>835</td>\n",
       "      <td>3780</td>\n",
       "      <td>4.294611</td>\n",
       "      <td>308</td>\n",
       "      <td>527</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.083885</td>\n",
       "      <td>0.710211</td>\n",
       "      <td>0.655348</td>\n",
       "      <td>0.233441</td>\n",
       "      <td>0.085987</td>\n",
       "      <td>0.079332</td>\n",
       "      <td>-1.450856e-24</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>0.260629</td>\n",
       "      <td>0.005894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>40.0</td>\n",
       "      <td>272</td>\n",
       "      <td>7.0</td>\n",
       "      <td>250</td>\n",
       "      <td>576</td>\n",
       "      <td>2492</td>\n",
       "      <td>4.109375</td>\n",
       "      <td>188</td>\n",
       "      <td>388</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.040934</td>\n",
       "      <td>0.727465</td>\n",
       "      <td>0.662646</td>\n",
       "      <td>0.364033</td>\n",
       "      <td>0.099010</td>\n",
       "      <td>0.095116</td>\n",
       "      <td>-9.458844e-19</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>0.272364</td>\n",
       "      <td>0.010456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>40.0</td>\n",
       "      <td>208</td>\n",
       "      <td>8.0</td>\n",
       "      <td>232</td>\n",
       "      <td>475</td>\n",
       "      <td>2059</td>\n",
       "      <td>4.181053</td>\n",
       "      <td>163</td>\n",
       "      <td>312</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.077300</td>\n",
       "      <td>0.802151</td>\n",
       "      <td>0.565459</td>\n",
       "      <td>0.278010</td>\n",
       "      <td>0.105616</td>\n",
       "      <td>0.098037</td>\n",
       "      <td>-6.220687e-14</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>0.246744</td>\n",
       "      <td>0.013335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>723 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     grade  nr_stopwords  avg_sentence_len_words  unique_words  word_count  \\\n",
       "0     34.0           281                    15.0           259         679   \n",
       "1     46.0           348                    12.0           308         785   \n",
       "2     40.0           381                    10.0           323         861   \n",
       "3     30.0           378                    15.0           255         712   \n",
       "4     26.0           370                    11.0           203         671   \n",
       "..     ...           ...                     ...           ...         ...   \n",
       "719   32.0           259                     9.0           218         556   \n",
       "720   40.0           390                     9.0           369         835   \n",
       "721   40.0           272                     7.0           250         576   \n",
       "722   40.0           208                     8.0           232         475   \n",
       "713   10.0             1                     4.0             5           5   \n",
       "\n",
       "     char_count  avg_word_len  nr_long_words  nr_short_words  \\\n",
       "0          2741      3.826215            335             344   \n",
       "1          3180      3.945223            444             341   \n",
       "2          3547      3.979094            447             414   \n",
       "3          2692      3.693820            349             363   \n",
       "4          2299      3.329359            313             358   \n",
       "..          ...           ...            ...             ...   \n",
       "719        2269      3.865108            304             252   \n",
       "720        3780      4.294611            308             527   \n",
       "721        2492      4.109375            188             388   \n",
       "722        2059      4.181053            163             312   \n",
       "713          18      3.600000              2               3   \n",
       "\n",
       "     most_freq_word_length  ...  max_eucl_centroid_distances  \\\n",
       "0                        3  ...                     1.083884   \n",
       "1                        4  ...                     1.066734   \n",
       "2                        3  ...                     1.096784   \n",
       "3                        2  ...                     1.122352   \n",
       "4                        4  ...                     1.170515   \n",
       "..                     ...  ...                          ...   \n",
       "719                      4  ...                     1.098227   \n",
       "720                      3  ...                     1.083885   \n",
       "721                      3  ...                     1.040934   \n",
       "722                      4  ...                     1.077300   \n",
       "713                      2  ...                     0.000000   \n",
       "\n",
       "     max_cos_centroid_similarities  eucl_centroid_min_max  \\\n",
       "0                         0.787180               0.579380   \n",
       "1                         0.729358               0.647402   \n",
       "2                         0.702259               0.655923   \n",
       "3                         0.782002               0.564701   \n",
       "4                         0.795780               0.522859   \n",
       "..                             ...                    ...   \n",
       "719                       0.781997               0.576724   \n",
       "720                       0.710211               0.655348   \n",
       "721                       0.727465               0.662646   \n",
       "722                       0.802151               0.565459   \n",
       "713                       0.000000               0.000000   \n",
       "\n",
       "     cos_centroid_min_max  standard_distance  eucl_relative_distance  \\\n",
       "0                0.258798           0.090155                0.083177   \n",
       "1                0.286441           0.086912                0.081475   \n",
       "2                0.185003           0.085868                0.078291   \n",
       "3                0.177563           0.087804                0.078232   \n",
       "4                0.115634           0.085726                0.073238   \n",
       "..                    ...                ...                     ...   \n",
       "719              0.231126           0.099277                0.090397   \n",
       "720              0.233441           0.085987                0.079332   \n",
       "721              0.364033           0.099010                0.095116   \n",
       "722              0.278010           0.105616                0.098037   \n",
       "713              0.000000           0.000000                0.000000   \n",
       "\n",
       "     det_eucl_dist_matrix  morans_i  gearys_c  gettis_g  \n",
       "0           -4.476404e-22 -0.000170  0.304821  0.007702  \n",
       "1            1.701262e-24 -0.000132  0.290923  0.006433  \n",
       "2            1.425617e-28 -0.000117  0.307679  0.006090  \n",
       "3           -4.205273e-23 -0.000192  0.325623  0.007084  \n",
       "4           -2.971179e-23 -0.000266  0.389794  0.007050  \n",
       "..                    ...       ...       ...       ...  \n",
       "719         -8.136834e-19 -0.000206  0.296006  0.010910  \n",
       "720         -1.450856e-24 -0.000116  0.260629  0.005894  \n",
       "721         -9.458844e-19 -0.000193  0.272364  0.010456  \n",
       "722         -6.220687e-14 -0.000262  0.246744  0.013335  \n",
       "713          0.000000e+00  0.000000  0.000000  0.000000  \n",
       "\n",
       "[723 rows x 86 columns]"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df = pd.concat([training_data[feature_cols], save_short_essays[feature_cols]], axis = 0)\n",
    "feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.to_csv(\"csv_files/features_2_training_set_8.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "454b10af5dccce86783b9f8c85ade064919a97f3a1663be32e61664f18b345a3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
